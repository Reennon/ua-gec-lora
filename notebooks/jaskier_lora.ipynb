{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.27.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.19.4)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.4.1)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "zsh:1: no matches found: huggingface_hub[cli]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install -U \"huggingface_hub[cli]\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T17:41:15.235623Z",
     "start_time": "2024-03-01T17:41:13.536582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/rkovalch/miniconda3/envs/simulationllm/bin/huggingface-cli\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 49, in main\r\n",
      "    service.run()\r\n",
      "  File \"/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/huggingface_hub/commands/delete_cache.py\", line 137, in run\r\n",
      "    selected_hashes = _manual_review_tui(hf_cache_info, preselected=[])\r\n",
      "  File \"/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/huggingface_hub/commands/delete_cache.py\", line 86, in _inner\r\n",
      "    raise ImportError(\r\n",
      "ImportError: The `delete-cache` command requires extra dependencies to work with the TUI.\r\n",
      "Please run `pip install huggingface_hub[cli]` to install them.\r\n",
      "Otherwise, disable TUI using the `--disable-tui` flag.\r\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli delete-cache"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T17:41:34.261969Z",
     "start_time": "2024-03-01T17:41:33.874740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f5425c445c9495bad7d51b1e05bd5b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland, as a nation, doesn't physically travel to space. However, Poland has contributed to the field of space exploration through its scientists, engineers, and collaborations with international space agencies. The Polish Space Agency, established in 2016, aims to promote and coordinate the country's space activities.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "import torch\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "base_model_name = \"bardsai/jaskier-7b-dpo-v6.1\"\n",
    "chatbot = pipeline(\"conversational\", model=base_model_name, torch_dtype=torch.half, device_map=parameter_server.settings.device)\n",
    "conversation = Conversation(\"Can Poland into space?\")\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:39:45.733637Z",
     "start_time": "2024-03-02T14:38:31.234989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: scipy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from bitsandbytes) (1.11.3)\r\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.24.4)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.28.0.dev0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.21.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.4.1)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:15:46.305948Z",
     "start_time": "2024-03-02T11:15:43.703451Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.30 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (4.30.0)\r\n",
      "Requirement already satisfied: accelerate in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.28.0.dev0)\r\n",
      "Requirement already satisfied: bitsandbytes in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (0.21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (4.66.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (2.3.0.dev20240301)\r\n",
      "Requirement already satisfied: scipy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from bitsandbytes) (1.11.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.31 accelerate bitsandbytes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:11:51.878124Z",
     "start_time": "2024-03-02T12:11:50.607695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:16:15.397541Z",
     "start_time": "2024-03-02T12:16:15.373662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-38s7vmlf\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-38s7vmlf\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 831bc25d8fdb85768402f772cf65cc3d7872b211\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\r\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.0.dev0)\r\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/01/04/45d88b8bddc09bf56ae1631721393255b75798af515c65c26389713a2072/tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2023.7.22)\r\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8593748 sha256=1749d90b38a8aa113244e31c097feb67e687b6731d409cd84e6f0b02a09ef003\r\n",
      "  Stored in directory: /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-ephem-wheel-cache-le8hllh0/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.13.3\r\n",
      "    Uninstalling tokenizers-0.13.3:\r\n",
      "      Successfully uninstalled tokenizers-0.13.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.30.0\r\n",
      "    Uninstalling transformers-4.30.0:\r\n",
      "      Successfully uninstalled transformers-4.30.0\r\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.39.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:13:10.077880Z",
     "start_time": "2024-03-02T12:12:48.113452Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto_gptq==0.2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: accelerate>=0.19.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (0.28.0.dev0)\r\n",
      "Requirement already satisfied: datasets in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (2.18.0)\r\n",
      "Requirement already satisfied: numpy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (1.24.4)\r\n",
      "Requirement already satisfied: rouge in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (1.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (2.3.0.dev20240301)\r\n",
      "Requirement already satisfied: safetensors in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (0.4.1)\r\n",
      "Requirement already satisfied: transformers>=4.26.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from auto_gptq==0.2.0) (4.39.0.dev0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate>=0.19.0->auto_gptq==0.2.0) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate>=0.19.0->auto_gptq==0.2.0) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate>=0.19.0->auto_gptq==0.2.0) (6.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate>=0.19.0->auto_gptq==0.2.0) (0.21.3)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->auto_gptq==0.2.0) (2023.12.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers>=4.26.1->auto_gptq==0.2.0) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers>=4.26.1->auto_gptq==0.2.0) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers>=4.26.1->auto_gptq==0.2.0) (0.15.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers>=4.26.1->auto_gptq==0.2.0) (4.66.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (14.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (1.5.3)\r\n",
      "Requirement already satisfied: xxhash in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->auto_gptq==0.2.0) (3.8.6)\r\n",
      "Requirement already satisfied: six in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from rouge->auto_gptq==0.2.0) (1.16.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (3.3.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->auto_gptq==0.2.0) (1.3.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto_gptq==0.2.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto_gptq==0.2.0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers>=4.26.1->auto_gptq==0.2.0) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto_gptq==0.2.0) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from pandas->datasets->auto_gptq==0.2.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from pandas->datasets->auto_gptq==0.2.0) (2023.3.post1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto_gptq==0.2.0) (1.3.0)\r\n",
      "Collecting transformers==4.30\r\n",
      "  Obtaining dependency information for transformers==4.30 from https://files.pythonhosted.org/packages/e2/72/1af3d38e98fdcceb3876de4567ac395a66c26976e259fe2d46266e052d61/transformers-4.30.0-py3-none-any.whl.metadata\r\n",
      "  Using cached transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (0.21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (2.31.0)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30)\r\n",
      "  Obtaining dependency information for tokenizers!=0.11.3,<0.14,>=0.11.1 from https://files.pythonhosted.org/packages/70/68/0a450e4dc488031b82fcd869840c542b86aad4a07d0eca1d7e9cbb9d742e/tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata\r\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.30) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.30) (2023.7.22)\r\n",
      "Using cached transformers-4.30.0-py3-none-any.whl (7.2 MB)\r\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\r\n",
      "Installing collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.15.2\r\n",
      "    Uninstalling tokenizers-0.15.2:\r\n",
      "      Successfully uninstalled tokenizers-0.15.2\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.39.0.dev0\r\n",
      "    Uninstalling transformers-4.39.0.dev0:\r\n",
      "      Successfully uninstalled transformers-4.39.0.dev0\r\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.30.0\r\n",
      "Requirement already satisfied: optimum in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (1.17.1)\r\n",
      "Requirement already satisfied: coloredlogs in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (15.0.1)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (1.12)\r\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (4.30.0)\r\n",
      "Requirement already satisfied: torch>=1.11 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (2.3.0.dev20240301)\r\n",
      "Requirement already satisfied: packaging in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (23.1)\r\n",
      "Requirement already satisfied: numpy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (1.24.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (0.21.3)\r\n",
      "Requirement already satisfied: datasets in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from optimum) (2.18.0)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2023.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.4.1)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\r\n",
      "Requirement already satisfied: protobuf<=3.20.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.20.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (14.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (1.5.3)\r\n",
      "Requirement already satisfied: xxhash in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from datasets->optimum) (3.8.6)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (3.3.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\r\n",
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-n_6le_ho\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-n_6le_ho\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 831bc25d8fdb85768402f772cf65cc3d7872b211\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\r\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.0.dev0)\r\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/01/04/45d88b8bddc09bf56ae1631721393255b75798af515c65c26389713a2072/tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata\r\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2023.7.22)\r\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8593748 sha256=8582b3a200560e63ac486b9226a2679d96c94e716264de1024b777b6536bca5f\r\n",
      "  Stored in directory: /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-ephem-wheel-cache-qqrixdxq/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.13.3\r\n",
      "    Uninstalling tokenizers-0.13.3:\r\n",
      "      Successfully uninstalled tokenizers-0.13.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.30.0\r\n",
      "    Uninstalling transformers-4.30.0:\r\n",
      "      Successfully uninstalled transformers-4.30.0\r\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.39.0.dev0\r\n",
      "Requirement already satisfied: accelerate in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.28.0.dev0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (2.3.0.dev20240301)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.21.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.4.1)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install auto_gptq==0.2.0\n",
    "!pip install transformers==4.30\n",
    "!pip install --upgrade optimum\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade accelerate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:35:52.948871Z",
     "start_time": "2024-03-02T14:35:21.978503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0727a6bb1f3e4e779a4e7bb59d502572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "auto-gptq is required in order to perform quantzation : `pip install auto-gptq`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 19\u001B[0m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16)\n\u001B[1;32m     13\u001B[0m quantizer \u001B[38;5;241m=\u001B[39m GPTQQuantizer(\n\u001B[1;32m     14\u001B[0m     bits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     15\u001B[0m     dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m     block_name_to_quantize\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel.decoder.layers\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     model_seqlen \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2048\u001B[39m\n\u001B[1;32m     18\u001B[0m )\n\u001B[0;32m---> 19\u001B[0m quantized_model \u001B[38;5;241m=\u001B[39m \u001B[43mquantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m save_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/jaskier-7b-dpo-v6.1-gptq-4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     22\u001B[0m quantizer\u001B[38;5;241m.\u001B[39msave(model,save_folder)\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/optimum/gptq/quantizer.py:321\u001B[0m, in \u001B[0;36mGPTQQuantizer.quantize_model\u001B[0;34m(self, model, tokenizer)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;124;03mQuantizes the model using the dataset\u001B[39;00m\n\u001B[1;32m    304\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;124;03m    `nn.Module`: The quantized model\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_auto_gptq_available():\n\u001B[0;32m--> 321\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto-gptq is required in order to perform quantzation : `pip install auto-gptq`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo GPU found. A GPU is needed to quantize model.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: auto-gptq is required in order to perform quantzation : `pip install auto-gptq`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "model_name = \"bardsai/jaskier-7b-dpo-v6.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "quantizer = GPTQQuantizer(\n",
    "    bits=4,\n",
    "    dataset=\"c4\",\n",
    "    block_name_to_quantize=\"model.decoder.layers\",\n",
    "    model_seqlen = 2048\n",
    ")\n",
    "quantized_model = quantizer.quantize_model(model, tokenizer)\n",
    "\n",
    "save_folder = \"../models/jaskier-7b-dpo-v6.1-gptq-4bit\"\n",
    "quantizer.save(model,save_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:36:29.720506Z",
     "start_time": "2024-03-02T14:35:58.602568Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 14\u001B[0m\n\u001B[1;32m      8\u001B[0m bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(\n\u001B[1;32m      9\u001B[0m     load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     10\u001B[0m     bnb_4bit_quant_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnf4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m     bnb_4bit_compute_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[1;32m     12\u001B[0m )\n\u001B[1;32m     13\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbardsai/jaskier-7b-dpo-v6.1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 14\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameter_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccessfully loaded the model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m into memory\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/modeling_utils.py:3029\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3026\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3028\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3029\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3030\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\n\u001B[1;32m   3031\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3032\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3033\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_environment\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_accelerate_available() \u001B[38;5;129;01mand\u001B[39;00m is_bitsandbytes_available()):\n\u001B[0;32m---> 62\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m         )\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_tf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_flax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     69\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m sure the weights are in PyTorch format.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         )\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model_name = \"bardsai/jaskier-7b-dpo-v6.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device=parameter_server.settings.device,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T12:16:09.874089Z",
     "start_time": "2024-03-02T12:16:09.490457Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      5\u001B[0m parameter_server \u001B[38;5;241m=\u001B[39m ParameterServer()\n\u001B[0;32m----> 6\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbardsai/jaskier-7b-dpo-v6.1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#llm_int8_enable_fp32_cpu_offload=True,\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameter_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameter_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m model\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    560\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    565\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    566\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    567\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/modeling_utils.py:3024\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3021\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3024\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3025\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\n\u001B[1;32m   3026\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3027\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3028\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_environment\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_accelerate_available() \u001B[38;5;129;01mand\u001B[39;00m is_bitsandbytes_available()):\n\u001B[0;32m---> 62\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m         )\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_tf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_flax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     69\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m sure the weights are in PyTorch format.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         )\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline, Conversation\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "import torch\n",
    "parameter_server = ParameterServer()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bardsai/jaskier-7b-dpo-v6.1\",\n",
    "    load_in_4bit=True,\n",
    "    #llm_int8_enable_fp32_cpu_offload=True,\n",
    "    device_map=parameter_server.settings.device,\n",
    "    device=parameter_server.settings.device,\n",
    ")\n",
    "model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T11:27:45.773875Z",
     "start_time": "2024-03-02T11:27:45.502480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\r\n",
      "  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/08/87/3e7eb34ac06d3f4ac72e2302e9e69bef12247a8a627c59a4d8a498135727/peft-0.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (2.1.2)\r\n",
      "Requirement already satisfied: transformers in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (4.35.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (0.27.2)\r\n",
      "Requirement already satisfied: safetensors in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (0.4.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from peft) (0.19.4)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.1)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers->peft) (2023.10.3)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n",
      "Downloading peft-0.9.0-py3-none-any.whl (190 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m190.9/190.9 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.9.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T15:41:09.520267Z",
     "start_time": "2024-03-01T15:41:07.217427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = chatbot.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:22:49.493453Z",
     "start_time": "2024-03-01T20:22:49.488485Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T17:19:56.057036Z",
     "start_time": "2024-03-01T17:19:56.045700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,703,936 || all params: 7,243,436,032 || trainable%: 0.023523863432663224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:44:32.884440Z",
     "start_time": "2024-03-02T14:44:32.821912Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ua_gec\r\n",
      "  Obtaining dependency information for ua_gec from https://files.pythonhosted.org/packages/5b/be/7366527adb1982b877ac9f176e1bca31a3104d59937b40ed6f46dd75ac4d/ua_gec-2.1.3-py3-none-any.whl.metadata\r\n",
      "  Downloading ua_gec-2.1.3-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Downloading ua_gec-2.1.3-py3-none-any.whl (36.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m36.0/36.0 MB\u001B[0m \u001B[31m39.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: ua_gec\r\n",
      "Successfully installed ua_gec-2.1.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ua_gec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T16:34:22.012229Z",
     "start_time": "2024-03-01T16:34:12.559725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  _   _          _      _\r\n",
      " | | | |_ __  __| |__ _| |_ ___\r\n",
      " | |_| | '_ \\/ _` / _` |  _/ -_)\r\n",
      "  \\___/| .__/\\__,_\\__,_|\\__\\___|\r\n",
      "       |_|\r\n",
      "                       \r\n",
      "Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\r\n",
      "\r\n",
      "https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html\r\n",
      "\r\n",
      "Please note that updating to Notebook 7 might break some of your extensions.\r\n",
      "\r\n",
      "\u001B[33m[W 18:35:23.186 NotebookApp]\u001B[m Loading JupyterLab as a classic notebook (v6) extension.\r\n",
      "\u001B[33m[W 2024-03-01 18:35:23.188 LabApp]\u001B[m 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\r\n",
      "\u001B[33m[W 2024-03-01 18:35:23.188 LabApp]\u001B[m 'iopub_data_rate_limit' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.\r\n",
      "\u001B[32m[I 2024-03-01 18:35:23.190 LabApp]\u001B[m JupyterLab extension loaded from /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/jupyterlab\r\n",
      "\u001B[32m[I 2024-03-01 18:35:23.190 LabApp]\u001B[m JupyterLab application directory is /Users/rkovalch/miniconda3/envs/simulationllm/share/jupyter/lab\r\n",
      "\u001B[32m[I 18:35:23.192 NotebookApp]\u001B[m The port 8888 is already in use, trying another port.\r\n",
      "\u001B[32m[I 18:35:23.193 NotebookApp]\u001B[m Serving notebooks from local directory: /Users/rkovalch/Documents/UCU/ua-gec-lora/notebooks\r\n",
      "\u001B[32m[I 18:35:23.193 NotebookApp]\u001B[m Jupyter Notebook 6.5.4 is running at:\r\n",
      "\u001B[32m[I 18:35:23.193 NotebookApp]\u001B[m http://localhost:8889/?token=007542e6bbff700fcc1b2771e82589d305b71878e9f718dc\r\n",
      "\u001B[32m[I 18:35:23.193 NotebookApp]\u001B[m  or http://127.0.0.1:8889/?token=007542e6bbff700fcc1b2771e82589d305b71878e9f718dc\r\n",
      "\u001B[32m[I 18:35:23.193 NotebookApp]\u001B[m Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n",
      "\u001B[35m[C 18:35:23.196 NotebookApp]\u001B[m \r\n",
      "    \r\n",
      "    To access the notebook, open this file in a browser:\r\n",
      "        file:///Users/rkovalch/Library/Jupyter/runtime/nbserver-35788-open.html\r\n",
      "    Or copy and paste one of these URLs:\r\n",
      "        http://localhost:8889/?token=007542e6bbff700fcc1b2771e82589d305b71878e9f718dc\r\n",
      "     or http://127.0.0.1:8889/?token=007542e6bbff700fcc1b2771e82589d305b71878e9f718dc\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T16:35:41.385539Z",
     "start_time": "2024-03-01T16:35:21.723006Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaTokenizerFast(name_or_path='bardsai/jaskier-7b-dpo-v6.1', vocab_size=32000, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:46:10.356593Z",
     "start_time": "2024-03-02T14:46:10.176812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T14:44:43.660556Z",
     "start_time": "2024-03-02T14:44:43.656879Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Source starts:---\n",
      "\n",
      "Byte for France або “Мій досвід ведення блогу у Instagram”\n",
      "Останні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\n",
      "\n",
      "Сьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\n",
      "\n",
      "Моє бачення Instagram\n",
      "Колись давно я прочитав статтю, чи просто коментарій – вже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.\n",
      "\n",
      "Було це за декілька років до появи усім відомого сервісу. Як же автору вдалося передбачити майбутнє? Дуже просто!\n",
      "\n",
      "Instagram втілює глибинні бажання кожної людини:\n",
      "\n",
      "емоційне збудження від перегляду гарних фото (читання – це менш природний процес, ніж споглядання)\n",
      "нескінченне поглинання ніби-то важливої інформації про інших людей (користовуч відчуває себе у потоці)\n",
      "соціальне підтвердження того, що користувач виглядає та поводиться відповідно до тренду\n",
      "Якщо честно, то всі зазначені моменти мені не дуже близькі, тому до поїздки у Францію у мене в Instagram не було жодного фото. Але що сталося потім?\n",
      "\n",
      "Byte for France\n",
      "Французькою мовою до Франції я володів не дуже добре. Треба було якось виправляти ситуацію. Не знаю як у інших, а у мене в житті траплялось не так багато див. Знаєте, так щоб прокинувся одного ранку і володієш мовою як місцевий.\n",
      "\n",
      "Все, що я зараз вмію, давалося мені доволі важко, через щоденну працю протягом багатьох років. Але як змусити себе займатися французькою кожен день? Особливо письмом?\n",
      "\n",
      "Мені завжди було цікаво читати статті про життя закордоном. Але в них, на мій погляд, завжди не вистачало системності. Тож я вирішив започаткувати проект під назвою Byte For France – Instagram-блог, в якому б я честно та системно (кожного дня) ділився враженнями про життя у Франції французькою та російською мовами.\n",
      "\n",
      "Перші кроки\n",
      "Свій перший пост я опублікував 3-го лютого 2018 року. Це було фото валіз, котрі ми зібрали з собою до Франції. Тоді на мене були підписані лише декілька моїх друзів, але початок будо покладено.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій перший пост\n",
      "Я відразу ж вирішив, що буду буду писати все як є, щоб не вийшло такого, знаєте, ідеального Instagram, що показує лише приємні моменти. Я прагнув максимально точно передати свій досвід.\n",
      "\n",
      "До речі, всі ці ідеальні Instagram зламали психіку далеко не одному підлітку (та не тільки). Досі не розумію чому люди так бояться бути самими собою і створюють собі ці нудні образи “успішних людей”.\n",
      "\n",
      "Але повернемося до теми. Спочатку написання постів мені давалося дуже важко, адже я витрачав по декілька годин на їх переклад кожного дня. На щастя тем для записів було багато, залишалось тільки обирати про що розповісти.\n",
      "\n",
      "До речі поступово на мене почали підписуватися друзі та знайомі, а згодом і не знайомі люди… Як це сталося, розповім трохи далі.\n",
      "\n",
      "Прогрес та мотивація\n",
      "Якщо щось робити, то щось починає виходити. Так сталося і цього разу. Несподівано мені почала допомагати мій викладач французької мови з університету – вона корегувала кожен мій запис. Помилок було дуже багато, але кожного разу їх було все менше і це не могло не радувати.\n",
      "\n",
      "Поступово мені дуже сподобалося ділитися цікавою інформацією, що могла стати у нагоді іншим людям. Мені здається, що я навіть почав жити цікавіше, щоб було про що розповісти.\n",
      "\n",
      "Ми відвідували багато музеїв, парків, подорожували по іншим містам та країнам. Я навіть писав про свої співбесіди та роботу, власні думки.\n",
      "\n",
      "Звідки беруться підписники?\n",
      "Instagram створено таким чином, щоб отримати підписників органічним шляхом було максимально важко. У головній стрічці кожен користувач бачить лише тих на кого вже підписаний. У пошуку користувач переглядає контент, не звертаючи особливої уваги на його автора.\n",
      "\n",
      "Таким чином кожен користувач має привести підписників з іншого джерела (реальних знайомих, наприклад) або ж придбати рекламу в Instagram, щоб влізти зі своїм контентом у стрічку незнайомців.\n",
      "\n",
      "Третій спосіб, на котрий витрачають час звичайні користувачі – лайки та підписки на інших користувачів у надії на те, що ті підпишуться у відповідь. Зазвичай кількість підписок / підписників у таких користувачів майже однакова.\n",
      "\n",
      "Є ще четвертий спосіб, але він вже для зовсім зневірених людей – придбання підписок та лайків. Є (чи принаймні були) цілі мережі фейкових аккаунтів, що за невелику винагороду підпишуться, пролайкають та навіть прокоментують ваші пости.\n",
      "\n",
      "Тільки задумайтесь, що люди готові витрачати власні кошти та брехати тисячам інших людей, лиша заради того, щоб виглядати краще у очах тих же самих людей. Але проблема мабуть не в Instagram, чи не так?\n",
      "\n",
      "Розумне чи хитре рішення?\n",
      "Як і кожному Instagram-блогеру, мені здавалося, що мій контент має побачити більше людей. Тому після того як перша хвиля підписників закінчилася (знайомі та друзі із реального життя), я вирішив, що потрібно збільшити охоплення іншим шляхом.\n",
      "\n",
      "Саморекламу можно було організувати за рахунок лайків іншим користувачам, що доволі просто було автоматизувати за допомогою цього проекту – https://github.com/instabot-py/instabot.py.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій IoT набір, який я пізніше використовував для Instagram\n",
      "Але ставити лайки рандомним користувачам було не дуже еффективно, тож я написав пошук в ширину і зібрав список підписників моїх підписників, що ймовірно могли про мене чути.\n",
      "\n",
      "У результаті мій скрипт пропрацював близько 2 місяців запущений на Raspberry Pi вдень та вночі, збільшивши кількість моїх підписніків до 1000. Потім мені стало нудно і я перестав займатися цією “дуже важливою” справою.\n",
      "\n",
      "Результати\n",
      "Я успішно завершив свій проект через 256 днів після його початку. Рівень моєї французької значно виріс. Крім того я дізнався та розповів про багато цікавих речей та подій. Нарешті мені вдалося розібратися з тим що так Instagram і навіщо він потрібен.\n",
      "\n",
      "Нажаль Instagram перетворився у місце маніфестації свого “успіху” та марнування власного часу для більшості користувачів. Але не думаю, що проблема у самому ресурсі – він просто дав людям те, чого вони хотіли.\n",
      "\n",
      "Мені дуже подобається освітній контент, що спрямовано на розвиток людини – звіти про івенти, подорожі, книги та фільми, лайфхаки. Загалом все, що несе у собі цінність для когось окрім самолюбства автора…\n",
      "\n",
      "\n",
      "---Source ends:---\n",
      "\n",
      "\n",
      "---Target starts:---\n",
      "\n",
      "Byte for France або “Мій досвід ведення блогу в Instagram”\n",
      "Останні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії, щоб продовжити серію записів щодо мого досвіду блогерства.\n",
      "\n",
      "Сьогодні розповім про те, як і навіщо мене занесло в Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням — https://www.instagram.com/yevhenii_kanivets/\n",
      "\n",
      "Моє бачення Instagram\n",
      "Колись давно я прочитав статтю чи просто коментарій – уже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на котрій можна буде лише ділитися світлинами та ставити лайки.\n",
      "\n",
      "Було це за декілька років до появи всім відомого сервісу. Як же авторові вдалося передбачити майбутнє? Дуже просто!\n",
      "\n",
      "Instagram втілює глибинні бажання кожної людини:\n",
      "\n",
      "емоційне збудження від перегляду гарних фото (читання — це менш природний процес, аніж споглядання);\n",
      "нескінченне поглинання нібито важливої інформації про інших людей (користувач відчуває себе у потоці);\n",
      "соціальне підтвердження того, що користувач виглядає та поводиться відповідно до тренду;\n",
      "Якщо чесно, то всі зазначені моменти мені не дуже близькі, тому до поїздки у Францію у мене в Instagram не було жодного фото. Але що сталося потім?\n",
      "\n",
      "Byte for France\n",
      "Французькою мовою до Франції я володів не дуже добре. Треба було якось виправляти ситуацію. Не знаю, як у інших, а у мене в житті траплялось не так багато див. Знаєте, так щоб прокинувся одного ранку і володієш мовою як місцевий.\n",
      "\n",
      "Все, що я зараз вмію, давалося мені доволі важко, через щоденну працю протягом багатьох років. Але як змусити себе займатися французькою кожен день? Особливо письмом?\n",
      "\n",
      "Мені завжди було цікаво читати статті про життя за кордоном. Але їм, на мій погляд, завжди не вистачало системності. Тож я вирішив започаткувати проєкт під назвою Byte For France — Instagram-блог, в якому б я чесно та системно (кожного дня) ділився враженнями про життя у Франції французькою та російською мовами.\n",
      "\n",
      "Перші кроки\n",
      "Свій перший пост я опублікував 3-го лютого 2018 року. Це було фото валіз, котрі ми зібрали з собою до Франції. Тоді на мене були підписані лише декілька моїх друзів, але початок було покладено.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій перший пост\n",
      "Я відразу ж вирішив, що описуватиму все як є, щоб не вийшло такого, знаєте, ідеального Instagram, що показує лише приємні моменти. Я прагнув максимально точно передати свій досвід.\n",
      "\n",
      "До речі, всі ці ідеальні Instagram зламали психіку далеко не одному підлітку (та не тільки). Досі не розумію, чому люди так бояться бути самими собою і створюють собі ці нудні образи “успішних людей”.\n",
      "\n",
      "Але повернімося до теми. Спочатку написання постів мені давалося дуже важко, адже я витрачав по декілька годин на їхній переклад кожного дня. На щастя, тем для записів було багато, залишалось тільки обирати, про що розповісти.\n",
      "\n",
      "До речі, поступово на мене почали підписуватися друзі та знайомі, а згодом і незнайомі люди… Як це сталося, розповім трохи далі.\n",
      "\n",
      "Прогрес та мотивація\n",
      "Якщо щось робити, то щось починає виходити. Так сталося і цього разу. Несподівано мені почала допомагати моя викладачка французької мови з університету — вона корегувала кожен мій запис. Помилок було дуже багато, але кожного разу їх було все менше і це не могло не радувати.\n",
      "\n",
      "Поступово мені дуже сподобалося ділитися цікавою інформацією, що могла стати у нагоді іншим людям. Мені здається, що я навіть почав жити цікавіше, щоб було про що розповісти.\n",
      "\n",
      "Ми відвідували багато музеїв, парків, подорожували іншими містами та країнами. Я навіть писав про свої співбесіди та роботу, власні думки.\n",
      "\n",
      "Звідки беруться підписники?\n",
      "Instagram створено таким чином, щоб отримати підписників органічним шляхом було максимально важко. У головній стрічці кожен користувач бачить лише тих, на кого вже підписаний. У пошуку користувач переглядає контент, не звертаючи особливої уваги на його автора.\n",
      "\n",
      "Таким чином кожен користувач має привести підписників з іншого джерела (реальних знайомих, наприклад) або ж придбати рекламу в Instagram, щоб влізти зі своїм контентом у стрічку незнайомців.\n",
      "\n",
      "Третій спосіб, на котрий витрачають час звичайні користувачі, – лайки та підписки за іншими користувачами у надії на те, що ті підпишуться у відповідь. Зазвичай кількість підписок / підписників у таких користувачів майже однакова.\n",
      "\n",
      "Є ще четвертий спосіб, але він уже для зовсім зневірених людей — придбання підписок та лайків. Є (чи принаймні були) цілі мережі фейкових акаунтів, що за невелику винагороду за вами підпишуться, пролайкають і навіть прокоментують ваші пости.\n",
      "\n",
      "Тільки задумайтесь, що люди готові витрачати власні кошти та брехати тисячам інших людей, лише заради того, щоб виглядати кращими, у очах тих же самих людей. Але проблема мабуть не в Instagram, чи не так?\n",
      "\n",
      "Розумне чи хитре рішення?\n",
      "Як і кожному Instagram-блогеру, мені здавалося, що мій контент має побачити більше людей. Тому після того як перша хвиля підписників закінчилася (знайомі та друзі із реального життя), я вирішив, що потрібно збільшити охоплення іншим шляхом.\n",
      "\n",
      "Саморекламу можна було організувати за рахунок лайків іншим користувачам, що доволі просто було автоматизувати за допомогою цього проекту — https://github.com/instabot-py/instabot.py.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій IoT набір, який я пізніше використовував для Instagram.\n",
      "Але ставити лайки рандомним користувачам було не дуже ефективно, тож я написав пошук у ширину і зібрав список підписників моїх підписників, що ймовірно могли про мене чути.\n",
      "\n",
      "У результаті мій скрипт, запущений на Raspberry Pi, пропрацював близько 2 місяців  удень та вночі, збільшивши кількість моїх підписніків до 1000. Потім мені стало нудно, і я перестав займатися цією “дуже важливою” справою.\n",
      "\n",
      "Результати\n",
      "Я успішно завершив свій проект через 256 днів після його початку. Рівень моєї французької значно виріс. Крім того, я дізнався і розповів про багато цікавих речей та подій. Нарешті мені вдалося розібратися з тим, що таке Instagram і навіщо він потрібен.\n",
      "\n",
      "На жаль, Instagram перетворився на місце маніфестації свого “успіху” та марнування власного часу для більшості користувачів. Але не думаю, що проблема у самому ресурсі — він просто дав людям те, чого вони хотіли.\n",
      "\n",
      "Мені дуже подобається освітній контент, який спрямовано на розвиток людини — звіти про івенти, подорожі, книги та фільми, лайфхаки. Загалом усе, що несе у собі цінність для когось, окрім самолюбства автора…\n",
      "\n",
      "\n",
      "---Target ends---\n",
      "\n",
      "\n",
      "---Annotation starts:---\n",
      "\n",
      "Byte for France або “Мій досвід ведення блогу {у=>в:::error_type=Spelling} Instagram”\n",
      "Останні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії{=>,:::error_type=Punctuation} щоб продовжити серію записів щодо мого досвіду блогерства.\n",
      "\n",
      "Сьогодні розповім про те{=>,:::error_type=Punctuation} як і навіщо мене занесло {у=>в:::error_type=Spelling} Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням {–=>—:::error_type=Punctuation} https://www.instagram.com/yevhenii_kanivets/\n",
      "\n",
      "Моє бачення Instagram\n",
      "Колись давно я прочитав статтю{,=>:::error_type=Punctuation} чи просто коментарій – {вже=>уже:::error_type=Spelling} не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа{=>,:::error_type=Punctuation} на котрій {можно=>можна:::error_type=Spelling} буде лише ділитися світлинами та ставити лайки.\n",
      "\n",
      "Було це за декілька років до появи {усім=>всім:::error_type=Spelling} відомого сервісу. Як же {автору=>авторові:::error_type=G/Case} вдалося передбачити майбутнє? Дуже просто!\n",
      "\n",
      "Instagram втілює глибинні бажання кожної людини:\n",
      "\n",
      "емоційне збудження від перегляду гарних фото (читання {–=>—:::error_type=Punctuation} це менш природний процес, {ніж=>аніж:::error_type=Spelling} споглядання){=>;:::error_type=Punctuation}\n",
      "нескінченне поглинання {ніби-то=>нібито:::error_type=Spelling} важливої інформації про інших людей ({користовуч=>користувач:::error_type=Spelling} відчуває себе у потоці){=>;:::error_type=Punctuation}\n",
      "соціальне підтвердження того, що користувач виглядає та поводиться відповідно до тренду{=>;:::error_type=Punctuation}\n",
      "Якщо {честно=>чесно:::error_type=Spelling}, то всі зазначені моменти мені не дуже близькі, тому до поїздки у Францію у мене в Instagram не було жодного фото. Але що сталося потім?\n",
      "\n",
      "Byte for France\n",
      "Французькою мовою до Франції я володів не дуже добре. Треба було якось виправляти ситуацію. Не знаю{=>,:::error_type=Punctuation} як у інших, а у мене в житті траплялось не так багато див. Знаєте, так щоб прокинувся одного ранку і володієш мовою як місцевий.\n",
      "\n",
      "Все, що я зараз вмію, давалося мені доволі важко, через щоденну працю протягом багатьох років. Але як змусити себе займатися французькою кожен день? Особливо письмом?\n",
      "\n",
      "Мені завжди було цікаво читати статті про життя {закордоном=>за кордоном:::error_type=Spelling}. Але {в них=>їм:::error_type=G/Case}, на мій погляд, завжди не вистачало системності. Тож я вирішив започаткувати {проект=>проєкт:::error_type=Spelling} під назвою Byte For France {–=>—:::error_type=Punctuation} Instagram-блог, в якому б я {честно=>чесно:::error_type=Spelling} та системно (кожного дня) ділився враженнями про життя у Франції французькою та російською мовами.\n",
      "\n",
      "Перші кроки\n",
      "Свій перший пост я опублікував 3-го лютого 2018 року. Це було фото валіз, котрі ми зібрали з собою до Франції. Тоді на мене були підписані лише декілька моїх друзів, але початок {будо=>було:::error_type=Spelling} покладено.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій перший пост\n",
      "Я відразу ж вирішив, що {буду =>:::error_type=Spelling}{буду писати=>описуватиму:::error_type=G/VerbAForm} все як є, щоб не вийшло такого, знаєте, ідеального Instagram, що показує лише приємні моменти. Я прагнув максимально точно передати свій досвід.\n",
      "\n",
      "До речі, всі ці ідеальні Instagram зламали психіку далеко не одному підлітку (та не тільки). Досі не розумію{=>,:::error_type=Punctuation} чому люди так бояться бути самими собою і створюють собі ці нудні образи “успішних людей”.\n",
      "\n",
      "Але {повернемося=>повернімося:::error_type=G/VerbVoice} до теми. Спочатку написання постів мені давалося дуже важко, адже я витрачав по декілька годин на {їх=>їхній:::error_type=G/Case} переклад кожного дня. На щастя{=>,:::error_type=Punctuation} тем для записів було багато, залишалось тільки обирати{=>,:::error_type=Punctuation} про що розповісти.\n",
      "\n",
      "До речі{=>,:::error_type=Punctuation} поступово на мене почали підписуватися друзі та знайомі, а згодом і {не знайомі=>незнайомі:::error_type=Spelling} люди… Як це сталося, розповім трохи далі.\n",
      "\n",
      "Прогрес та мотивація\n",
      "Якщо щось робити, то щось починає виходити. Так сталося і цього разу. Несподівано мені почала допомагати {мій викладач=>моя викладачка:::error_type=G/Gender} французької мови з університету {–=>—:::error_type=Punctuation} вона корегувала кожен мій запис. Помилок було дуже багато, але кожного разу їх було все менше і це не могло не радувати.\n",
      "\n",
      "Поступово мені дуже сподобалося ділитися цікавою інформацією, що могла стати у нагоді іншим людям. Мені здається, що я навіть почав жити цікавіше, щоб було про що розповісти.\n",
      "\n",
      "Ми відвідували багато музеїв, парків, подорожували {по іншим містам=>іншими містами:::error_type=G/Case} та {країнам=>країнами:::error_type=G/Case}. Я навіть писав про свої співбесіди та роботу, власні думки.\n",
      "\n",
      "Звідки беруться підписники?\n",
      "Instagram створено таким чином, щоб отримати підписників органічним шляхом було максимально важко. У головній стрічці кожен користувач бачить лише тих{=>,:::error_type=Punctuation} на кого вже підписаний. У пошуку користувач переглядає контент, не звертаючи особливої уваги на його автора.\n",
      "\n",
      "Таким чином кожен користувач має привести підписників з іншого джерела (реальних знайомих, наприклад) або ж придбати рекламу в Instagram, щоб влізти зі своїм контентом у стрічку незнайомців.\n",
      "\n",
      "Третій спосіб, на котрий витрачають час звичайні користувачі{=>,:::error_type=Punctuation} – лайки та підписки {на інших користувачів=>за іншими користувачами:::error_type=G/Case} у надії на те, що ті підпишуться у відповідь. Зазвичай кількість підписок / підписників у таких користувачів майже однакова.\n",
      "\n",
      "Є ще четвертий спосіб, але він {вже=>уже:::error_type=Spelling} для зовсім зневірених людей {–=>—:::error_type=Punctuation} придбання підписок та лайків. Є (чи принаймні були) цілі мережі фейкових {аккаунтів=>акаунтів:::error_type=Spelling}, що за невелику винагороду{=> за вами:::error_type=G/UngrammaticalStructure} підпишуться, пролайкають {та=>і:::error_type=Spelling} навіть прокоментують ваші пости.\n",
      "\n",
      "Тільки задумайтесь, що люди готові витрачати власні кошти та брехати тисячам інших людей, {лиша=>лише:::error_type=Spelling} заради того, щоб виглядати {краще=>кращими:::error_type=G/Case}{=>,:::error_type=Punctuation} у очах тих же самих людей. Але проблема мабуть не в Instagram, чи не так?\n",
      "\n",
      "Розумне чи хитре рішення?\n",
      "Як і кожному Instagram-блогеру, мені здавалося, що мій контент має побачити більше людей. Тому після того як перша хвиля підписників закінчилася (знайомі та друзі із реального життя), я вирішив, що потрібно збільшити охоплення іншим шляхом.\n",
      "\n",
      "Саморекламу {можно=>можна:::error_type=Spelling} було організувати за рахунок лайків іншим користувачам, що доволі просто було автоматизувати за допомогою цього проекту {–=>—:::error_type=Punctuation} https://github.com/instabot-py/instabot.py.\n",
      "\n",
      "Yevhenii Kanivets's Blog\n",
      "Мій IoT набір, який я пізніше використовував для Instagram{=>.:::error_type=Punctuation}\n",
      "Але ставити лайки рандомним користувачам було не дуже {еффективно=>ефективно:::error_type=Spelling}, тож я написав пошук {в=>у:::error_type=Spelling} ширину і зібрав список підписників моїх підписників, що ймовірно могли про мене чути.\n",
      "\n",
      "У результаті мій скрипт{=>, запущений на Raspberry Pi,:::error_type=G/UngrammaticalStructure} пропрацював близько 2 місяців {запущений на Raspberry Pi=>:::error_type=G/UngrammaticalStructure} {вдень=>удень:::error_type=Spelling} та вночі, збільшивши кількість моїх підписніків до 1000. Потім мені стало нудно{=>,:::error_type=Punctuation} і я перестав займатися цією “дуже важливою” справою.\n",
      "\n",
      "Результати\n",
      "Я успішно завершив свій проект через 256 днів після його початку. Рівень моєї французької значно виріс. Крім того{=>,:::error_type=Punctuation} я дізнався {та=>і:::error_type=Spelling} розповів про багато цікавих речей та подій. Нарешті мені вдалося розібратися з тим{=>,:::error_type=Punctuation} що {так=>таке:::error_type=Spelling} Instagram і навіщо він потрібен.\n",
      "\n",
      "{Нажаль=>На жаль:::error_type=Spelling}{=>,:::error_type=Punctuation} Instagram перетворився {у=>на:::error_type=G/Prep} місце маніфестації свого “успіху” та марнування власного часу для більшості користувачів. Але не думаю, що проблема у самому ресурсі {–=>—:::error_type=Punctuation} він просто дав людям те, чого вони хотіли.\n",
      "\n",
      "Мені дуже подобається освітній контент, {що=>який:::error_type=G/Conjunction} спрямовано на розвиток людини {–=>—:::error_type=Punctuation} звіти про івенти, подорожі, книги та фільми, лайфхаки. Загалом {все=>усе:::error_type=Spelling}, що несе у собі цінність для когось{=>,:::error_type=Punctuation} окрім самолюбства автора…\n",
      "\n",
      "Харківська\n",
      "\n",
      "---Annotation ends\n",
      "\n",
      "---Prompt starts\n",
      "text='Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.\\n\\nБуло це за декілька років до появи усім відомого сервісу. Як же автору вдалося передбачити майбутнє? Дуже просто!\\n\\nInstagram втілює глибинні бажання кожної людини:\\n\\nемоційне збудження від перегляду гарних фото (читання – це менш природний процес, ніж споглядання)\\nнескінченне поглинання ніби-то важливої інформації про інших людей (користовуч відчуває себе у потоці)\\nсоціальне підтвердження того, що користувач виглядає та поводиться відповідно до тренду\\nЯкщо честно, то всі зазначені моменти мені не дуже близькі, тому до поїздки у Францію у мене в Instagram не було жодного фото. Але що сталося потім?\\n\\nByte for France\\nФранцузькою мовою до Франції я володів не дуже добре. Треба було якось виправляти ситуацію. Не знаю як у інших, а у мене в житті траплялось не так багато див. Знаєте, так щоб прокинувся одного ранку і володієш мовою як місцевий.\\n\\nВсе, що я зараз вмію, давалося мені доволі важко, через щоденну працю протягом багатьох років. Але як змусити себе займатися французькою кожен день? Особливо письмом?\\n\\nМені завжди було цікаво читати статті про життя закордоном. Але в них, на мій погляд, завжди не вистачало системності. Тож я вирішив започаткувати проект під назвою Byte For France – Instagram-блог, в якому б я честно та системно (кожного дня) ділився враженнями про життя у Франції французькою та російською мовами.\\n\\nПерші кроки\\nСвій перший пост я опублікував 3-го лютого 2018 року. Це було фото валіз, котрі ми зібрали з собою до Франції. Тоді на мене були підписані лише декілька моїх друзів, але початок будо покладено.\\n\\nYevhenii Kanivets\\'s Blog\\nМій перший пост\\nЯ відразу ж вирішив, що буду буду писати все як є, щоб не вийшло такого, знаєте, ідеального Instagram, що показує лише приємні моменти. Я прагнув максимально точно передати свій досвід.\\n\\nДо речі, всі ці ідеальні Instagram зламали психіку далеко не одному підлітку (та не тільки). Досі не розумію чому люди так бояться бути самими собою і створюють собі ці нудні образи “успішних людей”.\\n\\nАле повернемося до теми. Спочатку написання постів мені давалося дуже важко, адже я витрачав по декілька годин на їх переклад кожного дня. На щастя тем для записів було багато, залишалось тільки обирати про що розповісти.\\n\\nДо речі поступово на мене почали підписуватися друзі та знайомі, а згодом і не знайомі люди… Як це сталося, розповім трохи далі.\\n\\nПрогрес та мотивація\\nЯкщо щось робити, то щось починає виходити. Так сталося і цього разу. Несподівано мені почала допомагати мій викладач французької мови з університету – вона корегувала кожен мій запис. Помилок було дуже багато, але кожного разу їх було все менше і це не могло не радувати.\\n\\nПоступово мені дуже сподобалося ділитися цікавою інформацією, що могла стати у нагоді іншим людям. Мені здається, що я навіть почав жити цікавіше, щоб було про що розповісти.\\n\\nМи відвідували багато музеїв, парків, подорожували по іншим містам та країнам. Я навіть писав про свої співбесіди та роботу, власні думки.\\n\\nЗвідки беруться підписники?\\nInstagram створено таким чином, щоб отримати підписників органічним шляхом було максимально важко. У головній стрічці кожен користувач бачить лише тих на кого вже підписаний. У пошуку користувач переглядає контент, не звертаючи особливої уваги на його автора.\\n\\nТаким чином кожен користувач має привести підписників з іншого джерела (реальних знайомих, наприклад) або ж придбати рекламу в Instagram, щоб влізти зі своїм контентом у стрічку незнайомців.\\n\\nТретій спосіб, на котрий витрачають час звичайні користувачі – лайки та підписки на інших користувачів у надії на те, що ті підпишуться у відповідь. Зазвичай кількість підписок / підписників у таких користувачів майже однакова.\\n\\nЄ ще четвертий спосіб, але він вже для зовсім зневірених людей – придбання підписок та лайків. Є (чи принаймні були) цілі мережі фейкових аккаунтів, що за невелику винагороду підпишуться, пролайкають та навіть прокоментують ваші пости.\\n\\nТільки задумайтесь, що люди готові витрачати власні кошти та брехати тисячам інших людей, лиша заради того, щоб виглядати краще у очах тих же самих людей. Але проблема мабуть не в Instagram, чи не так?\\n\\nРозумне чи хитре рішення?\\nЯк і кожному Instagram-блогеру, мені здавалося, що мій контент має побачити більше людей. Тому після того як перша хвиля підписників закінчилася (знайомі та друзі із реального життя), я вирішив, що потрібно збільшити охоплення іншим шляхом.\\n\\nСаморекламу можно було організувати за рахунок лайків іншим користувачам, що доволі просто було автоматизувати за допомогою цього проекту – https://github.com/instabot-py/instabot.py.\\n\\nYevhenii Kanivets\\'s Blog\\nМій IoT набір, який я пізніше використовував для Instagram\\nАле ставити лайки рандомним користувачам було не дуже еффективно, тож я написав пошук в ширину і зібрав список підписників моїх підписників, що ймовірно могли про мене чути.\\n\\nУ результаті мій скрипт пропрацював близько 2 місяців запущений на Raspberry Pi вдень та вночі, збільшивши кількість моїх підписніків до 1000. Потім мені стало нудно і я перестав займатися цією “дуже важливою” справою.\\n\\nРезультати\\nЯ успішно завершив свій проект через 256 днів після його початку. Рівень моєї французької значно виріс. Крім того я дізнався та розповів про багато цікавих речей та подій. Нарешті мені вдалося розібратися з тим що так Instagram і навіщо він потрібен.\\n\\nНажаль Instagram перетворився у місце маніфестації свого “успіху” та марнування власного часу для більшості користувачів. Але не думаю, що проблема у самому ресурсі – він просто дав людям те, чого вони хотіли.\\n\\nМені дуже подобається освітній контент, що спрямовано на розвиток людини – звіти про івенти, подорожі, книги та фільми, лайфхаки. Загалом все, що несе у собі цінність для когось окрім самолюбства автора…\\n\\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'\n"
     ]
    }
   ],
   "source": [
    "from src.packages.constants.error_constants import ErrorConstants\n",
    "from src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\n",
    "from ua_gec import Corpus\n",
    "corpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\n",
    "for doc in corpus:\n",
    "    print(\"\\n---Source starts:---\\n\")\n",
    "    print(doc.source)         # \"I likes it.\"\n",
    "    print(\"\\n---Source ends:---\\n\")\n",
    "    print(\"\\n---Target starts:---\\n\")\n",
    "    print(doc.target)         # \"I like it.\"\n",
    "    print(\"\\n---Target ends---\\n\")\n",
    "    print(\"\\n---Annotation starts:---\\n\")\n",
    "    print(doc.annotated)      # <AnnotatedText(\"I {likes=>like} it.\")\n",
    "    print(doc.meta.region)    # \"Київська\"\n",
    "    print(\"\\n---Annotation ends\")\n",
    "    print(\"\\n---Prompt starts\")\n",
    "    prompt = InstructionTuningGecPrompts.PROMPT.format_prompt(\n",
    "        query=doc.source,\n",
    "        error_types=ErrorConstants.ERROR_TYPES\n",
    "    )\n",
    "    print(prompt)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:22:59.708257Z",
     "start_time": "2024-03-01T20:22:59.634903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation(\"Привіт, чи розмовляєш ти українською?\")\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:30:52.839844Z",
     "start_time": "2024-03-01T20:28:00.194283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.\\n\\nБуло це за декілька років до появи усім відомого сервісу. Як же автору вдалося передбачити майбутнє? Дуже просто!\\n\\nInstagram втілює глибинні бажання кожної людини:\\n\\nемоційне збудження від перегляду гарних фото (читання – це менш природний процес, ніж споглядання)\\nнескінченне поглинання ніби-то важливої інформації про інших людей (користовуч відчуває себе у потоці)\\nсоціальне підтвердження того, що користувач виглядає та поводиться відповідно до тренду\\nЯкщо честно, то всі зазначені моменти мені не дуже близькі, тому до поїздки у Францію у мене в Instagram не було жодного фото. Але що сталося потім?\\n\\nByte for France\\nФранцузькою мовою до Франції я володів не дуже добре. Треба було якось виправляти ситуацію. Не знаю як у інших, а у мене в житті траплялось не так багато див. Знаєте, так щоб прокинувся одного ранку і володієш мовою як місцевий.\\n\\nВсе, що я зараз вмію, давалося мені доволі важко, через щоденну працю протягом багатьох років. Але як змусити себе займатися французькою кожен день? Особливо письмом?\\n\\nМені завжди було цікаво читати статті про життя закордоном. Але в них, на мій погляд, завжди не вистачало системності. Тож я вирішив започаткувати проект під назвою Byte For France – Instagram-блог, в якому б я честно та системно (кожного дня) ділився враженнями про життя у Франції французькою та російською мовами.\\n\\nПерші кроки\\nСвій перший пост я опублікував 3-го лютого 2018 року. Це було фото валіз, котрі ми зібрали з собою до Франції. Тоді на мене були підписані лише декілька моїх друзів, але початок будо покладено.\\n\\nYevhenii Kanivets\\'s Blog\\nМій перший пост\\nЯ відразу ж вирішив, що буду буду писати все як є, щоб не вийшло такого, знаєте, ідеального Instagram, що показує лише приємні моменти. Я прагнув максимально точно передати свій досвід.\\n\\nДо речі, всі ці ідеальні Instagram зламали психіку далеко не одному підлітку (та не тільки). Досі не розумію чому люди так бояться бути самими собою і створюють собі ці нудні образи “успішних людей”.\\n\\nАле повернемося до теми. Спочатку написання постів мені давалося дуже важко, адже я витрачав по декілька годин на їх переклад кожного дня. На щастя тем для записів було багато, залишалось тільки обирати про що розповісти.\\n\\nДо речі поступово на мене почали підписуватися друзі та знайомі, а згодом і не знайомі люди… Як це сталося, розповім трохи далі.\\n\\nПрогрес та мотивація\\nЯкщо щось робити, то щось починає виходити. Так сталося і цього разу. Несподівано мені почала допомагати мій викладач французької мови з університету – вона корегувала кожен мій запис. Помилок було дуже багато, але кожного разу їх було все менше і це не могло не радувати.\\n\\nПоступово мені дуже сподобалося ділитися цікавою інформацією, що могла стати у нагоді іншим людям. Мені здається, що я навіть почав жити цікавіше, щоб було про що розповісти.\\n\\nМи відвідували багато музеїв, парків, подорожували по іншим містам та країнам. Я навіть писав про свої співбесіди та роботу, власні думки.\\n\\nЗвідки беруться підписники?\\nInstagram створено таким чином, щоб отримати підписників органічним шляхом було максимально важко. У головній стрічці кожен користувач бачить лише тих на кого вже підписаний. У пошуку користувач переглядає контент, не звертаючи особливої уваги на його автора.\\n\\nТаким чином кожен користувач має привести підписників з іншого джерела (реальних знайомих, наприклад) або ж придбати рекламу в Instagram, щоб влізти зі своїм контентом у стрічку незнайомців.\\n\\nТретій спосіб, на котрий витрачають час звичайні користувачі – лайки та підписки на інших користувачів у надії на те, що ті підпишуться у відповідь. Зазвичай кількість підписок / підписників у таких користувачів майже однакова.\\n\\nЄ ще четвертий спосіб, але він вже для зовсім зневірених людей – придбання підписок та лайків. Є (чи принаймні були) цілі мережі фейкових аккаунтів, що за невелику винагороду підпишуться, пролайкають та навіть прокоментують ваші пости.\\n\\nТільки задумайтесь, що люди готові витрачати власні кошти та брехати тисячам інших людей, лиша заради того, щоб виглядати краще у очах тих же самих людей. Але проблема мабуть не в Instagram, чи не так?\\n\\nРозумне чи хитре рішення?\\nЯк і кожному Instagram-блогеру, мені здавалося, що мій контент має побачити більше людей. Тому після того як перша хвиля підписників закінчилася (знайомі та друзі із реального життя), я вирішив, що потрібно збільшити охоплення іншим шляхом.\\n\\nСаморекламу можно було організувати за рахунок лайків іншим користувачам, що доволі просто було автоматизувати за допомогою цього проекту – https://github.com/instabot-py/instabot.py.\\n\\nYevhenii Kanivets\\'s Blog\\nМій IoT набір, який я пізніше використовував для Instagram\\nАле ставити лайки рандомним користувачам було не дуже еффективно, тож я написав пошук в ширину і зібрав список підписників моїх підписників, що ймовірно могли про мене чути.\\n\\nУ результаті мій скрипт пропрацював близько 2 місяців запущений на Raspberry Pi вдень та вночі, збільшивши кількість моїх підписніків до 1000. Потім мені стало нудно і я перестав займатися цією “дуже важливою” справою.\\n\\nРезультати\\nЯ успішно завершив свій проект через 256 днів після його початку. Рівень моєї французької значно виріс. Крім того я дізнався та розповів про багато цікавих речей та подій. Нарешті мені вдалося розібратися з тим що так Instagram і навіщо він потрібен.\\n\\nНажаль Instagram перетворився у місце маніфестації свого “успіху” та марнування власного часу для більшості користувачів. Але не думаю, що проблема у самому ресурсі – він просто дав людям те, чого вони хотіли.\\n\\nМені дуже подобається освітній контент, що спрямовано на розвиток людини – звіти про івенти, подорожі, книги та фільми, лайфхаки. Загалом все, що несе у собі цінність для когось окрім самолюбства автора…\\n\\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:27:29.517626Z",
     "start_time": "2024-03-01T20:27:29.486153Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ua_gec import AnnotationLayer\n",
    "\n",
    "AnnotationLayer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MistralForCausalLM' object has no attribute 'print_trainable_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/peft/tuners/lora/model.py:273\u001B[0m, in \u001B[0;36mLoraModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattr__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LoraModel' object has no attribute 'print_trainable_parameters'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m config \u001B[38;5;241m=\u001B[39m LoraConfig(\n\u001B[1;32m      5\u001B[0m     task_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSEQ_2_SEQ_LM\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     lora_dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m lora_model \u001B[38;5;241m=\u001B[39m LoraModel(model, config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdefault\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mlora_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprint_trainable_parameters\u001B[49m()\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/peft/tuners/lora/model.py:275\u001B[0m, in \u001B[0;36mLoraModel.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattr__\u001B[39m(name)  \u001B[38;5;66;03m# defer to nn.Module's logic\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'MistralForCausalLM' object has no attribute 'print_trainable_parameters'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraModel, LoraConfig\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "\n",
    "lora_model = LoraModel(model, config, \"default\")\n",
    "lora_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T16:18:10.721448Z",
     "start_time": "2024-03-01T16:18:10.423551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T13:04:15.775935Z",
     "start_time": "2024-03-01T13:04:15.773282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a classic one for you: Why don't scientists trust atoms? Because they make up everything, and sometimes they just seem unpredictable!\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation(\"Tell me a joke\")\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T13:15:47.136845Z",
     "start_time": "2024-03-01T13:15:36.524085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-01T08:48:52.545665Z",
     "start_time": "2024-03-01T08:47:41.378513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device=mps\n",
      "Loading model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load model: 71.10 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "Transformer(\n  (token_embd): Embedding()\n  (blk): ModuleList(\n    (0-31): 32 x TransformerBlock(\n      (attn_norm): RMSNorm()\n      (attn_q): Linear()\n      (attn_k): Linear()\n      (attn_v): Linear()\n      (attn_output): Linear()\n      (ffn_norm): RMSNorm()\n      (ffn_gate): Linear()\n      (ffn_up): Linear()\n      (ffn_down): Linear()\n    )\n  )\n  (output_norm): RMSNorm()\n  (output): Linear()\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "from src.packages.training.load_from_bin import load_from_bin\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "model = load_from_bin(\n",
    "    checkpoint_path=Path(\"/Users/rkovalch/Documents/UCU/llama-cpp-torch/jaskier-f16\"),\n",
    "    device=parameter_server.settings.device\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed locating file constants.pkl: file not found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/rkovalch/Documents/UCU/llama-cpp-torch/jaskier-f16/pytorch_model.bin\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/jit/_serialization.py:162\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, _extra_files, _restore_shapes)\u001B[0m\n\u001B[1;32m    160\u001B[0m cu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mCompilationUnit()\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, (\u001B[38;5;28mstr\u001B[39m, pathlib\u001B[38;5;241m.\u001B[39mPath)):\n\u001B[0;32m--> 162\u001B[0m     cpp_module \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_ir_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_extra_files\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_restore_shapes\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    164\u001B[0m     cpp_module \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mimport_ir_module_from_buffer(\n\u001B[1;32m    165\u001B[0m         cu, f\u001B[38;5;241m.\u001B[39mread(), map_location, _extra_files, _restore_shapes\n\u001B[1;32m    166\u001B[0m     )  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: PytorchStreamReader failed locating file constants.pkl: file not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(Path(\"/Users/rkovalch/Documents/UCU/llama-cpp-torch/jaskier-f16/pytorch_model.bin\"))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T09:07:00.836669Z",
     "start_time": "2024-03-01T09:07:00.793333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['token_embd.weight', 'token_embd.weight_type', 'blk.0.attn_norm.weight', 'blk.0.attn_norm.weight_type', 'blk.0.ffn_down.weight', 'blk.0.ffn_down.weight_type', 'blk.0.ffn_gate.weight', 'blk.0.ffn_gate.weight_type', 'blk.0.ffn_up.weight', 'blk.0.ffn_up.weight_type', 'blk.0.ffn_norm.weight', 'blk.0.ffn_norm.weight_type', 'blk.0.attn_k.weight', 'blk.0.attn_k.weight_type', 'blk.0.attn_output.weight', 'blk.0.attn_output.weight_type', 'blk.0.attn_q.weight', 'blk.0.attn_q.weight_type', 'blk.0.attn_v.weight', 'blk.0.attn_v.weight_type', 'blk.1.attn_norm.weight', 'blk.1.attn_norm.weight_type', 'blk.1.ffn_down.weight', 'blk.1.ffn_down.weight_type', 'blk.1.ffn_gate.weight', 'blk.1.ffn_gate.weight_type', 'blk.1.ffn_up.weight', 'blk.1.ffn_up.weight_type', 'blk.1.ffn_norm.weight', 'blk.1.ffn_norm.weight_type', 'blk.1.attn_k.weight', 'blk.1.attn_k.weight_type', 'blk.1.attn_output.weight', 'blk.1.attn_output.weight_type', 'blk.1.attn_q.weight', 'blk.1.attn_q.weight_type', 'blk.1.attn_v.weight', 'blk.1.attn_v.weight_type', 'blk.10.ffn_gate.weight', 'blk.10.ffn_gate.weight_type', 'blk.10.ffn_up.weight', 'blk.10.ffn_up.weight_type', 'blk.10.attn_k.weight', 'blk.10.attn_k.weight_type', 'blk.10.attn_output.weight', 'blk.10.attn_output.weight_type', 'blk.10.attn_q.weight', 'blk.10.attn_q.weight_type', 'blk.10.attn_v.weight', 'blk.10.attn_v.weight_type', 'blk.2.attn_norm.weight', 'blk.2.attn_norm.weight_type', 'blk.2.ffn_down.weight', 'blk.2.ffn_down.weight_type', 'blk.2.ffn_gate.weight', 'blk.2.ffn_gate.weight_type', 'blk.2.ffn_up.weight', 'blk.2.ffn_up.weight_type', 'blk.2.ffn_norm.weight', 'blk.2.ffn_norm.weight_type', 'blk.2.attn_k.weight', 'blk.2.attn_k.weight_type', 'blk.2.attn_output.weight', 'blk.2.attn_output.weight_type', 'blk.2.attn_q.weight', 'blk.2.attn_q.weight_type', 'blk.2.attn_v.weight', 'blk.2.attn_v.weight_type', 'blk.3.attn_norm.weight', 'blk.3.attn_norm.weight_type', 'blk.3.ffn_down.weight', 'blk.3.ffn_down.weight_type', 'blk.3.ffn_gate.weight', 'blk.3.ffn_gate.weight_type', 'blk.3.ffn_up.weight', 'blk.3.ffn_up.weight_type', 'blk.3.ffn_norm.weight', 'blk.3.ffn_norm.weight_type', 'blk.3.attn_k.weight', 'blk.3.attn_k.weight_type', 'blk.3.attn_output.weight', 'blk.3.attn_output.weight_type', 'blk.3.attn_q.weight', 'blk.3.attn_q.weight_type', 'blk.3.attn_v.weight', 'blk.3.attn_v.weight_type', 'blk.4.attn_norm.weight', 'blk.4.attn_norm.weight_type', 'blk.4.ffn_down.weight', 'blk.4.ffn_down.weight_type', 'blk.4.ffn_gate.weight', 'blk.4.ffn_gate.weight_type', 'blk.4.ffn_up.weight', 'blk.4.ffn_up.weight_type', 'blk.4.ffn_norm.weight', 'blk.4.ffn_norm.weight_type', 'blk.4.attn_k.weight', 'blk.4.attn_k.weight_type', 'blk.4.attn_output.weight', 'blk.4.attn_output.weight_type', 'blk.4.attn_q.weight', 'blk.4.attn_q.weight_type', 'blk.4.attn_v.weight', 'blk.4.attn_v.weight_type', 'blk.5.attn_norm.weight', 'blk.5.attn_norm.weight_type', 'blk.5.ffn_down.weight', 'blk.5.ffn_down.weight_type', 'blk.5.ffn_gate.weight', 'blk.5.ffn_gate.weight_type', 'blk.5.ffn_up.weight', 'blk.5.ffn_up.weight_type', 'blk.5.ffn_norm.weight', 'blk.5.ffn_norm.weight_type', 'blk.5.attn_k.weight', 'blk.5.attn_k.weight_type', 'blk.5.attn_output.weight', 'blk.5.attn_output.weight_type', 'blk.5.attn_q.weight', 'blk.5.attn_q.weight_type', 'blk.5.attn_v.weight', 'blk.5.attn_v.weight_type', 'blk.6.attn_norm.weight', 'blk.6.attn_norm.weight_type', 'blk.6.ffn_down.weight', 'blk.6.ffn_down.weight_type', 'blk.6.ffn_gate.weight', 'blk.6.ffn_gate.weight_type', 'blk.6.ffn_up.weight', 'blk.6.ffn_up.weight_type', 'blk.6.ffn_norm.weight', 'blk.6.ffn_norm.weight_type', 'blk.6.attn_k.weight', 'blk.6.attn_k.weight_type', 'blk.6.attn_output.weight', 'blk.6.attn_output.weight_type', 'blk.6.attn_q.weight', 'blk.6.attn_q.weight_type', 'blk.6.attn_v.weight', 'blk.6.attn_v.weight_type', 'blk.7.attn_norm.weight', 'blk.7.attn_norm.weight_type', 'blk.7.ffn_down.weight', 'blk.7.ffn_down.weight_type', 'blk.7.ffn_gate.weight', 'blk.7.ffn_gate.weight_type', 'blk.7.ffn_up.weight', 'blk.7.ffn_up.weight_type', 'blk.7.ffn_norm.weight', 'blk.7.ffn_norm.weight_type', 'blk.7.attn_k.weight', 'blk.7.attn_k.weight_type', 'blk.7.attn_output.weight', 'blk.7.attn_output.weight_type', 'blk.7.attn_q.weight', 'blk.7.attn_q.weight_type', 'blk.7.attn_v.weight', 'blk.7.attn_v.weight_type', 'blk.8.attn_norm.weight', 'blk.8.attn_norm.weight_type', 'blk.8.ffn_down.weight', 'blk.8.ffn_down.weight_type', 'blk.8.ffn_gate.weight', 'blk.8.ffn_gate.weight_type', 'blk.8.ffn_up.weight', 'blk.8.ffn_up.weight_type', 'blk.8.ffn_norm.weight', 'blk.8.ffn_norm.weight_type', 'blk.8.attn_k.weight', 'blk.8.attn_k.weight_type', 'blk.8.attn_output.weight', 'blk.8.attn_output.weight_type', 'blk.8.attn_q.weight', 'blk.8.attn_q.weight_type', 'blk.8.attn_v.weight', 'blk.8.attn_v.weight_type', 'blk.9.attn_norm.weight', 'blk.9.attn_norm.weight_type', 'blk.9.ffn_down.weight', 'blk.9.ffn_down.weight_type', 'blk.9.ffn_gate.weight', 'blk.9.ffn_gate.weight_type', 'blk.9.ffn_up.weight', 'blk.9.ffn_up.weight_type', 'blk.9.ffn_norm.weight', 'blk.9.ffn_norm.weight_type', 'blk.9.attn_k.weight', 'blk.9.attn_k.weight_type', 'blk.9.attn_output.weight', 'blk.9.attn_output.weight_type', 'blk.9.attn_q.weight', 'blk.9.attn_q.weight_type', 'blk.9.attn_v.weight', 'blk.9.attn_v.weight_type', 'blk.10.attn_norm.weight', 'blk.10.attn_norm.weight_type', 'blk.10.ffn_down.weight', 'blk.10.ffn_down.weight_type', 'blk.10.ffn_norm.weight', 'blk.10.ffn_norm.weight_type', 'blk.11.attn_norm.weight', 'blk.11.attn_norm.weight_type', 'blk.11.ffn_down.weight', 'blk.11.ffn_down.weight_type', 'blk.11.ffn_gate.weight', 'blk.11.ffn_gate.weight_type', 'blk.11.ffn_up.weight', 'blk.11.ffn_up.weight_type', 'blk.11.ffn_norm.weight', 'blk.11.ffn_norm.weight_type', 'blk.11.attn_k.weight', 'blk.11.attn_k.weight_type', 'blk.11.attn_output.weight', 'blk.11.attn_output.weight_type', 'blk.11.attn_q.weight', 'blk.11.attn_q.weight_type', 'blk.11.attn_v.weight', 'blk.11.attn_v.weight_type', 'blk.12.attn_norm.weight', 'blk.12.attn_norm.weight_type', 'blk.12.ffn_down.weight', 'blk.12.ffn_down.weight_type', 'blk.12.ffn_gate.weight', 'blk.12.ffn_gate.weight_type', 'blk.12.ffn_up.weight', 'blk.12.ffn_up.weight_type', 'blk.12.ffn_norm.weight', 'blk.12.ffn_norm.weight_type', 'blk.12.attn_k.weight', 'blk.12.attn_k.weight_type', 'blk.12.attn_output.weight', 'blk.12.attn_output.weight_type', 'blk.12.attn_q.weight', 'blk.12.attn_q.weight_type', 'blk.12.attn_v.weight', 'blk.12.attn_v.weight_type', 'blk.13.attn_norm.weight', 'blk.13.attn_norm.weight_type', 'blk.13.ffn_down.weight', 'blk.13.ffn_down.weight_type', 'blk.13.ffn_gate.weight', 'blk.13.ffn_gate.weight_type', 'blk.13.ffn_up.weight', 'blk.13.ffn_up.weight_type', 'blk.13.ffn_norm.weight', 'blk.13.ffn_norm.weight_type', 'blk.13.attn_k.weight', 'blk.13.attn_k.weight_type', 'blk.13.attn_output.weight', 'blk.13.attn_output.weight_type', 'blk.13.attn_q.weight', 'blk.13.attn_q.weight_type', 'blk.13.attn_v.weight', 'blk.13.attn_v.weight_type', 'blk.14.attn_norm.weight', 'blk.14.attn_norm.weight_type', 'blk.14.ffn_down.weight', 'blk.14.ffn_down.weight_type', 'blk.14.ffn_gate.weight', 'blk.14.ffn_gate.weight_type', 'blk.14.ffn_up.weight', 'blk.14.ffn_up.weight_type', 'blk.14.ffn_norm.weight', 'blk.14.ffn_norm.weight_type', 'blk.14.attn_k.weight', 'blk.14.attn_k.weight_type', 'blk.14.attn_output.weight', 'blk.14.attn_output.weight_type', 'blk.14.attn_q.weight', 'blk.14.attn_q.weight_type', 'blk.14.attn_v.weight', 'blk.14.attn_v.weight_type', 'blk.15.attn_norm.weight', 'blk.15.attn_norm.weight_type', 'blk.15.ffn_down.weight', 'blk.15.ffn_down.weight_type', 'blk.15.ffn_gate.weight', 'blk.15.ffn_gate.weight_type', 'blk.15.ffn_up.weight', 'blk.15.ffn_up.weight_type', 'blk.15.ffn_norm.weight', 'blk.15.ffn_norm.weight_type', 'blk.15.attn_k.weight', 'blk.15.attn_k.weight_type', 'blk.15.attn_output.weight', 'blk.15.attn_output.weight_type', 'blk.15.attn_q.weight', 'blk.15.attn_q.weight_type', 'blk.15.attn_v.weight', 'blk.15.attn_v.weight_type', 'blk.16.attn_norm.weight', 'blk.16.attn_norm.weight_type', 'blk.16.ffn_down.weight', 'blk.16.ffn_down.weight_type', 'blk.16.ffn_gate.weight', 'blk.16.ffn_gate.weight_type', 'blk.16.ffn_up.weight', 'blk.16.ffn_up.weight_type', 'blk.16.ffn_norm.weight', 'blk.16.ffn_norm.weight_type', 'blk.16.attn_k.weight', 'blk.16.attn_k.weight_type', 'blk.16.attn_output.weight', 'blk.16.attn_output.weight_type', 'blk.16.attn_q.weight', 'blk.16.attn_q.weight_type', 'blk.16.attn_v.weight', 'blk.16.attn_v.weight_type', 'blk.17.attn_norm.weight', 'blk.17.attn_norm.weight_type', 'blk.17.ffn_down.weight', 'blk.17.ffn_down.weight_type', 'blk.17.ffn_gate.weight', 'blk.17.ffn_gate.weight_type', 'blk.17.ffn_up.weight', 'blk.17.ffn_up.weight_type', 'blk.17.ffn_norm.weight', 'blk.17.ffn_norm.weight_type', 'blk.17.attn_k.weight', 'blk.17.attn_k.weight_type', 'blk.17.attn_output.weight', 'blk.17.attn_output.weight_type', 'blk.17.attn_q.weight', 'blk.17.attn_q.weight_type', 'blk.17.attn_v.weight', 'blk.17.attn_v.weight_type', 'blk.18.attn_norm.weight', 'blk.18.attn_norm.weight_type', 'blk.18.ffn_down.weight', 'blk.18.ffn_down.weight_type', 'blk.18.ffn_gate.weight', 'blk.18.ffn_gate.weight_type', 'blk.18.ffn_up.weight', 'blk.18.ffn_up.weight_type', 'blk.18.ffn_norm.weight', 'blk.18.ffn_norm.weight_type', 'blk.18.attn_k.weight', 'blk.18.attn_k.weight_type', 'blk.18.attn_output.weight', 'blk.18.attn_output.weight_type', 'blk.18.attn_q.weight', 'blk.18.attn_q.weight_type', 'blk.18.attn_v.weight', 'blk.18.attn_v.weight_type', 'blk.19.attn_norm.weight', 'blk.19.attn_norm.weight_type', 'blk.19.ffn_down.weight', 'blk.19.ffn_down.weight_type', 'blk.19.ffn_gate.weight', 'blk.19.ffn_gate.weight_type', 'blk.19.ffn_up.weight', 'blk.19.ffn_up.weight_type', 'blk.19.ffn_norm.weight', 'blk.19.ffn_norm.weight_type', 'blk.19.attn_k.weight', 'blk.19.attn_k.weight_type', 'blk.19.attn_output.weight', 'blk.19.attn_output.weight_type', 'blk.19.attn_q.weight', 'blk.19.attn_q.weight_type', 'blk.19.attn_v.weight', 'blk.19.attn_v.weight_type', 'blk.20.attn_norm.weight', 'blk.20.attn_norm.weight_type', 'blk.20.ffn_down.weight', 'blk.20.ffn_down.weight_type', 'blk.20.ffn_gate.weight', 'blk.20.ffn_gate.weight_type', 'blk.20.ffn_up.weight', 'blk.20.ffn_up.weight_type', 'blk.20.ffn_norm.weight', 'blk.20.ffn_norm.weight_type', 'blk.20.attn_k.weight', 'blk.20.attn_k.weight_type', 'blk.20.attn_output.weight', 'blk.20.attn_output.weight_type', 'blk.20.attn_q.weight', 'blk.20.attn_q.weight_type', 'blk.20.attn_v.weight', 'blk.20.attn_v.weight_type', 'blk.21.attn_norm.weight', 'blk.21.attn_norm.weight_type', 'blk.21.ffn_down.weight', 'blk.21.ffn_down.weight_type', 'blk.21.ffn_gate.weight', 'blk.21.ffn_gate.weight_type', 'blk.21.ffn_up.weight', 'blk.21.ffn_up.weight_type', 'blk.21.ffn_norm.weight', 'blk.21.ffn_norm.weight_type', 'blk.21.attn_k.weight', 'blk.21.attn_k.weight_type', 'blk.21.attn_output.weight', 'blk.21.attn_output.weight_type', 'blk.21.attn_q.weight', 'blk.21.attn_q.weight_type', 'blk.21.attn_v.weight', 'blk.21.attn_v.weight_type', 'blk.22.attn_k.weight', 'blk.22.attn_k.weight_type', 'blk.22.attn_output.weight', 'blk.22.attn_output.weight_type', 'blk.22.attn_q.weight', 'blk.22.attn_q.weight_type', 'blk.22.attn_v.weight', 'blk.22.attn_v.weight_type', 'output.weight', 'output.weight_type', 'blk.22.attn_norm.weight', 'blk.22.attn_norm.weight_type', 'blk.22.ffn_down.weight', 'blk.22.ffn_down.weight_type', 'blk.22.ffn_gate.weight', 'blk.22.ffn_gate.weight_type', 'blk.22.ffn_up.weight', 'blk.22.ffn_up.weight_type', 'blk.22.ffn_norm.weight', 'blk.22.ffn_norm.weight_type', 'blk.23.attn_norm.weight', 'blk.23.attn_norm.weight_type', 'blk.23.ffn_down.weight', 'blk.23.ffn_down.weight_type', 'blk.23.ffn_gate.weight', 'blk.23.ffn_gate.weight_type', 'blk.23.ffn_up.weight', 'blk.23.ffn_up.weight_type', 'blk.23.ffn_norm.weight', 'blk.23.ffn_norm.weight_type', 'blk.23.attn_k.weight', 'blk.23.attn_k.weight_type', 'blk.23.attn_output.weight', 'blk.23.attn_output.weight_type', 'blk.23.attn_q.weight', 'blk.23.attn_q.weight_type', 'blk.23.attn_v.weight', 'blk.23.attn_v.weight_type', 'blk.24.attn_norm.weight', 'blk.24.attn_norm.weight_type', 'blk.24.ffn_down.weight', 'blk.24.ffn_down.weight_type', 'blk.24.ffn_gate.weight', 'blk.24.ffn_gate.weight_type', 'blk.24.ffn_up.weight', 'blk.24.ffn_up.weight_type', 'blk.24.ffn_norm.weight', 'blk.24.ffn_norm.weight_type', 'blk.24.attn_k.weight', 'blk.24.attn_k.weight_type', 'blk.24.attn_output.weight', 'blk.24.attn_output.weight_type', 'blk.24.attn_q.weight', 'blk.24.attn_q.weight_type', 'blk.24.attn_v.weight', 'blk.24.attn_v.weight_type', 'blk.25.attn_norm.weight', 'blk.25.attn_norm.weight_type', 'blk.25.ffn_down.weight', 'blk.25.ffn_down.weight_type', 'blk.25.ffn_gate.weight', 'blk.25.ffn_gate.weight_type', 'blk.25.ffn_up.weight', 'blk.25.ffn_up.weight_type', 'blk.25.ffn_norm.weight', 'blk.25.ffn_norm.weight_type', 'blk.25.attn_k.weight', 'blk.25.attn_k.weight_type', 'blk.25.attn_output.weight', 'blk.25.attn_output.weight_type', 'blk.25.attn_q.weight', 'blk.25.attn_q.weight_type', 'blk.25.attn_v.weight', 'blk.25.attn_v.weight_type', 'blk.26.attn_norm.weight', 'blk.26.attn_norm.weight_type', 'blk.26.ffn_down.weight', 'blk.26.ffn_down.weight_type', 'blk.26.ffn_gate.weight', 'blk.26.ffn_gate.weight_type', 'blk.26.ffn_up.weight', 'blk.26.ffn_up.weight_type', 'blk.26.ffn_norm.weight', 'blk.26.ffn_norm.weight_type', 'blk.26.attn_k.weight', 'blk.26.attn_k.weight_type', 'blk.26.attn_output.weight', 'blk.26.attn_output.weight_type', 'blk.26.attn_q.weight', 'blk.26.attn_q.weight_type', 'blk.26.attn_v.weight', 'blk.26.attn_v.weight_type', 'blk.27.attn_norm.weight', 'blk.27.attn_norm.weight_type', 'blk.27.ffn_down.weight', 'blk.27.ffn_down.weight_type', 'blk.27.ffn_gate.weight', 'blk.27.ffn_gate.weight_type', 'blk.27.ffn_up.weight', 'blk.27.ffn_up.weight_type', 'blk.27.ffn_norm.weight', 'blk.27.ffn_norm.weight_type', 'blk.27.attn_k.weight', 'blk.27.attn_k.weight_type', 'blk.27.attn_output.weight', 'blk.27.attn_output.weight_type', 'blk.27.attn_q.weight', 'blk.27.attn_q.weight_type', 'blk.27.attn_v.weight', 'blk.27.attn_v.weight_type', 'blk.28.attn_norm.weight', 'blk.28.attn_norm.weight_type', 'blk.28.ffn_down.weight', 'blk.28.ffn_down.weight_type', 'blk.28.ffn_gate.weight', 'blk.28.ffn_gate.weight_type', 'blk.28.ffn_up.weight', 'blk.28.ffn_up.weight_type', 'blk.28.ffn_norm.weight', 'blk.28.ffn_norm.weight_type', 'blk.28.attn_k.weight', 'blk.28.attn_k.weight_type', 'blk.28.attn_output.weight', 'blk.28.attn_output.weight_type', 'blk.28.attn_q.weight', 'blk.28.attn_q.weight_type', 'blk.28.attn_v.weight', 'blk.28.attn_v.weight_type', 'blk.29.attn_norm.weight', 'blk.29.attn_norm.weight_type', 'blk.29.ffn_down.weight', 'blk.29.ffn_down.weight_type', 'blk.29.ffn_gate.weight', 'blk.29.ffn_gate.weight_type', 'blk.29.ffn_up.weight', 'blk.29.ffn_up.weight_type', 'blk.29.ffn_norm.weight', 'blk.29.ffn_norm.weight_type', 'blk.29.attn_k.weight', 'blk.29.attn_k.weight_type', 'blk.29.attn_output.weight', 'blk.29.attn_output.weight_type', 'blk.29.attn_q.weight', 'blk.29.attn_q.weight_type', 'blk.29.attn_v.weight', 'blk.29.attn_v.weight_type', 'blk.30.attn_norm.weight', 'blk.30.attn_norm.weight_type', 'blk.30.ffn_down.weight', 'blk.30.ffn_down.weight_type', 'blk.30.ffn_gate.weight', 'blk.30.ffn_gate.weight_type', 'blk.30.ffn_up.weight', 'blk.30.ffn_up.weight_type', 'blk.30.ffn_norm.weight', 'blk.30.ffn_norm.weight_type', 'blk.30.attn_k.weight', 'blk.30.attn_k.weight_type', 'blk.30.attn_output.weight', 'blk.30.attn_output.weight_type', 'blk.30.attn_q.weight', 'blk.30.attn_q.weight_type', 'blk.30.attn_v.weight', 'blk.30.attn_v.weight_type', 'blk.31.attn_norm.weight', 'blk.31.attn_norm.weight_type', 'blk.31.ffn_down.weight', 'blk.31.ffn_down.weight_type', 'blk.31.ffn_gate.weight', 'blk.31.ffn_gate.weight_type', 'blk.31.ffn_up.weight', 'blk.31.ffn_up.weight_type', 'blk.31.ffn_norm.weight', 'blk.31.ffn_norm.weight_type', 'blk.31.attn_k.weight', 'blk.31.attn_k.weight_type', 'blk.31.attn_output.weight', 'blk.31.attn_output.weight_type', 'blk.31.attn_q.weight', 'blk.31.attn_q.weight_type', 'blk.31.attn_v.weight', 'blk.31.attn_v.weight_type', 'output_norm.weight', 'output_norm.weight_type'])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T08:59:22.961273Z",
     "start_time": "2024-03-01T08:59:22.958025Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T08:55:23.397838Z",
     "start_time": "2024-03-01T08:55:21.508167Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Caches must be initialized first",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchsummary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m torchsummary\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtorchsummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001B[0m, in \u001B[0;36msummary\u001B[0;34m(model, input_size, batch_size, device)\u001B[0m\n\u001B[1;32m     68\u001B[0m model\u001B[38;5;241m.\u001B[39mapply(register_hook)\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# make a forward pass\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# remove these hooks\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m hooks:\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCU/ua-gec-lora/src/packages/training/model_utils.py:93\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, idx, input_pos)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx: Tensor, input_pos: Optional[Tensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 93\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfreqs_cis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaches must be initialized first\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     94\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcausal_mask[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, input_pos]\n\u001B[1;32m     95\u001B[0m     freqs_cis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfreqs_cis[input_pos]\n",
      "\u001B[0;31mAssertionError\u001B[0m: Caches must be initialized first"
     ]
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T08:55:47.867148Z",
     "start_time": "2024-03-01T08:55:47.770879Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
