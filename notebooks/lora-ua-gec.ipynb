{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/romankovalchuk/lora-ua-gec?scriptVersionId=167469943\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!git clone https://github.com/Reennon/ua-gec-lora.git\n!cd ua-gec-lora && pip install -r requirements.txt\n!pwd && ls -a\n# Install additional libs\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n# CD into the project directory\n%cd ua-gec-lora\n!git pull origin \"feature/fine-tuning-research\"\n!git status","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:08:21.070707Z","iopub.execute_input":"2024-03-17T13:08:21.071062Z","iopub.status.idle":"2024-03-17T13:12:17.014744Z","shell.execute_reply.started":"2024-03-17T13:08:21.071035Z","shell.execute_reply":"2024-03-17T13:12:17.013575Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ua-gec-lora'...\nremote: Enumerating objects: 78, done.\u001b[K\nremote: Counting objects: 100% (78/78), done.\u001b[K\nremote: Compressing objects: 100% (61/61), done.\u001b[K\nremote: Total 78 (delta 15), reused 73 (delta 12), pack-reused 0\u001b[K\nUnpacking objects: 100% (78/78), 68.78 KiB | 2.99 MiB/s, done.\nCollecting anyio==3.7.1 (from -r requirements.txt (line 1))\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting dynaconf==3.2.4 (from -r requirements.txt (line 2))\n  Downloading dynaconf-3.2.4-py2.py3-none-any.whl.metadata (9.3 kB)\nCollecting huggingface-hub==0.19.4 (from -r requirements.txt (line 3))\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nCollecting langchain==0.0.329 (from -r requirements.txt (line 4))\n  Downloading langchain-0.0.329-py3-none-any.whl.metadata (16 kB)\nCollecting langsmith==0.0.56 (from -r requirements.txt (line 5))\n  Downloading langsmith-0.0.56-py3-none-any.whl.metadata (10 kB)\nCollecting llama_cpp_python==0.2.13 (from -r requirements.txt (line 6))\n  Downloading llama_cpp_python-0.2.13.tar.gz (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting matplotlib==3.8.1 (from -r requirements.txt (line 7))\n  Downloading matplotlib-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\nCollecting numpy==1.26.1 (from -r requirements.txt (line 8))\n  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protobuf==4.25.0 (from -r requirements.txt (line 9))\n  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting pyarrow==14.0.0 (from -r requirements.txt (line 10))\n  Downloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pydantic==1.10.13 (from -r requirements.txt (line 11))\n  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dotenv==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.0.0)\nCollecting redis==5.0.1 (from -r requirements.txt (line 13))\n  Downloading redis-5.0.1-py3-none-any.whl.metadata (8.9 kB)\nCollecting safetensors==0.4.1 (from -r requirements.txt (line 14))\n  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting singleton-decorator==1.0.0 (from -r requirements.txt (line 15))\n  Downloading singleton-decorator-1.0.0.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting SQLAlchemy==1.4.50 (from -r requirements.txt (line 16))\n  Downloading SQLAlchemy-1.4.50-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10.0 kB)\nCollecting streamlit==1.27.2 (from -r requirements.txt (line 17))\n  Downloading streamlit-1.27.2-py2.py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: tenacity==8.2.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (8.2.3)\nCollecting tokenizers==0.15.0 (from -r requirements.txt (line 19))\n  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting transformers==4.35.2 (from -r requirements.txt (line 20))\n  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (3.6)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (1.33)\nCollecting diskcache>=5.6.1 (from llama_cpp_python==0.2.13->-r requirements.txt (line 6))\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (1.4.5)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (2.8.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy==1.4.50->-r requirements.txt (line 16)) (3.0.3)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (5.2.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (8.1.7)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (6.11.0)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (2.1.4)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (13.7.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (0.10.2)\nRequirement already satisfied: tzlocal<6,>=1.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (5.2)\nCollecting validators<1,>=0.2 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (6.3.3)\nCollecting watchdog>=2.1.5 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 20)) (2023.12.25)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.12.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (0.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.2->-r requirements.txt (line 17)) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.27.2->-r requirements.txt (line 17)) (3.17.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.329->-r requirements.txt (line 4)) (2.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.8.1->-r requirements.txt (line 7)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.2->-r requirements.txt (line 17)) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2.1.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (1.0.0)\nDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dynaconf-3.2.4-py2.py3-none-any.whl (223 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.9/223.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.0.329-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.0.56-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading redis-5.0.1-py3-none-any.whl (250 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-1.4.50-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading streamlit-1.27.2-py2.py3-none-any.whl (7.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading validators-0.22.0-py3-none-any.whl (26 kB)\nDownloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama_cpp_python, singleton-decorator\n  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.13-cp310-cp310-manylinux_2_31_x86_64.whl size=1041996 sha256=cb7977a71e14f6776d026cf5be1459ea7e189907c06d9372f3fe7437c5627c59\n  Stored in directory: /root/.cache/pip/wheels/d3/c9/89/ec02bbfa2283812eb24639ba52e929b9d773f86e4419a7da58\n  Building wheel for singleton-decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for singleton-decorator: filename=singleton_decorator-1.0.0-py3-none-any.whl size=3104 sha256=c0b7b0e0ff3b93bb7c9605aeb47082d21f5be82528efea14f594d14da1401122\n  Stored in directory: /root/.cache/pip/wheels/8e/70/3a/4d16f5c76159a9685f2b247e6501131b67d3c97b078bac2c1e\nSuccessfully built llama_cpp_python singleton-decorator\nInstalling collected packages: singleton-decorator, watchdog, validators, SQLAlchemy, safetensors, redis, pydantic, protobuf, numpy, dynaconf, diskcache, anyio, pydeck, pyarrow, llama_cpp_python, langsmith, huggingface-hub, tokenizers, matplotlib, transformers, langchain, streamlit\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.25\n    Uninstalling SQLAlchemy-2.0.25:\n      Successfully uninstalled SQLAlchemy-2.0.25\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.2\n    Uninstalling safetensors-0.4.2:\n      Successfully uninstalled safetensors-0.4.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.2.0\n    Uninstalling anyio-4.2.0:\n      Successfully uninstalled anyio-4.2.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.1 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.0 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 14.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.0 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.0 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.0 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.50 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.0 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.0 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.0 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.1 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.13 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-1.4.50 anyio-3.7.1 diskcache-5.6.3 dynaconf-3.2.4 huggingface-hub-0.19.4 langchain-0.0.329 langsmith-0.0.56 llama_cpp_python-0.2.13 matplotlib-3.8.1 numpy-1.26.1 protobuf-4.21.12 pyarrow-14.0.0 pydantic-1.10.13 pydeck-0.8.1b0 redis-5.0.1 safetensors-0.4.1 singleton-decorator-1.0.0 streamlit-1.27.2 tokenizers-0.15.0 transformers-4.35.2 validators-0.22.0 watchdog-4.0.0\n/kaggle/working\n.  ..  .virtual_documents  state.db  ua-gec-lora\n/kaggle/working/ua-gec-lora\nFrom https://github.com/Reennon/ua-gec-lora\n * branch            feature/fine-tuning-research -> FETCH_HEAD\nUpdating 198406c..20d73b2\nFast-forward\n notebooks/gec_peft.ipynb                           | 150 \u001b[32m+++++++++++++++++++\u001b[m\u001b[31m--\u001b[m\n .../prompts/instruction_tuning_gec_prompts.py      |  10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 2 files changed, 141 insertions(+), 19 deletions(-)\nOn branch master\nYour branch is ahead of 'origin/master' by 2 commits.\n  (use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, pipeline, Conversation, AutoTokenizer\nfrom src.packages.utils.parameter_server import ParameterServer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:12:17.01701Z","iopub.execute_input":"2024-03-17T13:12:17.017346Z","iopub.status.idle":"2024-03-17T13:12:34.996384Z","shell.execute_reply.started":"2024-03-17T13:12:17.017317Z","shell.execute_reply":"2024-03-17T13:12:34.99556Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-17 13:12:24.000195: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-17 13:12:24.000298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-17 13:12:24.136119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:12:34.997496Z","iopub.execute_input":"2024-03-17T13:12:34.998035Z","iopub.status.idle":"2024-03-17T13:12:35.025288Z","shell.execute_reply.started":"2024-03-17T13:12:34.998009Z","shell.execute_reply":"2024-03-17T13:12:35.024291Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"parameter_server = ParameterServer()\n\nbase_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"mistralai/Mistral-7B-Instruct-v0.2\" # \"bardsai/jaskier-7b-dpo-v6.1\" # \"microsoft/phi-2\" # \"bardsai/jaskier-7b-dpo-v6.1\"","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:12:35.027617Z","iopub.execute_input":"2024-03-17T13:12:35.027895Z","iopub.status.idle":"2024-03-17T13:12:35.038193Z","shell.execute_reply.started":"2024-03-17T13:12:35.027872Z","shell.execute_reply":"2024-03-17T13:12:35.037208Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(base_model_name, load_in_4bit=True, device_map=\"cuda\")\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:12:35.039399Z","iopub.execute_input":"2024-03-17T13:12:35.039782Z","iopub.status.idle":"2024-03-17T13:14:27.032267Z","shell.execute_reply.started":"2024-03-17T13:12:35.039751Z","shell.execute_reply":"2024-03-17T13:14:27.031324Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e1cb980a884e1db326359e573fdf3a"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64b138fa5fb4eada31a66c73ef299e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8a9b4092482462786cb15db9c23b427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c2a8227e064b599d9dc2ccba2df432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e759e3c4b0f49c6b6e50a5ee2a0c7a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482149fb1cb44e429c646333339c2d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1587d7d1a23c46618106d96368ba738d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5c4f7db7624190bdf598b66dfed28d"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:14:27.033535Z","iopub.execute_input":"2024-03-17T13:14:27.033828Z","iopub.status.idle":"2024-03-17T13:14:28.203018Z","shell.execute_reply.started":"2024-03-17T13:14:27.033804Z","shell.execute_reply":"2024-03-17T13:14:28.202084Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc7aa08165d4bbb9a81064c1fda0269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b0265d74054d809d076cfb88c4bc33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e640ffaf2a42cd871c95dd50769656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8aaa0044a5455785efb8b0b3aa1e16"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    inference_mode=False,\n    r=4,\n    lora_alpha=32,\n    lora_dropout=0.1,\n)\n\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:14:28.204145Z","iopub.execute_input":"2024-03-17T13:14:28.204435Z","iopub.status.idle":"2024-03-17T13:14:28.376319Z","shell.execute_reply.started":"2024-03-17T13:14:28.204404Z","shell.execute_reply":"2024-03-17T13:14:28.375289Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable params: 1,703,936 || all params: 7,243,436,032 || trainable%: 0.023523863432663224\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ua_gec","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:14:28.377625Z","iopub.execute_input":"2024-03-17T13:14:28.377977Z","iopub.status.idle":"2024-03-17T13:14:50.647326Z","shell.execute_reply.started":"2024-03-17T13:14:28.377945Z","shell.execute_reply":"2024-03-17T13:14:50.646315Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting ua_gec\n  Downloading ua_gec-2.1.3-py3-none-any.whl.metadata (9.4 kB)\nDownloading ua_gec-2.1.3-py3-none-any.whl (36.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ua_gec\nSuccessfully installed ua_gec-2.1.3\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\nConsider following set of possible error types (\"ERROR_TYPES\"):\n{error_types}\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\n(\"ERROR\") => (\"CORRECTION\")\nWhere (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change.\nThe generated text (\"FIXED_TEXT\") should not contain original errors, or part of these instructions.\nCorrect only Ukrainian language, and do not correct or translate foreign words, like English.\nKeep original information intact, prioritizing the semantics of original text being intact.\n\nORIGINAL_TEXT: {query}\nFIXED_TEXT:\n[/INST]\"\"\"\n\nit_prompt = PromptTemplate(\n    template=template,\n    input_variables=['query', 'error_types']\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:44:19.615656Z","iopub.execute_input":"2024-03-17T14:44:19.616593Z","iopub.status.idle":"2024-03-17T14:44:19.622205Z","shell.execute_reply.started":"2024-03-17T14:44:19.616551Z","shell.execute_reply":"2024-03-17T14:44:19.621251Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:49:46.984679Z","iopub.execute_input":"2024-03-17T14:49:46.985113Z","iopub.status.idle":"2024-03-17T14:49:59.933699Z","shell.execute_reply.started":"2024-03-17T14:49:46.985081Z","shell.execute_reply":"2024-03-17T14:49:59.932482Z"},"trusted":true},"execution_count":186,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')  # Download the necessary resources for sentence tokenization\n\nfrom nltk.tokenize import sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:49:59.935721Z","iopub.execute_input":"2024-03-17T14:49:59.93604Z","iopub.status.idle":"2024-03-17T14:50:00.61719Z","shell.execute_reply.started":"2024-03-17T14:49:59.936013Z","shell.execute_reply":"2024-03-17T14:50:00.616225Z"},"trusted":true},"execution_count":187,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"from src.packages.constants.error_constants import ErrorConstants\nfrom src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\nfrom ua_gec import Corpus\ncorpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\nfor doc in corpus:\n    print(\"\\n---Source starts:---\\n\")\n    print(doc.source[:1000])         # \"I likes it.\"\n    print(\"\\n---Source ends:---\\n\")\n    print(\"\\n---Target starts:---\\n\")\n    print(doc.target[:1100])         # \"I like it.\"\n    print(\"\\n---Target ends---\\n\")\n    print(\"\\n---Annotation starts:---\\n\")\n    print(str(doc.annotated)[:1200])      # <AnnotatedText(\"I {likes=>like} it.\")\n    print(doc.meta.region)    # \"Київська\"\n    print(\"\\n---Annotation ends\")\n    print(\"\\n---Prompt starts\")\n    prompt = it_prompt.format_prompt(\n        query=\"\".join(sent_tokenize(doc.source)[:6]),\n        error_types=ErrorConstants.ERROR_TYPES\n    )\n    print(prompt)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:52:47.683648Z","iopub.execute_input":"2024-03-17T14:52:47.684804Z","iopub.status.idle":"2024-03-17T14:52:47.716739Z","shell.execute_reply.started":"2024-03-17T14:52:47.684768Z","shell.execute_reply":"2024-03-17T14:52:47.715724Z"},"trusted":true},"execution_count":196,"outputs":[{"name":"stdout","text":"\n---Source starts:---\n\nByte for France або “Мій досвід ведення блогу у Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи усім відомого сервісу. Як же автору вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудження від перегляду гарних фото (читання – це менш природний процес, ніж споглядання)\nнескінченне поглинання ніби-то важливої інф\n\n---Source ends:---\n\n\n---Target starts:---\n\nByte for France або “Мій досвід ведення блогу в Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії, щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те, як і навіщо мене занесло в Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням — https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю чи просто коментарій – уже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на котрій можна буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи всім відомого сервісу. Як же авторові вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудження від перегляду гарних фото (читання — це менш природний процес, аніж споглядання);\nнескінченне поглинання нібито важливої інформації про інших людей (користувач відчуває себе у потоці);\nсоціальне підтвердження того, що к\n\n---Target ends---\n\n\n---Annotation starts:---\n\nByte for France або “Мій досвід ведення блогу {у=>в:::error_type=Spelling} Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії{=>,:::error_type=Punctuation} щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те{=>,:::error_type=Punctuation} як і навіщо мене занесло {у=>в:::error_type=Spelling} Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням {–=>—:::error_type=Punctuation} https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю{,=>:::error_type=Punctuation} чи просто коментарій – {вже=>уже:::error_type=Spelling} не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа{=>,:::error_type=Punctuation} на котрій {можно=>можна:::error_type=Spelling} буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи {усім=>всім:::error_type=Spelling} відомого сервісу. Як же {автору=>авторові:::error_type=G/Case} вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудженн\nХарківська\n\n---Annotation ends\n\n---Prompt starts\ntext='[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nConsider following set of possible error types (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\\n(\"ERROR\") => (\"CORRECTION\")\\nWhere (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change.\\nThe generated text (\"FIXED_TEXT\") should not contain original errors, or part of these instructions.\\nCorrect only Ukrainian language, and do not correct or translate foreign words, like English.\\nKeep original information intact, prioritizing the semantics of original text being intact.\\n\\nORIGINAL_TEXT: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про те як і навіщо мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як же автору вдалося передбачити майбутнє?\\nFIXED_TEXT:\\n[/INST]'\n","output_type":"stream"}]},{"cell_type":"code","source":"InstructionTuningGecPrompts.PROMPT","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:14:50.863324Z","iopub.execute_input":"2024-03-17T13:14:50.863684Z","iopub.status.idle":"2024-03-17T13:14:50.869691Z","shell.execute_reply.started":"2024-03-17T13:14:50.863659Z","shell.execute_reply":"2024-03-17T13:14:50.86875Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PromptTemplate(input_variables=['error_types', 'query'], template='Given (\"SOURCE\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nConsider following set of error types (\"ERROR_TYPES\"):\\n{error_types}\\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\\n{{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}}\\nWhere (\"CORRECTION\") Specifies how the error should be corrected.\\nKeep the original text, only correcting errors in it, without outputting this instructions.\\n\\nSOURCE: {query}\\nFINAL ANSWER:')"},"metadata":{}}]},{"cell_type":"code","source":"!git fetch && git pull","metadata":{"execution":{"iopub.status.busy":"2024-03-17T13:14:50.870814Z","iopub.execute_input":"2024-03-17T13:14:50.871107Z","iopub.status.idle":"2024-03-17T13:14:52.329961Z","shell.execute_reply.started":"2024-03-17T13:14:50.871084Z","shell.execute_reply":"2024-03-17T13:14:52.3289Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_len = len(tokenizer.tokenize(prompt.to_string()))\ntokenizer_max_len = 768\nmax_correction_addtional_tokens = 0.1\nmax_new_tokens = prompt_len + int(prompt_len * 0.1)\n\nprint(f\"\"\"\nTokenizer max tokens: {tokenizer_max_len}\nPrompt len: {prompt_len}\nMax token difference because of corrections: {max_correction_addtional_tokens}\nMax new tokens len (output without input): {max_new_tokens}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:52:52.808041Z","iopub.execute_input":"2024-03-17T14:52:52.808449Z","iopub.status.idle":"2024-03-17T14:52:52.81637Z","shell.execute_reply.started":"2024-03-17T14:52:52.808417Z","shell.execute_reply":"2024-03-17T14:52:52.815147Z"},"trusted":true},"execution_count":197,"outputs":[{"name":"stdout","text":"\nTokenizer max tokens: 768\nPrompt len: 543\nMax token difference because of corrections: 0.1\nMax new tokens len (output without input): 597\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fix padding token for Mistral and Phi-2 models\ntokenizer.pad_token = \"[PAD]\"","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:50:20.673675Z","iopub.execute_input":"2024-03-17T14:50:20.67467Z","iopub.status.idle":"2024-03-17T14:50:20.67837Z","shell.execute_reply.started":"2024-03-17T14:50:20.674639Z","shell.execute_reply":"2024-03-17T14:50:20.677581Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"model_inputs = tokenizer(\n    prompt.to_string(), \n    max_length=tokenizer_max_len, \n    padding=\"max_length\", \n    truncation=True, \n    return_tensors=\"pt\"\n)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:52:57.260966Z","iopub.execute_input":"2024-03-17T14:52:57.261314Z","iopub.status.idle":"2024-03-17T14:52:57.279295Z","shell.execute_reply.started":"2024-03-17T14:52:57.261289Z","shell.execute_reply":"2024-03-17T14:52:57.278562Z"},"trusted":true},"execution_count":198,"outputs":[{"execution_count":198,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     1,   733, 16289, 28793, 28777,  5067,\n          4734,  1017,  2153,   775,  1086, 28730,  9219,  1243,  2245,   395,\n          7559, 28725,  4714,   706,   513,  2169, 28725, 16427,   288,   420,\n          8619,   325, 28777,  3212,  3479,  7419,  3198, 19445, 28731, 10290,\n           354, 23129,   753, 15589, 28723,    13, 21432,  1184,  2296,   808,\n           302,  2572,  2118,  4514,  4734,  4732, 28730,  3657, 28735, 21021,\n            13,  1410,  3188, 28718,  2090,   647,   464, 28777,  3212,  3479,\n           647,   464, 28753, 18181, 10223,   647,   464,  4941,  3572,  1421,\n            13,  1227,   996,   406,   272,  2245, 28725,   513,   368,   682,\n          6705,  2118,  4734,  4732,  3548,  6293,   378,  4771,   298,   272,\n          4693, 28747,    13,   618,  4732,  1243,   953,  4734, 28743,  1017,\n           896,  5324,  1243,    13,  9607,  4734, 28743,  1017,   896,  5324,\n          1243,  5583,  8961,   910,   272,  2118,  1023,   347, 27840, 28725,\n          1671, 24685,   272,  2268, 28723,    13,  1014,  7138,  2245,  4734,\n         13286,  1906, 28730,  9219,  1243,  1023,   459,  7001,  3493,  7559,\n         28725,   442,   744,   302,  1167, 11382, 28723,    13,  9903,  3123,\n           865, 23129,   753,  3842, 28725,   304,   511,   459,  4714,   442,\n         17824,  7223,  3085, 28725,   737,  4300, 28723,    13, 24205,  3493,\n          1871, 27248, 28725,  4681,   279,  3864,   272,  3546,   440,  1063,\n           302,  3493,  2245,  1250, 27248, 28723,    13,    13,  1017,  2153,\n           775,  1086, 28730,  9219, 28747, 20376,   354,  4843, 22860,   981,\n         28856, 28813, 28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,\n          1279,  1120,  5378,  1351, 18351, 28838,    13, 28874, 12073,  2077,\n         28705, 28770,  7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,\n          1225,   922,  1931,  1622, 28836,  1586, 20982,   914,   929,  2206,\n          1696, 15610,   929,  1051,  5777, 28869,  2937,  2385,  2200, 11071,\n         28725, 28497,   853,  1974,   929,   800, 28841,  8240,  1351,  1131,\n          8977,  1119, 28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,\n           649,  7361,  5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,\n           939,  1226, 10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224,\n         16822, 28813, 28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,\n          2149, 28788,  4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986,  1131,  8977,  1586,  1185,\n         12309,  1351, 18351, 28723, 28937, 28795,  6986, 28705,  3299,   917,\n          1175,  1051,  1454,  1451,  1224,  1931,  6233,   665,  6307,   929,\n         20959, 28786, 28725,  3846, 28705,  4522,   728,  2127, 16672,  1586,\n         18950, 28803,  1051,  2206, 10328,  1971, 28803,   764,  4449,  1508,\n          2849, 28723,  4138,  6806, 28723,   675, 28748, 28724,   828,   540,\n          2773, 28730,  9763,   449,  1468, 28748,    13,    13, 28856, 28761,\n         28893,  7460, 22948, 18351,    13, 24037, 13279,  5005, 28791,   703,\n          3806,  2127, 27749, 28791,  3553, 28786, 28786, 28842, 28725,  9069,\n          2127,  2276,  1619, 24455,  4278, 28819,   764,   649,  2353,  2409,\n          1119,  2618,  1225, 28842, 28723, 28858,  1125, 22092, 28841,  1120,\n          1931,  2937, 28803,  2127,  3882, 28725,  8647, 15977,  1901,  4873,\n         22020,  2077,  9334, 28842,  2573,  3299,  8067, 19130,  1131,  2850,\n          2353, 28842,  3553,  1185, 12454, 28786, 22131,   929,  1619, 28786,\n          4278, 28819,  4025, 16288,  5213,  1903,  5952,  3647, 15754,   922,\n          1224,  1931,   698, 19505, 23067,  1563,  2937,  3553,  1451,  1224,\n         16064, 28819,  1107, 28723, 28861, 28805,  1120, 13536,  1586,  3697,\n          6531,  1049,   917, 19057,  2149,  1051, 20241,  1351,  7758, 28803,\n          7716,  2054,  2200,   803, 16822,  4093,  4878, 28723, 28937, 28795,\n          9438, 14460,  1728,   649,  1225,  1120,  1931, 27844,  4820,  2348,\n          1224, 24827,  5512, 28786, 28778, 28893, 28804,    13, 13286,  1906,\n         28730,  9219, 28747,    13, 28792, 28748, 16289, 28793]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"response = peft_model.generate(\n    input_ids=model_inputs[\"input_ids\"].to(\"cuda\"),\n    attention_mask=model_inputs[\"attention_mask\"].to(\"cuda\"),\n    max_new_tokens=max_new_tokens\n)\nresponse","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:53:01.06731Z","iopub.execute_input":"2024-03-17T14:53:01.068083Z","iopub.status.idle":"2024-03-17T14:53:31.681043Z","shell.execute_reply.started":"2024-03-17T14:53:01.068051Z","shell.execute_reply":"2024-03-17T14:53:31.679975Z"},"trusted":true},"execution_count":199,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":199,"output_type":"execute_result","data":{"text/plain":"tensor([[   0,    0,    0,  ..., 4714, 2974,    2]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"decoded_outputs = tokenizer.batch_decode(response.detach().cpu().numpy(), skip_special_tokens=True)\ntext = decoded_outputs[0][len(prompt.to_string())-1:]\ntext","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:53:31.682553Z","iopub.execute_input":"2024-03-17T14:53:31.682841Z","iopub.status.idle":"2024-03-17T14:53:31.69044Z","shell.execute_reply.started":"2024-03-17T14:53:31.682815Z","shell.execute_reply":"2024-03-17T14:53:31.689571Z"},"trusted":true},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"'] Byte for France або \"Мій досвід ведення блогу на Instagram\"\\n\\nОстанні три місяці мого життя видалися надто засипаними подіями та емоціями, але нарешті у мене з\\'явилося декілька вільних годин та малої енергії, щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні я розповім про те, як і для чого мене занесло на Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – <https://www.instagram.com/yevhenii_kanivets/>\\n\\nМоє бачення Instagram\\n\\nКолись давно я прочитав статтю, чи було це коментарій – вже не згадую.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на якій можна буде лише розміщувати зображення та ставити лайки.Було це за кілька років до появи усім відомого сервісу.Як автору вдалося передбачити майбутнє?\\n\\n(Note: The text provided is grammatically correct in Ukrainian language. There are no errors to correct.)'"},"metadata":{}}]},{"cell_type":"code","source":"from difflib import SequenceMatcher\n\n\ndef highlight_changes(text1, text2):\n    # Tokenize the texts into words\n    words1 = re.findall(r'\\w+|[^\\w\\s]', text1)\n    words2 = re.findall(r'\\w+|[^\\w\\s]', text2)\n\n\n    # Find the unique words present in both texts\n    all_words = set(words1 + words2)\n\n    # Initialize a SequenceMatcher object\n    matcher = SequenceMatcher(None, words1, words2)\n\n    # Get the differences\n    diff = matcher.get_opcodes()\n\n    highlighted_text = []\n\n    for op, start1, end1, start2, end2 in diff:\n        if op == 'equal':\n            # No change, just append the words as is\n            highlighted_text.extend(words1[start1:end1])\n        elif op == 'delete':\n            # Word(s) removed, highlight with red\n            for word in words1[start1:end1]:\n                word = '\\u0336'.join(word) + '\\u0336'\n                highlighted_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        elif op == 'insert':\n            # Word(s) added, highlight with green\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        elif op == 'replace':\n            # Word(s) replaced, highlight with yellow\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[93m\\033[1m' + word + '\\033[0m')\n\n    return ' '.join(highlighted_text)\n\ndef generate_original_corrected_texts(original_text, corrected_text, highlighted_comparison):\n    # Split the original and corrected texts\n    original_words = original_text.split()\n    corrected_words = corrected_text.split()\n\n    # Initialize empty lists for marked original and corrected texts\n    marked_original_text = []\n    marked_corrected_text = []\n\n    # Track words from the original text that were removed\n    removed_words = set(original_words) - set(corrected_words)\n\n    # Track words from the corrected text that were added\n    added_words = set(corrected_words) - set(original_words)\n\n    # Mark removed words in the original text as red\n    for word in original_words:\n        if word in removed_words:\n            marked_original_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_original_text.append(word)\n\n    # Mark added words in the corrected text as green\n    for word in corrected_words:\n        if word in added_words:\n            marked_corrected_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_corrected_text.append(word)\n\n    return (' '.join(marked_original_text), ' '.join(marked_corrected_text), highlighted_comparison)\n\ntext1 = normalize_spaces(\"\".join(sent_tokenize(doc.source)[:6]))\ntext2 = normalize_spaces(text[2:])\n\nhighlighted_text = highlight_changes(text1, text2)\n\noriginal_text, corrected_text, _ = generate_original_corrected_texts(\n    original_text=text1, \n    corrected_text=text2, \n    highlighted_comparison=highlighted_text)\n\nprint(\"Original Text:\")\nprint(original_text)\nprint()\n\nprint(\"Corrected Text:\")\nprint(corrected_text)\nprint()\n\nprint(\"Changes comparison:\")\nprint(highlighted_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:54:35.325394Z","iopub.execute_input":"2024-03-17T14:54:35.325855Z","iopub.status.idle":"2024-03-17T14:54:35.348448Z","shell.execute_reply.started":"2024-03-17T14:54:35.325823Z","shell.execute_reply":"2024-03-17T14:54:35.347442Z"},"trusted":true},"execution_count":202,"outputs":[{"name":"stdout","text":"Original Text:\nByte for France або \u001b[91m\u001b[1m“Мій\u001b[0m досвід ведення блогу у \u001b[91m\u001b[1mInstagram”\u001b[0m Останні \u001b[91m\u001b[1m3\u001b[0m місяці мого життя видалися \u001b[91m\u001b[1mаж\u001b[0m \u001b[91m\u001b[1mзанадто\u001b[0m \u001b[91m\u001b[1mнасиченими\u001b[0m на \u001b[91m\u001b[1mподії\u001b[0m та \u001b[91m\u001b[1mемоції,\u001b[0m але \u001b[91m\u001b[1mось\u001b[0m нарешті у мене \u001b[91m\u001b[1mз’явилося\u001b[0m декілька вільних годин та \u001b[91m\u001b[1mтрохи\u001b[0m \u001b[91m\u001b[1mенергії\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про \u001b[91m\u001b[1mте\u001b[0m як і \u001b[91m\u001b[1mнавіщо\u001b[0m мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – \u001b[91m\u001b[1mhttps://www.instagram.com/yevhenii_kanivets/\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю, чи \u001b[91m\u001b[1mпросто\u001b[0m коментарій – вже не \u001b[91m\u001b[1mзгадаю.Але\u001b[0m йшлося там про те, що найпопулярнішою соціальною мережею стане \u001b[91m\u001b[1mплатформа\u001b[0m на \u001b[91m\u001b[1mкотрій\u001b[0m \u001b[91m\u001b[1mможно\u001b[0m буде лише \u001b[91m\u001b[1mділитися\u001b[0m \u001b[91m\u001b[1mсвітлинами\u001b[0m та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як \u001b[91m\u001b[1mже\u001b[0m автору вдалося передбачити майбутнє?\n\nCorrected Text:\nByte for France або \u001b[92m\u001b[1m\"Мій\u001b[0m досвід ведення блогу на \u001b[92m\u001b[1mInstagram\"\u001b[0m Останні \u001b[92m\u001b[1mтри\u001b[0m місяці мого життя видалися \u001b[92m\u001b[1mнадто\u001b[0m \u001b[92m\u001b[1mзасипаними\u001b[0m \u001b[92m\u001b[1mподіями\u001b[0m та \u001b[92m\u001b[1mемоціями,\u001b[0m але нарешті у мене \u001b[92m\u001b[1mз'явилося\u001b[0m декілька вільних годин та \u001b[92m\u001b[1mмалої\u001b[0m \u001b[92m\u001b[1mенергії,\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні я розповім про те, як і \u001b[92m\u001b[1mдля\u001b[0m \u001b[92m\u001b[1mчого\u001b[0m мене занесло на Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – \u001b[92m\u001b[1m<https://www.instagram.com/yevhenii_kanivets/>\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю, чи \u001b[92m\u001b[1mбуло\u001b[0m це коментарій – вже не \u001b[92m\u001b[1mзгадую.Але\u001b[0m йшлося там про те, що найпопулярнішою соціальною мережею стане \u001b[92m\u001b[1mплатформа,\u001b[0m на \u001b[92m\u001b[1mякій\u001b[0m \u001b[92m\u001b[1mможна\u001b[0m буде лише \u001b[92m\u001b[1mрозміщувати\u001b[0m \u001b[92m\u001b[1mзображення\u001b[0m та ставити лайки.Було це за \u001b[92m\u001b[1mкілька\u001b[0m років до появи усім відомого сервісу.Як автору вдалося передбачити майбутнє? \u001b[92m\u001b[1m(Note:\u001b[0m \u001b[92m\u001b[1mThe\u001b[0m \u001b[92m\u001b[1mtext\u001b[0m \u001b[92m\u001b[1mprovided\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mgrammatically\u001b[0m \u001b[92m\u001b[1mcorrect\u001b[0m \u001b[92m\u001b[1min\u001b[0m \u001b[92m\u001b[1mUkrainian\u001b[0m \u001b[92m\u001b[1mlanguage.\u001b[0m \u001b[92m\u001b[1mThere\u001b[0m \u001b[92m\u001b[1mare\u001b[0m \u001b[92m\u001b[1mno\u001b[0m \u001b[92m\u001b[1merrors\u001b[0m \u001b[92m\u001b[1mto\u001b[0m \u001b[92m\u001b[1mcorrect.)\u001b[0m\n\nChanges comparison:\nByte for France або \u001b[93m\u001b[1m\"\u001b[0m Мій досвід ведення блогу \u001b[93m\u001b[1mна\u001b[0m Instagram \u001b[93m\u001b[1m\"\u001b[0m Останні \u001b[93m\u001b[1mтри\u001b[0m місяці мого життя видалися \u001b[93m\u001b[1mнадто\u001b[0m \u001b[93m\u001b[1mзасипаними\u001b[0m \u001b[93m\u001b[1mподіями\u001b[0m та \u001b[93m\u001b[1mемоціями\u001b[0m , але \u001b[91m\u001b[1mо̶с̶ь̶\u001b[0m нарешті у мене з \u001b[93m\u001b[1m'\u001b[0m явилося декілька вільних годин та \u001b[93m\u001b[1mмалої\u001b[0m енергії \u001b[92m\u001b[1m,\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства . Сьогодні \u001b[92m\u001b[1mя\u001b[0m розповім про те \u001b[92m\u001b[1m,\u001b[0m як і \u001b[93m\u001b[1mдля\u001b[0m \u001b[93m\u001b[1mчого\u001b[0m мене занесло \u001b[93m\u001b[1mна\u001b[0m Instagram . Якщо цікаво подивитися відразу на результат , то щиро прошу за цим посиланням – \u001b[92m\u001b[1m<\u001b[0m https : / / www . instagram . com / yevhenii_kanivets / \u001b[92m\u001b[1m>\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю , чи \u001b[93m\u001b[1mбуло\u001b[0m \u001b[93m\u001b[1mце\u001b[0m коментарій – вже не \u001b[93m\u001b[1mзгадую\u001b[0m . Але йшлося там про те , що найпопулярнішою соціальною мережею стане платформа \u001b[92m\u001b[1m,\u001b[0m на \u001b[93m\u001b[1mякій\u001b[0m \u001b[93m\u001b[1mможна\u001b[0m буде лише \u001b[93m\u001b[1mрозміщувати\u001b[0m \u001b[93m\u001b[1mзображення\u001b[0m та ставити лайки . Було це за \u001b[93m\u001b[1mкілька\u001b[0m років до появи усім відомого сервісу . Як \u001b[91m\u001b[1mж̶е̶\u001b[0m автору вдалося передбачити майбутнє ? \u001b[92m\u001b[1m(\u001b[0m \u001b[92m\u001b[1mNote\u001b[0m \u001b[92m\u001b[1m:\u001b[0m \u001b[92m\u001b[1mThe\u001b[0m \u001b[92m\u001b[1mtext\u001b[0m \u001b[92m\u001b[1mprovided\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mgrammatically\u001b[0m \u001b[92m\u001b[1mcorrect\u001b[0m \u001b[92m\u001b[1min\u001b[0m \u001b[92m\u001b[1mUkrainian\u001b[0m \u001b[92m\u001b[1mlanguage\u001b[0m \u001b[92m\u001b[1m.\u001b[0m \u001b[92m\u001b[1mThere\u001b[0m \u001b[92m\u001b[1mare\u001b[0m \u001b[92m\u001b[1mno\u001b[0m \u001b[92m\u001b[1merrors\u001b[0m \u001b[92m\u001b[1mto\u001b[0m \u001b[92m\u001b[1mcorrect\u001b[0m \u001b[92m\u001b[1m.\u001b[0m \u001b[92m\u001b[1m)\u001b[0m\n","output_type":"stream"}]}]}