{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/romankovalchuk/lora-ua-gec?scriptVersionId=167968298\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!git clone https://github.com/Reennon/ua-gec-lora.git\n!cd ua-gec-lora && pip install -r requirements.txt\n!pwd && ls -a\n# Install additional libs\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n!pip install ua_gec\n!pip install datasets==2.16.0\n# CD into the project directory\n%cd ua-gec-lora\n!git pull origin \"feature/fine-tuning-research\"\n!git status","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:35:03.810746Z","iopub.execute_input":"2024-03-20T10:35:03.811461Z","iopub.status.idle":"2024-03-20T10:39:56.572692Z","shell.execute_reply.started":"2024-03-20T10:35:03.811428Z","shell.execute_reply":"2024-03-20T10:39:56.571707Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ua-gec-lora'...\nremote: Enumerating objects: 82, done.\u001b[K\nremote: Counting objects: 100% (82/82), done.\u001b[K\nremote: Compressing objects: 100% (65/65), done.\u001b[K\nremote: Total 82 (delta 17), reused 71 (delta 12), pack-reused 0\u001b[K\nUnpacking objects: 100% (82/82), 84.10 KiB | 3.50 MiB/s, done.\nCollecting anyio==3.7.1 (from -r requirements.txt (line 1))\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting dynaconf==3.2.4 (from -r requirements.txt (line 2))\n  Downloading dynaconf-3.2.4-py2.py3-none-any.whl.metadata (9.3 kB)\nCollecting huggingface-hub==0.19.4 (from -r requirements.txt (line 3))\n  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\nCollecting langchain==0.0.329 (from -r requirements.txt (line 4))\n  Downloading langchain-0.0.329-py3-none-any.whl.metadata (16 kB)\nCollecting langsmith==0.0.56 (from -r requirements.txt (line 5))\n  Downloading langsmith-0.0.56-py3-none-any.whl.metadata (10 kB)\nCollecting llama_cpp_python==0.2.13 (from -r requirements.txt (line 6))\n  Downloading llama_cpp_python-0.2.13.tar.gz (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting matplotlib==3.8.1 (from -r requirements.txt (line 7))\n  Downloading matplotlib-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\nCollecting numpy==1.26.1 (from -r requirements.txt (line 8))\n  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protobuf==4.25.0 (from -r requirements.txt (line 9))\n  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting pyarrow==14.0.0 (from -r requirements.txt (line 10))\n  Downloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pydantic==1.10.13 (from -r requirements.txt (line 11))\n  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dotenv==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.0.0)\nCollecting redis==5.0.1 (from -r requirements.txt (line 13))\n  Downloading redis-5.0.1-py3-none-any.whl.metadata (8.9 kB)\nCollecting safetensors==0.4.1 (from -r requirements.txt (line 14))\n  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting singleton-decorator==1.0.0 (from -r requirements.txt (line 15))\n  Downloading singleton-decorator-1.0.0.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting SQLAlchemy==1.4.50 (from -r requirements.txt (line 16))\n  Downloading SQLAlchemy-1.4.50-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10.0 kB)\nCollecting streamlit==1.27.2 (from -r requirements.txt (line 17))\n  Downloading streamlit-1.27.2-py2.py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: tenacity==8.2.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (8.2.3)\nCollecting tokenizers==0.15.0 (from -r requirements.txt (line 19))\n  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting transformers==4.35.2 (from -r requirements.txt (line 20))\n  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (3.6)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio==3.7.1->-r requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.19.4->-r requirements.txt (line 3)) (21.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain==0.0.329->-r requirements.txt (line 4)) (1.33)\nCollecting diskcache>=5.6.1 (from llama_cpp_python==0.2.13->-r requirements.txt (line 6))\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (1.4.5)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.1->-r requirements.txt (line 7)) (2.8.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy==1.4.50->-r requirements.txt (line 16)) (3.0.3)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (5.2.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (8.1.7)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (6.11.0)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (2.1.4)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (13.7.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (0.10.2)\nRequirement already satisfied: tzlocal<6,>=1.1 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (5.2)\nCollecting validators<1,>=0.2 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading validators-0.23.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit==1.27.2->-r requirements.txt (line 17)) (6.3.3)\nCollecting watchdog>=2.1.5 (from streamlit==1.27.2->-r requirements.txt (line 17))\n  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35.2->-r requirements.txt (line 20)) (2023.12.25)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329->-r requirements.txt (line 4)) (1.3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.12.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (0.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.2->-r requirements.txt (line 17)) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.27.2->-r requirements.txt (line 17)) (3.17.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.329->-r requirements.txt (line 4)) (2.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.8.1->-r requirements.txt (line 7)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub==0.19.4->-r requirements.txt (line 3)) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.27.2->-r requirements.txt (line 17)) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2.1.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.27.2->-r requirements.txt (line 17)) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.329->-r requirements.txt (line 4)) (1.0.0)\nDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading dynaconf-3.2.4-py2.py3-none-any.whl (223 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.9/223.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.0.329-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.0.56-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-14.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading redis-5.0.1-py3-none-any.whl (250 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-1.4.50-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading streamlit-1.27.2-py2.py3-none-any.whl (7.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading validators-0.23.2-py3-none-any.whl (27 kB)\nDownloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama_cpp_python, singleton-decorator\n  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.13-cp310-cp310-manylinux_2_31_x86_64.whl size=1042008 sha256=a9a049dd54d7d500155fdfe8db8af66109963c5803f07165e83f360be48431fe\n  Stored in directory: /root/.cache/pip/wheels/d3/c9/89/ec02bbfa2283812eb24639ba52e929b9d773f86e4419a7da58\n  Building wheel for singleton-decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for singleton-decorator: filename=singleton_decorator-1.0.0-py3-none-any.whl size=3104 sha256=73295233aaf2e5db79c9ee85c25765042cf555bbda5277e7c093d08307745a51\n  Stored in directory: /root/.cache/pip/wheels/8e/70/3a/4d16f5c76159a9685f2b247e6501131b67d3c97b078bac2c1e\nSuccessfully built llama_cpp_python singleton-decorator\nInstalling collected packages: singleton-decorator, watchdog, validators, SQLAlchemy, safetensors, redis, pydantic, protobuf, numpy, dynaconf, diskcache, anyio, pydeck, pyarrow, llama_cpp_python, langsmith, huggingface-hub, tokenizers, matplotlib, transformers, langchain, streamlit\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.25\n    Uninstalling SQLAlchemy-2.0.25:\n      Successfully uninstalled SQLAlchemy-2.0.25\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.2\n    Uninstalling safetensors-0.4.2:\n      Successfully uninstalled safetensors-0.4.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: anyio\n    Found existing installation: anyio 4.2.0\n    Uninstalling anyio-4.2.0:\n      Successfully uninstalled anyio-4.2.0\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.1 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.0 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 14.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.0 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.0 which is incompatible.\ngoogle-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.0 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.50 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.0 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.0 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.0 which is incompatible.\ntensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.1 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.13 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-1.4.50 anyio-3.7.1 diskcache-5.6.3 dynaconf-3.2.4 huggingface-hub-0.19.4 langchain-0.0.329 langsmith-0.0.56 llama_cpp_python-0.2.13 matplotlib-3.8.1 numpy-1.26.1 protobuf-4.21.12 pyarrow-14.0.0 pydantic-1.10.13 pydeck-0.8.1b0 redis-5.0.1 safetensors-0.4.1 singleton-decorator-1.0.0 streamlit-1.27.2 tokenizers-0.15.0 transformers-4.35.2 validators-0.23.2 watchdog-4.0.0\n/kaggle/working\n.  ..  .virtual_documents  ua-gec-lora\nCollecting git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Cloning https://github.com/huggingface/trl.git (to revision 7630f877f91c556d9e5a3baa4b6e2894d90ff84c) to /tmp/pip-req-build-fbsip1nx\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-fbsip1nx\n  Running command git rev-parse -q --verify 'sha^7630f877f91c556d9e5a3baa4b6e2894d90ff84c'\n  Running command git fetch -q https://github.com/huggingface/trl.git 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Running command git checkout -q 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Resolved https://github.com/huggingface/trl.git to commit 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (4.39.0.dev0)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (1.26.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (0.29.0.dev0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (2.1.0)\nCollecting tyro>=0.5.11 (from trl==0.7.12.dev0)\n  Downloading tyro-0.7.3-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.19.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.15.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (4.66.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.12.dev0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.7.12.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (14.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.7.12.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.7.12.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.7.12.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.12.dev0) (1.16.0)\nDownloading tyro-0.7.3-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.7.12.dev0-py3-none-any.whl size=173433 sha256=00b984755b77495ebf74b139cf71c99d63e0fa453ac7c33c8abe9e9a97d4721d\n  Stored in directory: /root/.cache/pip/wheels/ad/f5/b1/f5ac48230936583c88cfde8596bc92cad4b0a4b24d0f819c06\nSuccessfully built trl\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.7.12.dev0 tyro-0.7.3\nCollecting ua_gec\n  Downloading ua_gec-2.1.3-py3-none-any.whl.metadata (9.4 kB)\nDownloading ua_gec-2.1.3-py3-none-any.whl (36.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ua_gec\nSuccessfully installed ua_gec-2.1.3\nCollecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (1.26.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (14.0.0)\nCollecting pyarrow-hotfix (from datasets==2.16.0)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.70.16)\nCollecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0)\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.19.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (2024.2.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.16.0)\nDownloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.2.0\n    Uninstalling fsspec-2024.2.0:\n      Successfully uninstalled fsspec-2024.2.0\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.1 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.21.12 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 14.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.6\n/kaggle/working/ua-gec-lora\nFrom https://github.com/Reennon/ua-gec-lora\n * branch            feature/fine-tuning-research -> FETCH_HEAD\n\n*** Please tell me who you are.\n\nRun\n\n  git config --global user.email \"you@example.com\"\n  git config --global user.name \"Your Name\"\n\nto set your account's default identity.\nOmit --global to set the identity only in this repository.\n\nfatal: unable to auto-detect email address (got 'root@ec61b82b66a0.(none)')\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nnothing to commit, working tree clean\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, pipeline, Conversation, AutoTokenizer\nfrom src.packages.utils.parameter_server import ParameterServer\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:39:56.574962Z","iopub.execute_input":"2024-03-20T10:39:56.575342Z","iopub.status.idle":"2024-03-20T10:40:14.464138Z","shell.execute_reply.started":"2024-03-20T10:39:56.575305Z","shell.execute_reply":"2024-03-20T10:40:14.46337Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-20 10:40:03.554298: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-20 10:40:03.554415: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-20 10:40:03.707069: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.cuda.is_available())","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:40:14.465165Z","iopub.execute_input":"2024-03-20T10:40:14.465667Z","iopub.status.idle":"2024-03-20T10:40:14.492303Z","shell.execute_reply.started":"2024-03-20T10:40:14.46564Z","shell.execute_reply":"2024-03-20T10:40:14.491045Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}]},{"cell_type":"code","source":"device = \"cuda\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:40:14.494534Z","iopub.execute_input":"2024-03-20T10:40:14.494823Z","iopub.status.idle":"2024-03-20T10:40:14.505022Z","shell.execute_reply.started":"2024-03-20T10:40:14.494799Z","shell.execute_reply":"2024-03-20T10:40:14.504172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"parameter_server = ParameterServer()\n\nbase_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"mistralai/Mistral-7B-Instruct-v0.2\" # \"bardsai/jaskier-7b-dpo-v6.1\" # \"microsoft/phi-2\" # \"bardsai/jaskier-7b-dpo-v6.1\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:40:14.5063Z","iopub.execute_input":"2024-03-20T10:40:14.507153Z","iopub.status.idle":"2024-03-20T10:40:14.516316Z","shell.execute_reply.started":"2024-03-20T10:40:14.507126Z","shell.execute_reply":"2024-03-20T10:40:14.51529Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(base_model_name, load_in_4bit=True, device_map={'':torch.cuda.current_device()},)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:40:14.517456Z","iopub.execute_input":"2024-03-20T10:40:14.517734Z","iopub.status.idle":"2024-03-20T10:41:57.122092Z","shell.execute_reply.started":"2024-03-20T10:40:14.517706Z","shell.execute_reply":"2024-03-20T10:41:57.121124Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb1feeb9801d4177b362ee6caaaa7bc5"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edf8e6ed5ab0419381998d0a6f6e92e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e7d9f23baa04a149cc4f9e195f5d034"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91eae5f824324c3c8eba149a59f4cdc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75ede384a844bc2925914e7b727fba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3297f45eb644824afe3cc8f4892a95b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f430e2b4c180444ea4687211361743a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93945927c2334eb3ae2839d8be13389b"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model_name, \n    use_fast=False, \n    padding_side=\"left\",\n)\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:41:57.123243Z","iopub.execute_input":"2024-03-20T10:41:57.123538Z","iopub.status.idle":"2024-03-20T10:41:58.486607Z","shell.execute_reply.started":"2024-03-20T10:41:57.123513Z","shell.execute_reply":"2024-03-20T10:41:58.485677Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"022af0ee53b445f394e7a58a37fef626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f042916bdb4a669e5a809e7d5e2ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64c50cf1ea34975ae8a76bfc6b331df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db31705212e14584b769f1dac4aba143"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"LlamaTokenizer(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=4,\n    lora_alpha=32,\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n#     target_modules=[\n#         \"gate_proj\", \n#         \"down_proj\", \n#         \"up_proj\", \n#         \"q_proj\", \n#         \"v_proj\", \n#         \"k_proj\", \n#         \"o_proj\",\n#     ],\n)\n\nmodel.enable_input_require_grads()\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:10:34.099956Z","iopub.execute_input":"2024-03-20T11:10:34.100547Z","iopub.status.idle":"2024-03-20T11:10:34.205911Z","shell.execute_reply.started":"2024-03-20T11:10:34.100502Z","shell.execute_reply":"2024-03-20T11:10:34.204917Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"trainable params: 10,485,760 || all params: 7,252,217,856 || trainable%: 0.14458694165295632\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\nConsider following set of possible error types (\"ERROR_TYPES\"):\n{error_types}\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\n(\"ERROR\") => (\"CORRECTION\")\nWhere (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change.\nThe generated text (\"FIXED_TEXT\") should not contain original errors, additional generated text, comments, nor part of these instructions.\nCorrect only Ukrainian language.\nDo not translate the text; perform grammar correction only for Ukrainian language.\nKeep original information intact, prioritizing the semantics of original text being intact.\n\nORIGINAL_TEXT: {query}\nFIXED_TEXT:\n[/INST]\"\"\"\n\nit_prompt = PromptTemplate(\n    template=template,\n    input_variables=['query', 'error_types']\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:41:58.658931Z","iopub.execute_input":"2024-03-20T10:41:58.659572Z","iopub.status.idle":"2024-03-20T10:41:58.817192Z","shell.execute_reply.started":"2024-03-20T10:41:58.659536Z","shell.execute_reply":"2024-03-20T10:41:58.816421Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:41:58.820446Z","iopub.execute_input":"2024-03-20T10:41:58.820786Z","iopub.status.idle":"2024-03-20T10:42:11.837052Z","shell.execute_reply.started":"2024-03-20T10:41:58.820762Z","shell.execute_reply":"2024-03-20T10:42:11.83575Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')  # Download the necessary resources for sentence tokenization\n\nfrom nltk.tokenize import sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:11.838747Z","iopub.execute_input":"2024-03-20T10:42:11.839112Z","iopub.status.idle":"2024-03-20T10:42:12.483485Z","shell.execute_reply.started":"2024-03-20T10:42:11.83908Z","shell.execute_reply":"2024-03-20T10:42:12.482539Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"from src.packages.constants.error_constants import ErrorConstants\nfrom src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\nfrom ua_gec import Corpus\ncorpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\nfor doc in corpus:\n    print(\"\\n---Source starts:---\\n\")\n    print(doc.source[:1000])         # \"I likes it.\"\n    print(\"\\n---Source ends:---\\n\")\n    print(\"\\n---Target starts:---\\n\")\n    print(doc.target[:1100])         # \"I like it.\"\n    print(\"\\n---Target ends---\\n\")\n    print(\"\\n---Annotation starts:---\\n\")\n    print(str(doc.annotated)[:1200])      # <AnnotatedText(\"I {likes=>like} it.\")\n    print(doc.meta.region)    # \"Київська\"\n    print(\"\\n---Annotation ends\")\n    print(\"\\n---Prompt starts\")\n    prompt = it_prompt.format_prompt(\n        query=\"\".join(sent_tokenize(doc.source)[:6]),\n        error_types=ErrorConstants.ERROR_TYPES\n    )\n    print(prompt)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:12.484762Z","iopub.execute_input":"2024-03-20T10:42:12.485055Z","iopub.status.idle":"2024-03-20T10:42:12.53604Z","shell.execute_reply.started":"2024-03-20T10:42:12.48503Z","shell.execute_reply":"2024-03-20T10:42:12.535065Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n---Source starts:---\n\nByte for France або “Мій досвід ведення блогу у Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи усім відомого сервісу. Як же автору вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудження від перегляду гарних фото (читання – це менш природний процес, ніж споглядання)\nнескінченне поглинання ніби-то важливої інф\n\n---Source ends:---\n\n\n---Target starts:---\n\nByte for France або “Мій досвід ведення блогу в Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії, щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те, як і навіщо мене занесло в Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням — https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю чи просто коментарій – уже не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на котрій можна буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи всім відомого сервісу. Як же авторові вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудження від перегляду гарних фото (читання — це менш природний процес, аніж споглядання);\nнескінченне поглинання нібито важливої інформації про інших людей (користувач відчуває себе у потоці);\nсоціальне підтвердження того, що к\n\n---Target ends---\n\n\n---Annotation starts:---\n\nByte for France або “Мій досвід ведення блогу {у=>в:::error_type=Spelling} Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії{=>,:::error_type=Punctuation} щоб продовжити серію записів щодо мого досвіду блогерства.\n\nСьогодні розповім про те{=>,:::error_type=Punctuation} як і навіщо мене занесло {у=>в:::error_type=Spelling} Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням {–=>—:::error_type=Punctuation} https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю{,=>:::error_type=Punctuation} чи просто коментарій – {вже=>уже:::error_type=Spelling} не згадаю. Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа{=>,:::error_type=Punctuation} на котрій {можно=>можна:::error_type=Spelling} буде лише ділитися світлинами та ставити лайки.\n\nБуло це за декілька років до появи {усім=>всім:::error_type=Spelling} відомого сервісу. Як же {автору=>авторові:::error_type=G/Case} вдалося передбачити майбутнє? Дуже просто!\n\nInstagram втілює глибинні бажання кожної людини:\n\nемоційне збудженн\nХарківська\n\n---Annotation ends\n\n---Prompt starts\ntext='[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nConsider following set of possible error types (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\\n(\"ERROR\") => (\"CORRECTION\")\\nWhere (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change.\\nThe generated text (\"FIXED_TEXT\") should not contain original errors, additional generated text, comments, nor part of these instructions.\\nCorrect only Ukrainian language.\\nDo not translate the text; perform grammar correction only for Ukrainian language.\\nKeep original information intact, prioritizing the semantics of original text being intact.\\n\\nORIGINAL_TEXT: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про те як і навіщо мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як же автору вдалося передбачити майбутнє?\\nFIXED_TEXT:\\n[/INST]'\n","output_type":"stream"}]},{"cell_type":"code","source":"InstructionTuningGecPrompts.PROMPT","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:12.537357Z","iopub.execute_input":"2024-03-20T10:42:12.53764Z","iopub.status.idle":"2024-03-20T10:42:12.543797Z","shell.execute_reply.started":"2024-03-20T10:42:12.537614Z","shell.execute_reply":"2024-03-20T10:42:12.542994Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PromptTemplate(input_variables=['error_types', 'query'], template='Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n{error_types}\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}}\\nOtherwise keep original text.\\n\\nSOURCE: {query}\\nERROR_TYPES: {error_types}\\nFINAL ANSWER:')"},"metadata":{}}]},{"cell_type":"code","source":"!git fetch && git pull","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:12.544811Z","iopub.execute_input":"2024-03-20T10:42:12.545076Z","iopub.status.idle":"2024-03-20T10:42:13.930763Z","shell.execute_reply.started":"2024-03-20T10:42:12.545053Z","shell.execute_reply":"2024-03-20T10:42:13.929781Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_len = len(tokenizer.tokenize(prompt.to_string()))\ntokenizer_max_len = 768\nmax_correction_addtional_tokens = 0.1\nmax_new_tokens = prompt_len + int(prompt_len * 0.1)\n\nprint(f\"\"\"\nTokenizer max tokens: {tokenizer_max_len}\nPrompt len: {prompt_len}\nMax token difference because of corrections: {max_correction_addtional_tokens}\nMax new tokens len (output without input): {max_new_tokens}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:13.932326Z","iopub.execute_input":"2024-03-20T10:42:13.932641Z","iopub.status.idle":"2024-03-20T10:42:13.943628Z","shell.execute_reply.started":"2024-03-20T10:42:13.93261Z","shell.execute_reply":"2024-03-20T10:42:13.942717Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nTokenizer max tokens: 768\nPrompt len: 554\nMax token difference because of corrections: 0.1\nMax new tokens len (output without input): 609\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fix padding token for Mistral and Phi-2 models\ntokenizer.pad_token = \"[PAD]\"","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:13.944864Z","iopub.execute_input":"2024-03-20T10:42:13.945133Z","iopub.status.idle":"2024-03-20T10:42:13.96698Z","shell.execute_reply.started":"2024-03-20T10:42:13.945109Z","shell.execute_reply":"2024-03-20T10:42:13.966159Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model_inputs = tokenizer(\n    prompt.to_string(), \n    max_length=tokenizer_max_len, \n    padding=\"max_length\", \n    truncation=True, \n    return_tensors=\"pt\"\n)\nmodel_inputs","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:13.968044Z","iopub.execute_input":"2024-03-20T10:42:13.968578Z","iopub.status.idle":"2024-03-20T10:42:13.994352Z","shell.execute_reply.started":"2024-03-20T10:42:13.968554Z","shell.execute_reply":"2024-03-20T10:42:13.993542Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     1,   733, 16289, 28793, 28777,  5067,  4734,\n          1017,  2153,   775,  1086, 28730,  9219,  1243,  2245,   395,  7559,\n         28725,  4714,   706,   513,  2169, 28725, 16427,   288,   420,  8619,\n           325, 28777,  3212,  3479,  7419,  3198, 19445, 28731, 10290,   354,\n         23129,   753, 15589, 28723,    13, 21432,  1184,  2296,   808,   302,\n          2572,  2118,  4514,  4734,  4732, 28730,  3657, 28735, 21021,    13,\n          1410,  3188, 28718,  2090,   647,   464, 28777,  3212,  3479,   647,\n           464, 28753, 18181, 10223,   647,   464,  4941,  3572,  1421,    13,\n          1227,   996,   406,   272,  2245, 28725,   513,   368,   682,  6705,\n          2118,  4734,  4732,  3548,  6293,   378,  4771,   298,   272,  4693,\n         28747,    13,   618,  4732,  1243,   953,  4734, 28743,  1017,   896,\n          5324,  1243,    13,  9607,  4734, 28743,  1017,   896,  5324,  1243,\n          5583,  8961,   910,   272,  2118,  1023,   347, 27840, 28725,  1671,\n         24685,   272,  2268, 28723,    13,  1014,  7138,  2245,  4734, 13286,\n          1906, 28730,  9219,  1243,  1023,   459,  7001,  3493,  7559, 28725,\n          4870,  7138,  2245, 28725,  7616, 28725,  4678,   744,   302,  1167,\n         11382, 28723,    13,  9903,  3123,   865, 23129,   753,  3842, 28723,\n            13,  4957,   459, 17824,   272,  2245, 28745,  2225, 18756,  3479,\n         22561,   865,   354, 23129,   753,  3842, 28723,    13, 24205,  3493,\n          1871, 27248, 28725,  4681,   279,  3864,   272,  3546,   440,  1063,\n           302,  3493,  2245,  1250, 27248, 28723,    13,    13,  1017,  2153,\n           775,  1086, 28730,  9219, 28747, 20376,   354,  4843, 22860,   981,\n         28856, 28813, 28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,\n          1279,  1120,  5378,  1351, 18351, 28838,    13, 28874, 12073,  2077,\n         28705, 28770,  7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,\n          1225,   922,  1931,  1622, 28836,  1586, 20982,   914,   929,  2206,\n          1696, 15610,   929,  1051,  5777, 28869,  2937,  2385,  2200, 11071,\n         28725, 28497,   853,  1974,   929,   800, 28841,  8240,  1351,  1131,\n          8977,  1119, 28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,\n           649,  7361,  5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,\n           939,  1226, 10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224,\n         16822, 28813, 28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,\n          2149, 28788,  4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986,  1131,  8977,  1586,  1185,\n         12309,  1351, 18351, 28723, 28937, 28795,  6986, 28705,  3299,   917,\n          1175,  1051,  1454,  1451,  1224,  1931,  6233,   665,  6307,   929,\n         20959, 28786, 28725,  3846, 28705,  4522,   728,  2127, 16672,  1586,\n         18950, 28803,  1051,  2206, 10328,  1971, 28803,   764,  4449,  1508,\n          2849, 28723,  4138,  6806, 28723,   675, 28748, 28724,   828,   540,\n          2773, 28730,  9763,   449,  1468, 28748,    13,    13, 28856, 28761,\n         28893,  7460, 22948, 18351,    13, 24037, 13279,  5005, 28791,   703,\n          3806,  2127, 27749, 28791,  3553, 28786, 28786, 28842, 28725,  9069,\n          2127,  2276,  1619, 24455,  4278, 28819,   764,   649,  2353,  2409,\n          1119,  2618,  1225, 28842, 28723, 28858,  1125, 22092, 28841,  1120,\n          1931,  2937, 28803,  2127,  3882, 28725,  8647, 15977,  1901,  4873,\n         22020,  2077,  9334, 28842,  2573,  3299,  8067, 19130,  1131,  2850,\n          2353, 28842,  3553,  1185, 12454, 28786, 22131,   929,  1619, 28786,\n          4278, 28819,  4025, 16288,  5213,  1903,  5952,  3647, 15754,   922,\n          1224,  1931,   698, 19505, 23067,  1563,  2937,  3553,  1451,  1224,\n         16064, 28819,  1107, 28723, 28861, 28805,  1120, 13536,  1586,  3697,\n          6531,  1049,   917, 19057,  2149,  1051, 20241,  1351,  7758, 28803,\n          7716,  2054,  2200,   803, 16822,  4093,  4878, 28723, 28937, 28795,\n          9438, 14460,  1728,   649,  1225,  1120,  1931, 27844,  4820,  2348,\n          1224, 24827,  5512, 28786, 28778, 28893, 28804,    13, 13286,  1906,\n         28730,  9219, 28747,    13, 28792, 28748, 16289, 28793]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"response = peft_model.generate(\n    input_ids=model_inputs[\"input_ids\"].to(device),\n    attention_mask=model_inputs[\"attention_mask\"].to(device),\n    max_new_tokens=max_new_tokens\n)\nresponse","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:13.995613Z","iopub.execute_input":"2024-03-20T10:42:13.996243Z","iopub.status.idle":"2024-03-20T10:42:46.169292Z","shell.execute_reply.started":"2024-03-20T10:42:13.996196Z","shell.execute_reply":"2024-03-20T10:42:46.168474Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[    0,     0,     0,  ..., 22561,  2974,     2]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"decoded_outputs = tokenizer.batch_decode(response.detach().cpu().numpy(), skip_special_tokens=True)\ntext = decoded_outputs[0][len(prompt.to_string()):]\ntext","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:46.170336Z","iopub.execute_input":"2024-03-20T10:42:46.170618Z","iopub.status.idle":"2024-03-20T10:42:46.228383Z","shell.execute_reply.started":"2024-03-20T10:42:46.170595Z","shell.execute_reply":"2024-03-20T10:42:46.227488Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"' Byte for France або \"Мій досвід ведення блогу на Instagram\"\\n\\nОстанні три місяці мого життя видалися надто засипаними подіями та емоціями, але нарешті у мене з\\'явилося декілька вільних годин та малої енергії, щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні я розповім про те, як і для чого мене занесло на Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – <https://www.instagram.com/yevhenii_kanivets/>\\n\\nМоє бачення Instagram\\n\\nКолись давно я прочитав статтю, чи лише коментарій – вже не згадую.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на якій можна буде лише розміщувати зображення та ставити лайки.Було це за кілька років до появи усім відомого сервісу.Як автору вдалося передбачити майбутнє?\\n\\n(Note: The text provided is grammatically correct in Ukrainian language. Therefore, there is no need for any correction.)'"},"metadata":{}}]},{"cell_type":"code","source":"print(decoded_outputs[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:46.229513Z","iopub.execute_input":"2024-03-20T10:42:46.229803Z","iopub.status.idle":"2024-03-20T10:42:46.234441Z","shell.execute_reply.started":"2024-03-20T10:42:46.229778Z","shell.execute_reply":"2024-03-20T10:42:46.233571Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\nConsider following set of possible error types (\"ERROR_TYPES\"):\n['Fluency', 'Grammar', 'Punctuation', 'Spelling']\nThroughout the text, if you would detect error (\"ERROR\"), fix it according to the structure:\n(\"ERROR\") => (\"CORRECTION\")\nWhere (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change.\nThe generated text (\"FIXED_TEXT\") should not contain original errors, additional generated text, comments, nor part of these instructions.\nCorrect only Ukrainian language.\nDo not translate the text; perform grammar correction only for Ukrainian language.\nKeep original information intact, prioritizing the semantics of original text being intact.\n\nORIGINAL_TEXT: Byte for France або “Мій досвід ведення блогу у Instagram”\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про те як і навіщо мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\n\nМоє бачення Instagram\nКолись давно я прочитав статтю, чи просто коментарій – вже не згадаю.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як же автору вдалося передбачити майбутнє?\nFIXED_TEXT:\n[/INST] Byte for France або \"Мій досвід ведення блогу на Instagram\"\n\nОстанні три місяці мого життя видалися надто засипаними подіями та емоціями, але нарешті у мене з'явилося декілька вільних годин та малої енергії, щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні я розповім про те, як і для чого мене занесло на Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – <https://www.instagram.com/yevhenii_kanivets/>\n\nМоє бачення Instagram\n\nКолись давно я прочитав статтю, чи лише коментарій – вже не згадую.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на якій можна буде лише розміщувати зображення та ставити лайки.Було це за кілька років до появи усім відомого сервісу.Як автору вдалося передбачити майбутнє?\n\n(Note: The text provided is grammatically correct in Ukrainian language. Therefore, there is no need for any correction.)\n","output_type":"stream"}]},{"cell_type":"code","source":"from difflib import SequenceMatcher\nimport re\n\ndef normalize_spaces(text):\n    return ' '.join(text.split())\n\ndef highlight_changes(text1, text2):\n    # Tokenize the texts into words\n    words1 = re.findall(r'\\w+|[^\\w\\s]', text1)\n    words2 = re.findall(r'\\w+|[^\\w\\s]', text2)\n\n\n    # Find the unique words present in both texts\n    all_words = set(words1 + words2)\n\n    # Initialize a SequenceMatcher object\n    matcher = SequenceMatcher(None, words1, words2)\n\n    # Get the differences\n    diff = matcher.get_opcodes()\n\n    highlighted_text = []\n\n    for op, start1, end1, start2, end2 in diff:\n        if op == 'equal':\n            # No change, just append the words as is\n            highlighted_text.extend(words1[start1:end1])\n        elif op == 'delete':\n            # Word(s) removed, highlight with red\n            for word in words1[start1:end1]:\n                word = '\\u0336'.join(word) + '\\u0336'\n                highlighted_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        elif op == 'insert':\n            # Word(s) added, highlight with green\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        elif op == 'replace':\n            # Word(s) replaced, highlight with yellow\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[93m\\033[1m' + word + '\\033[0m')\n\n    return ' '.join(highlighted_text)\n\ndef generate_original_corrected_texts(original_text, corrected_text, highlighted_comparison):\n    # Split the original and corrected texts\n    original_words = original_text.split()\n    corrected_words = corrected_text.split()\n\n    # Initialize empty lists for marked original and corrected texts\n    marked_original_text = []\n    marked_corrected_text = []\n\n    # Track words from the original text that were removed\n    removed_words = set(original_words) - set(corrected_words)\n\n    # Track words from the corrected text that were added\n    added_words = set(corrected_words) - set(original_words)\n\n    # Mark removed words in the original text as red\n    for word in original_words:\n        if word in removed_words:\n            marked_original_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_original_text.append(word)\n\n    # Mark added words in the corrected text as green\n    for word in corrected_words:\n        if word in added_words:\n            marked_corrected_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_corrected_text.append(word)\n\n    return (' '.join(marked_original_text), ' '.join(marked_corrected_text), highlighted_comparison)\n\ntext1 = normalize_spaces(\"\".join(sent_tokenize(doc.source)[:6]))\ntext2 = normalize_spaces(text[1:])\n\nhighlighted_text = highlight_changes(text1, text2)\n\noriginal_text, corrected_text, _ = generate_original_corrected_texts(\n    original_text=text1, \n    corrected_text=text2, \n    highlighted_comparison=highlighted_text)\n\nprint(\"Original Text:\")\nprint(original_text)\nprint()\n\nprint(\"Corrected Text:\")\nprint(corrected_text)\nprint()\n\nprint(\"Changes comparison:\")\nprint(highlighted_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:42:46.235733Z","iopub.execute_input":"2024-03-20T10:42:46.236042Z","iopub.status.idle":"2024-03-20T10:42:46.258052Z","shell.execute_reply.started":"2024-03-20T10:42:46.236017Z","shell.execute_reply":"2024-03-20T10:42:46.257104Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Original Text:\nByte for France або \u001b[91m\u001b[1m“Мій\u001b[0m досвід ведення блогу у \u001b[91m\u001b[1mInstagram”\u001b[0m Останні \u001b[91m\u001b[1m3\u001b[0m місяці мого життя видалися \u001b[91m\u001b[1mаж\u001b[0m \u001b[91m\u001b[1mзанадто\u001b[0m \u001b[91m\u001b[1mнасиченими\u001b[0m на \u001b[91m\u001b[1mподії\u001b[0m та \u001b[91m\u001b[1mемоції,\u001b[0m але \u001b[91m\u001b[1mось\u001b[0m нарешті у мене \u001b[91m\u001b[1mз’явилося\u001b[0m декілька вільних годин та \u001b[91m\u001b[1mтрохи\u001b[0m \u001b[91m\u001b[1mенергії\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про \u001b[91m\u001b[1mте\u001b[0m як і \u001b[91m\u001b[1mнавіщо\u001b[0m мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – \u001b[91m\u001b[1mhttps://www.instagram.com/yevhenii_kanivets/\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю, чи \u001b[91m\u001b[1mпросто\u001b[0m коментарій – вже не \u001b[91m\u001b[1mзгадаю.Але\u001b[0m йшлося там про те, що найпопулярнішою соціальною мережею стане \u001b[91m\u001b[1mплатформа\u001b[0m на \u001b[91m\u001b[1mкотрій\u001b[0m \u001b[91m\u001b[1mможно\u001b[0m буде лише \u001b[91m\u001b[1mділитися\u001b[0m \u001b[91m\u001b[1mсвітлинами\u001b[0m та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як \u001b[91m\u001b[1mже\u001b[0m автору вдалося передбачити майбутнє?\n\nCorrected Text:\nByte for France або \u001b[92m\u001b[1m\"Мій\u001b[0m досвід ведення блогу на \u001b[92m\u001b[1mInstagram\"\u001b[0m Останні \u001b[92m\u001b[1mтри\u001b[0m місяці мого життя видалися \u001b[92m\u001b[1mнадто\u001b[0m \u001b[92m\u001b[1mзасипаними\u001b[0m \u001b[92m\u001b[1mподіями\u001b[0m та \u001b[92m\u001b[1mемоціями,\u001b[0m але нарешті у мене \u001b[92m\u001b[1mз'явилося\u001b[0m декілька вільних годин та \u001b[92m\u001b[1mмалої\u001b[0m \u001b[92m\u001b[1mенергії,\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні я розповім про те, як і \u001b[92m\u001b[1mдля\u001b[0m \u001b[92m\u001b[1mчого\u001b[0m мене занесло на Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – \u001b[92m\u001b[1m<https://www.instagram.com/yevhenii_kanivets/>\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю, чи лише коментарій – вже не \u001b[92m\u001b[1mзгадую.Але\u001b[0m йшлося там про те, що найпопулярнішою соціальною мережею стане \u001b[92m\u001b[1mплатформа,\u001b[0m на \u001b[92m\u001b[1mякій\u001b[0m \u001b[92m\u001b[1mможна\u001b[0m буде лише \u001b[92m\u001b[1mрозміщувати\u001b[0m \u001b[92m\u001b[1mзображення\u001b[0m та ставити лайки.Було це за \u001b[92m\u001b[1mкілька\u001b[0m років до появи усім відомого сервісу.Як автору вдалося передбачити майбутнє? \u001b[92m\u001b[1m(Note:\u001b[0m \u001b[92m\u001b[1mThe\u001b[0m \u001b[92m\u001b[1mtext\u001b[0m \u001b[92m\u001b[1mprovided\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mgrammatically\u001b[0m \u001b[92m\u001b[1mcorrect\u001b[0m \u001b[92m\u001b[1min\u001b[0m \u001b[92m\u001b[1mUkrainian\u001b[0m \u001b[92m\u001b[1mlanguage.\u001b[0m \u001b[92m\u001b[1mTherefore,\u001b[0m \u001b[92m\u001b[1mthere\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mno\u001b[0m \u001b[92m\u001b[1mneed\u001b[0m for \u001b[92m\u001b[1many\u001b[0m \u001b[92m\u001b[1mcorrection.)\u001b[0m\n\nChanges comparison:\nByte for France або \u001b[93m\u001b[1m\"\u001b[0m Мій досвід ведення блогу \u001b[93m\u001b[1mна\u001b[0m Instagram \u001b[93m\u001b[1m\"\u001b[0m Останні \u001b[93m\u001b[1mтри\u001b[0m місяці мого життя видалися \u001b[93m\u001b[1mнадто\u001b[0m \u001b[93m\u001b[1mзасипаними\u001b[0m \u001b[93m\u001b[1mподіями\u001b[0m та \u001b[93m\u001b[1mемоціями\u001b[0m , але \u001b[91m\u001b[1mо̶с̶ь̶\u001b[0m нарешті у мене з \u001b[93m\u001b[1m'\u001b[0m явилося декілька вільних годин та \u001b[93m\u001b[1mмалої\u001b[0m енергії \u001b[92m\u001b[1m,\u001b[0m щоб продовжити серію записів щодо мого досвіду блогерства . Сьогодні \u001b[92m\u001b[1mя\u001b[0m розповім про те \u001b[92m\u001b[1m,\u001b[0m як і \u001b[93m\u001b[1mдля\u001b[0m \u001b[93m\u001b[1mчого\u001b[0m мене занесло \u001b[93m\u001b[1mна\u001b[0m Instagram . Якщо цікаво подивитися відразу на результат , то щиро прошу за цим посиланням – \u001b[92m\u001b[1m<\u001b[0m https : / / www . instagram . com / yevhenii_kanivets / \u001b[92m\u001b[1m>\u001b[0m Моє бачення Instagram Колись давно я прочитав статтю , чи \u001b[93m\u001b[1mлише\u001b[0m коментарій – вже не \u001b[93m\u001b[1mзгадую\u001b[0m . Але йшлося там про те , що найпопулярнішою соціальною мережею стане платформа \u001b[92m\u001b[1m,\u001b[0m на \u001b[93m\u001b[1mякій\u001b[0m \u001b[93m\u001b[1mможна\u001b[0m буде лише \u001b[93m\u001b[1mрозміщувати\u001b[0m \u001b[93m\u001b[1mзображення\u001b[0m та ставити лайки . Було це за \u001b[93m\u001b[1mкілька\u001b[0m років до появи усім відомого сервісу . Як \u001b[91m\u001b[1mж̶е̶\u001b[0m автору вдалося передбачити майбутнє ? \u001b[92m\u001b[1m(\u001b[0m \u001b[92m\u001b[1mNote\u001b[0m \u001b[92m\u001b[1m:\u001b[0m \u001b[92m\u001b[1mThe\u001b[0m \u001b[92m\u001b[1mtext\u001b[0m \u001b[92m\u001b[1mprovided\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mgrammatically\u001b[0m \u001b[92m\u001b[1mcorrect\u001b[0m \u001b[92m\u001b[1min\u001b[0m \u001b[92m\u001b[1mUkrainian\u001b[0m \u001b[92m\u001b[1mlanguage\u001b[0m \u001b[92m\u001b[1m.\u001b[0m \u001b[92m\u001b[1mTherefore\u001b[0m \u001b[92m\u001b[1m,\u001b[0m \u001b[92m\u001b[1mthere\u001b[0m \u001b[92m\u001b[1mis\u001b[0m \u001b[92m\u001b[1mno\u001b[0m \u001b[92m\u001b[1mneed\u001b[0m \u001b[92m\u001b[1mfor\u001b[0m \u001b[92m\u001b[1many\u001b[0m \u001b[92m\u001b[1mcorrection\u001b[0m \u001b[92m\u001b[1m.\u001b[0m \u001b[92m\u001b[1m)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\nfrom nltk.tokenize import sent_tokenize\n\nclass UAGECDataset(Dataset):\n    def __init__(\n        self, \n        generator: object, \n        device: str,\n        prompt: object,\n        max_sentences=6,\n        samples: int = None # if none will use all\n    ):\n        self.text_data = list(generator)\n        \n        if samples:\n            self.text_data = self.text_data[:samples]\n        \n        self.max_sentences = max_sentences\n        self.device = device\n        self.prompt = prompt\n\n    def __len__(self):\n        return len(self.text_data)\n\n    def __getitem__(self, idx):\n        sample = self.text_data[idx]\n        \n        inputs: str = self._preprocess_text(\n            text=sample.source, \n            target_text=sample.target\n        )\n        encodings = self._tokenize_text(\n            text=inputs,\n        ).to(self.device)\n\n        return {\n            'prompt': inputs,\n            'input_ids': encodings[\"input_ids\"].squeeze(0),\n            'attention_mask': encodings[\"attention_mask\"].squeeze(0),\n        }\n    \n    def _preprocess_text(self, text: str, target_text: str) -> torch.tensor:\n        # Select top n sentences\n        text = \"\".join(sent_tokenize(text)[:self.max_sentences])\n        target_text = \"\".join(sent_tokenize(target_text)[:self.max_sentences])\n        # Add instructions (prepend prompt)\n        text = self._format_prompt(text=text)\n\n        text = self._normalize_spaces(text=text)\n        target_text = self._normalize_spaces(text=target_text)\n        # Add target response to input text\n        text += target_text\n        \n        return text\n    \n    def _format_prompt(self, text: str) -> str:\n        return self.prompt.format_prompt(\n            query=text,\n            error_types=ErrorConstants.ERROR_TYPES\n        ).to_string()\n    \n    def _tokenize_text(self, text: str):\n        return tokenizer(\n            text, \n            max_length=tokenizer_max_len, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n    \n    def _add_target(self, text: str, target_text: str):\n        return self.tokenizer(\n            text, \n            max_length=self.tokenizer_max_len, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n    \n    def _normalize_spaces(self, text):\n        return ' '.join(text.split())","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:21:56.081107Z","iopub.execute_input":"2024-03-20T11:21:56.081552Z","iopub.status.idle":"2024-03-20T11:21:56.097485Z","shell.execute_reply.started":"2024-03-20T11:21:56.081518Z","shell.execute_reply":"2024-03-20T11:21:56.096545Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"from ua_gec import Corpus\n\ntrain_corpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\ntest_corpus = Corpus(partition=\"test\", annotation_layer=\"gec-only\")\ntrain_dataset, val_dataset = [UAGECDataset(\n    generator=corpus,\n    device=device,\n    prompt=it_prompt,\n    max_sentences=6,\n    samples=samples\n) for corpus, samples in [(train_corpus, 500), (test_corpus, 50)]]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:22:38.413255Z","iopub.execute_input":"2024-03-20T11:22:38.414177Z","iopub.status.idle":"2024-03-20T11:22:39.224173Z","shell.execute_reply.started":"2024-03-20T11:22:38.414131Z","shell.execute_reply":"2024-03-20T11:22:39.223065Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:22:44.689022Z","iopub.execute_input":"2024-03-20T11:22:44.689728Z","iopub.status.idle":"2024-03-20T11:22:44.736995Z","shell.execute_reply.started":"2024-03-20T11:22:44.689691Z","shell.execute_reply":"2024-03-20T11:22:44.736068Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"{'prompt': '[INST]Given (\"ORIGINAL_TEXT\") text with errors, correct them if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language. Consider following set of possible error types (\"ERROR_TYPES\"): [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\'] Throughout the text, if you would detect error (\"ERROR\"), fix it according to the structure: (\"ERROR\") => (\"CORRECTION\") Where (\"CORRECTION\") Specifies how the error should be corrected, without reasoning the change. The generated text (\"FIXED_TEXT\") should not contain original errors, additional generated text, comments, nor part of these instructions. Correct only Ukrainian language. Do not translate the text; perform grammar correction only for Ukrainian language. Keep original information intact, prioritizing the semantics of original text being intact. ORIGINAL_TEXT: Byte for France або “Мій досвід ведення блогу у Instagram” Останні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про те як і навіщо мене занесло у Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/ Моє бачення Instagram Колись давно я прочитав статтю, чи просто коментарій – вже не згадаю.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа на котрій можно буде лише ділитися світлинами та ставити лайки.Було це за декілька років до появи усім відомого сервісу.Як же автору вдалося передбачити майбутнє? FIXED_TEXT: [/INST]Byte for France або “Мій досвід ведення блогу в Instagram” Останні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії, щоб продовжити серію записів щодо мого досвіду блогерства.Сьогодні розповім про те, як і навіщо мене занесло в Instagram.Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням — https://www.instagram.com/yevhenii_kanivets/ Моє бачення Instagram Колись давно я прочитав статтю чи просто коментарій – уже не згадаю.Але йшлося там про те, що найпопулярнішою соціальною мережею стане платформа, на котрій можна буде лише ділитися світлинами та ставити лайки.Було це за декілька років до появи всім відомого сервісу.Як же авторові вдалося передбачити майбутнє?',\n 'input_ids': tensor([    1,   733, 16289, 28793, 28777,  5067,  4734,  1017,  2153,   775,\n          1086, 28730,  9219,  1243,  2245,   395,  7559, 28725,  4714,   706,\n           513,  2169, 28725, 16427,   288,   420,  8619,   325, 28777,  3212,\n          3479,  7419,  3198, 19445, 28731, 10290,   354, 23129,   753, 15589,\n         28723, 11772,  2296,   808,   302,  2572,  2118,  4514,  4734,  4732,\n         28730,  3657, 28735, 21021,  5936,  3188, 28718,  2090,   647,   464,\n         28777,  3212,  3479,   647,   464, 28753, 18181, 10223,   647,   464,\n          4941,  3572,  1421, 23501,   272,  2245, 28725,   513,   368,   682,\n          6705,  2118,  4734,  4732,  3548,  6293,   378,  4771,   298,   272,\n          4693, 28747,  4734,  4732,  1243,   953,  4734, 28743,  1017,   896,\n          5324,  1243,  6926,  4734, 28743,  1017,   896,  5324,  1243,  5583,\n          8961,   910,   272,  2118,  1023,   347, 27840, 28725,  1671, 24685,\n           272,  2268, 28723,   415,  7138,  2245,  4734, 13286,  1906, 28730,\n          9219,  1243,  1023,   459,  7001,  3493,  7559, 28725,  4870,  7138,\n          2245, 28725,  7616, 28725,  4678,   744,   302,  1167, 11382, 28723,\n          3198,  3123,   865, 23129,   753,  3842, 28723,  2378,   459, 17824,\n           272,  2245, 28745,  2225, 18756,  3479, 22561,   865,   354, 23129,\n           753,  3842, 28723, 10339,  3493,  1871, 27248, 28725,  4681,   279,\n          3864,   272,  3546,   440,  1063,   302,  3493,  2245,  1250, 27248,\n         28723,  3994,  2153,   775,  1086, 28730,  9219, 28747, 20376,   354,\n          4843, 22860,   981, 28856, 28813, 28819,  2149, 28788, 28791,  3319,\n          6048,  6775,  1971,  1279,  1120,  5378,  1351, 18351, 28838,  2807,\n         12073,  2077, 28705, 28770,  7864,  1931,  3299,  4025,   803,  7157,\n         21074,  3542,  1225,   922,  1931,  1622, 28836,  1586, 20982,   914,\n           929,  2206,  1696, 15610,   929,  1051,  5777, 28869,  2937,  2385,\n          2200, 11071, 28725, 28497,   853,  1974,   929,   800, 28841,  8240,\n          1351,  1131,  8977,  1119, 28809, 20241,  1120,  1931,  3697,  6531,\n          1049,   917,   649,  7361,  5109,  8481, 28778,  2937,  3265,   728,\n          8009, 28705,   939,  1226, 10217, 28869,  8647, 28817,  2127, 10410,\n          3139,  1224, 16822, 28813, 28842,  1586, 13014,  2813,  8647,  2054,\n          4025,   803,  2149, 28788,  4093,  2454,  1279,  1120, 28810,  1226,\n          6238, 28723, 28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093,\n         28803,  2127,  3882, 15143,  3213,   929,  4093,  6986,  1131,  8977,\n          1586,  1185, 12309,  1351, 18351, 28723, 28937, 28795,  6986, 28705,\n          3299,   917,  1175,  1051,  1454,  1451,  1224,  1931,  6233,   665,\n          6307,   929, 20959, 28786, 28725,  3846, 28705,  4522,   728,  2127,\n         16672,  1586, 18950, 28803,  1051,  2206, 10328,  1971, 28803,   764,\n          4449,  1508,  2849, 28723,  4138,  6806, 28723,   675, 28748, 28724,\n           828,   540,  2773, 28730,  9763,   449,  1468, 28748,  8968, 28893,\n          7460, 22948, 18351,  5779, 13279,  5005, 28791,   703,  3806,  2127,\n         27749, 28791,  3553, 28786, 28786, 28842, 28725,  9069,  2127,  2276,\n          1619, 24455,  4278, 28819,   764,   649,  2353,  2409,  1119,  2618,\n          1225, 28842, 28723, 28858,  1125, 22092, 28841,  1120,  1931,  2937,\n         28803,  2127,  3882, 28725,  8647, 15977,  1901,  4873, 22020,  2077,\n          9334, 28842,  2573,  3299,  8067, 19130,  1131,  2850,  2353, 28842,\n          3553,  1185, 12454, 28786, 22131,   929,  1619, 28786,  4278, 28819,\n          4025, 16288,  5213,  1903,  5952,  3647, 15754,   922,  1224,  1931,\n           698, 19505, 23067,  1563,  2937,  3553,  1451,  1224, 16064, 28819,\n          1107, 28723, 28861, 28805,  1120, 13536,  1586,  3697,  6531,  1049,\n           917, 19057,  2149,  1051, 20241,  1351,  7758, 28803,  7716,  2054,\n          2200,   803, 16822,  4093,  4878, 28723, 28937, 28795,  9438, 14460,\n          1728,   649,  1225,  1120,  1931, 27844,  4820,  2348,  1224, 24827,\n          5512, 28786, 28778, 28893, 28804, 21467,  1906, 28730,  9219, 28747,\n           733, 28748, 16289, 28793,  8150,   354,  4843, 22860,   981, 28856,\n         28813, 28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,  1279,\n          1120,  5378,   649, 18351, 28838,  2807, 12073,  2077, 28705, 28770,\n          7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,  1225,   922,\n          1931,  1622, 28836,  1586, 20982,   914,   929,  2206,  1696, 15610,\n           929,  1051,  5777, 28869,  2937,  2385,  2200, 11071, 28725, 28497,\n           853,  1974,   929,   800, 28841,  8240,  1351,  1131,  8977,  1119,\n         28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,   649,  7361,\n          5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,   939,  1226,\n         10217, 28869, 28725,  8647, 28817,  2127, 10410,  3139,  1224, 16822,\n         28813, 28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,  2149,\n         28788,  4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723, 28844,\n         28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,  3882,\n         28725, 15143,  3213,   929,  4093,  6986,  1131,  8977,  1586,  1185,\n         12309,   649, 18351, 28723, 28937, 28795,  6986, 28705,  3299,   917,\n          1175,  1051,  1454,  1451,  1224,  1931,  6233,   665,  6307,   929,\n         20959, 28786, 28725,  3846, 28705,  4522,   728,  2127, 16672,  1586,\n         18950, 28803,  1051,  2206, 10328,  1971, 28803,  1040,  4449,  1508,\n          2849, 28723,  4138,  6806, 28723,   675, 28748, 28724,   828,   540,\n          2773, 28730,  9763,   449,  1468, 28748,  8968, 28893,  7460, 22948,\n         18351,  5779, 13279,  5005, 28791,   703,  3806,  2127, 27749, 28791,\n          3553, 28786, 28786, 28842,  9069,  2127,  2276,  1619, 24455,  4278,\n         28819,   764,  1351,  2353,  2409,  1119,  2618,  1225, 28842, 28723,\n         28858,  1125, 22092, 28841,  1120,  1931,  2937, 28803],\n        device='cuda:0'),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:22:48.34336Z","iopub.execute_input":"2024-03-20T11:22:48.343739Z","iopub.status.idle":"2024-03-20T11:22:48.370117Z","shell.execute_reply.started":"2024-03-20T11:22:48.343707Z","shell.execute_reply":"2024-03-20T11:22:48.369276Z"},"trusted":true},"execution_count":109,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ba8b10131846b89579a331d47ff831"}},"metadata":{}}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nfine_tune_model_id = \"rkovalchuk/mistral-7b-it-ua-gec\"\n\n# # Since the model is loaded in 4bit precision, use right-side padding for tokenizer\n# tokenizer.padding_side = 'right'\n\ntraining_arguments = TrainingArguments(\n    output_dir=fine_tune_model_id.split(\"/\")[-1],\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=2e-4,\n    logging_steps=100,\n    num_train_epochs=5,\n    save_total_limit = 2,\n    save_strategy=\"no\",\n    load_best_model_at_end=True,\n    hub_private_repo=False,\n    report_to=None,\n    optim=\"paged_adamw_32bit\",\n)\npeft_model = peft_model.to(device)\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"prompt\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    max_seq_length=max_new_tokens,\n    packing=False,\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:28:36.77301Z","iopub.execute_input":"2024-03-20T11:28:36.773417Z","iopub.status.idle":"2024-03-20T11:28:36.816866Z","shell.execute_reply.started":"2024-03-20T11:28:36.773383Z","shell.execute_reply":"2024-03-20T11:28:36.816146Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"training_arguments.device","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:22:55.779192Z","iopub.execute_input":"2024-03-20T11:22:55.779646Z","iopub.status.idle":"2024-03-20T11:22:55.787826Z","shell.execute_reply.started":"2024-03-20T11:22:55.779611Z","shell.execute_reply":"2024-03-20T11:22:55.78701Z"},"trusted":true},"execution_count":111,"outputs":[{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:28:39.166307Z","iopub.execute_input":"2024-03-20T11:28:39.167279Z","iopub.status.idle":"2024-03-20T11:28:39.610518Z","shell.execute_reply.started":"2024-03-20T11:28:39.167241Z","shell.execute_reply":"2024-03-20T11:28:39.60973Z"},"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"60"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport time\nimport gc\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n\ndef wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n\n    for _ in range(max_retries):\n        info = nvmlDeviceGetMemoryInfo(handle)\n        if info.free >= min_memory_available:\n            break\n        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n        time.sleep(sleep_time)\n    else:\n        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n\n# Usage example\nmin_memory_available = 2 * 1024 * 1024 * 1024  # 2GB\nclear_gpu_memory()\nwait_until_enough_gpu_memory(min_memory_available)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:28:40.77719Z","iopub.execute_input":"2024-03-20T11:28:40.778085Z","iopub.status.idle":"2024-03-20T11:28:41.230645Z","shell.execute_reply.started":"2024-03-20T11:28:40.778049Z","shell.execute_reply":"2024-03-20T11:28:41.229641Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T11:28:42.749378Z","iopub.execute_input":"2024-03-20T11:28:42.749756Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4/75 05:12 < 3:05:07, 0.01 it/s, Epoch 0.19/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T10:45:10.895381Z","iopub.status.idle":"2024-03-20T10:45:10.895883Z","shell.execute_reply.started":"2024-03-20T10:45:10.89564Z","shell.execute_reply":"2024-03-20T10:45:10.895661Z"},"trusted":true},"execution_count":null,"outputs":[]}]}