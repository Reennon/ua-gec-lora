{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:22:32.148444Z",
     "start_time": "2024-03-14T08:22:32.020084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-6qpojs05\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-6qpojs05\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# !pip install auto_gptq==0.2.0\n",
    "# !pip install transformers==4.35\n",
    "# !pip install --upgrade optimum\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "# !pip install --upgrade accelerate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:23:49.011181Z",
     "start_time": "2024-03-14T08:22:32.150320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8894682b7fd4f7692ac78689b7613d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "import torch\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "base_model_name = \"bardsai/jaskier-7b-dpo-v6.1\"\n",
    "generator = pipeline(\"conversational\", model=base_model_name, torch_dtype=torch.half, device_map=parameter_server.settings.device)\n",
    "model = generator.model\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:50:27.848758Z",
     "start_time": "2024-03-14T08:49:43.122271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaTokenizerFast(name_or_path='bardsai/jaskier-7b-dpo-v6.1', vocab_size=32000, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:50:28.032904Z",
     "start_time": "2024-03-14T08:50:27.849802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дарт Вейдер, відомий також як Люк Скайвокер-Вейдер, є вигаданим персонажем зі всесвіту «Зоряних війн». Він постає як темний лицар-джедай, який став одним із головних антагоністів у оригінальній трилогії. Вейдер був асимільований Силою темними рицарями, аби стати Імперією їхнім інструментом, і відомий завдяки своїй чорній броні, поглинутій ненависті та зловісному диханню.\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation(\"Хто такий Дарт Вейдер?\")\n",
    "conversation = generator(conversation, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(conversation.messages[-1][\"content\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:06.718451Z",
     "start_time": "2024-03-14T08:50:28.034219Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:26.775477Z",
     "start_time": "2024-03-14T08:51:26.773084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,703,936 || all params: 7,243,436,032 || trainable%: 0.023523863432663224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:29.608812Z",
     "start_time": "2024-03-14T08:51:29.469121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:47.932148Z",
     "start_time": "2024-03-14T08:51:47.929231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Source starts:---\n",
      "\n",
      "Наступного ранку рівно о одинадцятій годині, коли я сидів сам, дядько Том шаштався в готелі і попросив у лікаря підійти і побачити Джанге Банк, хто, воно здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\" - сказав я: \"Чому вам не піти до лікаря?\".\n",
      "\"Босс\" - сказав він: \"Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він є єдиний лікар в місті і Масса банки сильно погано обурудувані. Він відправив мені, щоб спитав чи погоджуйтеся хоч прийти\".\n",
      "\"Як чоловік до чоловіка\" - \n",
      "\n",
      "---Source ends:---\n",
      "\n",
      "\n",
      "---Target starts:---\n",
      "\n",
      "Наступного ранку рівно об одинадцятій годині, коли я сидів сам, дядько Том шастав у готелі і попросив у лікаря підійти і побачити Джанге Банк, здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\", — сказав я. — Чому вам не піти до лікаря?\".\n",
      "\"Босс, — сказав він. — Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він — єдиний лікар у місті і Масса банки сильно погано обурудувані. Він відправив мене, щоб спитав, чи погоджуєтеся хоч прийти\".\n",
      "\"Як чоловік до чоловіка, — я сказа\n",
      "\n",
      "---Target ends---\n",
      "\n",
      "\n",
      "---Annotation starts:---\n",
      "\n",
      "Наступного ранку рівно {о=>об:::error_type=Spelling} одинадцятій годині, коли я сидів сам, дядько Том {шаштався=>шастав:::error_type=G/UngrammaticalStructure} {в=>у:::error_type=Spelling} готелі і попросив у лікаря підійти і побачити Джанге Банк, {хто, воно =>:::error_type=G/UngrammaticalStructure}здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\"{=>,:::error_type=Punctuation} {-=>—:::error_type=Punctuation} сказав я{: \"=>. — :::error_type=Punctuation}Чому вам не піти до лікаря?\".\n",
      "\"Босс{\"=>,:::error_type=Punctuation} {-=>—:::error_type=Punctuation} сказав він{: \"=>. — :::error_type=Punctuation}Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він {є=>—:::\n",
      "Львівська\n",
      "\n",
      "---Annotation ends\n",
      "\n",
      "---Prompt starts\n",
      "text='Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Наступного ранку рівно о одинадцятій годині, коли я сидів сам, дядько Том шаштався в готелі і попросив у лікаря підійти і побачити Джанге Банк, хто, воно здавалось, це був майор і дуже хворий чоловік.\\n\"Я не є доктор\" - сказав я: \"Чому вам не піти до лікаря?\".\\n\"Босс\" - сказав він: \"Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він є єдиний лікар в місті і Масса банки сильно погано обурудувані. Він відправив мені, щоб спитав чи погоджуйтеся хоч прийти\".\\n\"Як чоловік до чоловіка\" - \\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'\n"
     ]
    }
   ],
   "source": [
    "from src.packages.constants.error_constants import ErrorConstants\n",
    "from src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\n",
    "from ua_gec import Corpus\n",
    "corpus = Corpus(partition=\"test\", annotation_layer=\"gec-only\")\n",
    "for doc in corpus:\n",
    "    print(\"\\n---Source starts:---\\n\")\n",
    "    print(doc.source[:512])         # \"I likes it.\"\n",
    "    print(\"\\n---Source ends:---\\n\")\n",
    "    print(\"\\n---Target starts:---\\n\")\n",
    "    print(doc.target[:512])         # \"I like it.\"\n",
    "    print(\"\\n---Target ends---\\n\")\n",
    "    print(\"\\n---Annotation starts:---\\n\")\n",
    "    print(str(doc.annotated)[:700])      # <AnnotatedText(\"I {likes=>like} it.\")\n",
    "    print(doc.meta.region)    # \"Київська\"\n",
    "    print(\"\\n---Annotation ends\")\n",
    "    print(\"\\n---Prompt starts\")\n",
    "    prompt = InstructionTuningGecPrompts.PROMPT.format_prompt(\n",
    "        query=doc.source[:512],\n",
    "        error_types=ErrorConstants.ERROR_TYPES\n",
    "    )\n",
    "    print(prompt)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:39:56.290851Z",
     "start_time": "2024-03-14T09:39:56.282713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "2860"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(prompt.to_string()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:54:22.454505Z",
     "start_time": "2024-03-14T08:54:22.450844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "'Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю,\\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:30:24.220727Z",
     "start_time": "2024-03-14T09:30:24.218748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    1, 12628,  4734, 17426,  1243,  2245, 28725,  4714,  7559,   513,\n          2169, 28725, 16427,   288,   420,  8619,   325, 28777,  3212,  3479,\n          7419,  3198, 19445, 28731, 10290,   354, 23129,   753, 15589, 28723,\n            13,  8543,  2296,   808,   302,  7559,  4734,  4732, 28730,  3657,\n         28735, 21021,    13,  1410,  3188, 28718,  2090,   647,   464, 28777,\n          3212,  3479,   647,   464, 28753, 18181, 10223,   647,   464,  4941,\n          3572,  1421,    13,  3381,   368,   682,  6705,  2118,  4734,  4732,\n          3548,  8270, 22561,  4734, 28743,  1017,   896,  5324,  1243,  2296,\n           272,  4693,   304,  1229,  6219,  2118,  1212,  4734,  4732, 28730,\n          3657, 21021,    13, 28751,   618,  4732,  1243,  4929,   618, 28743,\n          1017,   896,  5324,  1243,   564, 28747,   618,  4732, 28730,  3657,\n          1243, 28752,    13, 15681,  3578,  1840,  3493,  2245, 28723,    13,\n            13, 17426, 28747, 20376,   354,  4843, 22860,   981, 28856, 28813,\n         28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,  1279,  1120,\n          5378,  1351, 18351, 28838,    13, 28874, 12073,  2077, 28705, 28770,\n          7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,  1225,   922,\n          1931,  1622, 28836,  1586, 20982,   914,   929,  2206,  1696, 15610,\n           929,  1051,  5777, 28869,  2937,  2385,  2200, 11071, 28725, 28497,\n           853,  1974,   929,   800, 28841,  8240,  1351,  1131,  8977,  1119,\n         28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,   649,  7361,\n          5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,   939,  1226,\n         10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224, 16822, 28813,\n         28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,  2149, 28788,\n          4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,    13,    13,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "model_inputs = tokenizer(prompt.to_string(), max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:22:42.190547Z",
     "start_time": "2024-03-14T09:22:42.181272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[    1, 12628,  4734, 17426,  1243,  2245, 28725,  4714,  7559,   513,\n          2169, 28725, 16427,   288,   420,  8619,   325, 28777,  3212,  3479,\n          7419,  3198, 19445, 28731, 10290,   354, 23129,   753, 15589, 28723,\n            13,  8543,  2296,   808,   302,  7559,  4734,  4732, 28730,  3657,\n         28735, 21021,    13,  1410,  3188, 28718,  2090,   647,   464, 28777,\n          3212,  3479,   647,   464, 28753, 18181, 10223,   647,   464,  4941,\n          3572,  1421,    13,  3381,   368,   682,  6705,  2118,  4734,  4732,\n          3548,  8270, 22561,  4734, 28743,  1017,   896,  5324,  1243,  2296,\n           272,  4693,   304,  1229,  6219,  2118,  1212,  4734,  4732, 28730,\n          3657, 21021,    13, 28751,   618,  4732,  1243,  4929,   618, 28743,\n          1017,   896,  5324,  1243,   564, 28747,   618,  4732, 28730,  3657,\n          1243, 28752,    13, 15681,  3578,  1840,  3493,  2245, 28723,    13,\n            13, 17426, 28747, 20376,   354,  4843, 22860,   981, 28856, 28813,\n         28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,  1279,  1120,\n          5378,  1351, 18351, 28838,    13, 28874, 12073,  2077, 28705, 28770,\n          7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,  1225,   922,\n          1931,  1622, 28836,  1586, 20982,   914,   929,  2206,  1696, 15610,\n           929,  1051,  5777, 28869,  2937,  2385,  2200, 11071, 28725, 28497,\n           853,  1974,   929,   800, 28841,  8240,  1351,  1131,  8977,  1119,\n         28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,   649,  7361,\n          5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,   939,  1226,\n         10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224, 16822, 28813,\n         28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,  2149, 28788,\n          4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,    13,    13,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986,  3806,  1051,  1956, 28791,\n          6048,  2266,  1279,  1120, 28810,  1351, 18351, 28725,  1622, 20768,\n          2127,  4025, 28869,  7726, 20159,  1878,   728,  1107, 28725,  4978,\n         28841,   702,  1454, 28725, 23008,  3806,  1119, 28805,   608, 18941,\n         28725,  1622, 20768,  2127,  3882, 28725, 15143,  3806,  1119,  5482,\n         28810,  4978,  2200, 28810,  1224, 12399, 11413,  1563, 28723,    13,\n          4732, 28747,  2807, 12073,  2077, 28705, 28770,  7864,  1931,  3299,\n            13, 28743,  1017,   896,  5324, 28747,  2807, 12073,  2077, 15397,\n          7864,  1931,  3299,  6210, 28747,  4941,  3572,    13,  4732, 28747,\n          4025,   803,  7157, 21074,    13, 28743,  1017,   896,  5324, 28747,\n          4025,   803,  7157, 28786, 10377,  6210]], device='mps:0')"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = peft_model.generate(\n",
    "    input_ids=model_inputs[\"input_ids\"].to(\"mps\"),\n",
    "    attention_mask=model_inputs[\"attention_mask\"].to(\"mps\"),\n",
    "    max_new_tokens=100\n",
    ")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:27:44.797842Z",
     "start_time": "2024-03-14T09:26:47.900216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 356])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:27:44.800094Z",
     "start_time": "2024-03-14T09:27:44.796758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "['<s> Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо я почав вести блог у Instagram, а також про мої перші кроки, перешкоди, які я зустрів, а також про те, як я зміг перемогти над ними.\\nERROR: Останні 3 місяці\\nCORRECTION: Останні три місяці :::Spelling\\nERROR: мого життя\\nCORRECTION: мого життя ::']"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(response.detach().cpu().numpy(), skip_special_tokens=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:28:34.343851Z",
     "start_time": "2024-03-14T09:28:34.340439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps', index=0)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model(\"привіт\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:56:43.947386Z",
     "start_time": "2024-03-14T08:56:43.945578Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
