{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:22:32.148444Z",
     "start_time": "2024-03-14T08:22:32.020084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rkovalch/Documents/UCU/ua-gec-lora/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T14:49:55.996220Z",
     "start_time": "2024-03-14T14:49:55.858332Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-6qpojs05\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /private/var/folders/gg/1kd93k4x47q5_gj92ljtjf000000gq/T/pip-req-build-6qpojs05\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# !pip install auto_gptq==0.2.0\n",
    "# !pip install transformers==4.35\n",
    "# !pip install --upgrade optimum\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "# !pip install --upgrade accelerate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:23:49.011181Z",
     "start_time": "2024-03-14T08:22:32.150320Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65602c9525db432c9eec5c68bf0e08b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "from src.packages.utils.parameter_server import ParameterServer\n",
    "import torch\n",
    "\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "base_model_name = \"bardsai/jaskier-7b-dpo-v6.1\"\n",
    "generator = pipeline(\"conversational\", model=base_model_name, torch_dtype=torch.half, device_map=parameter_server.settings.device)\n",
    "model = generator.model\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T09:04:17.080301Z",
     "start_time": "2024-03-15T09:03:38.565447Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (0.28.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (2.3.0.dev20240301)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.21.4)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from accelerate) (0.4.1)\r\n",
      "Requirement already satisfied: filelock in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\r\n",
      "Requirement already satisfied: sympy in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\r\n",
      "Requirement already satisfied: requests in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpip install accelerate\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM\n\u001B[0;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameter_server\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/modeling_utils.py:3045\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3042\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3045\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3046\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\n\u001B[1;32m   3047\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3048\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3049\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_environment\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_accelerate_available() \u001B[38;5;129;01mand\u001B[39;00m is_bitsandbytes_available()):\n\u001B[0;32m---> 62\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m         )\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_tf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_flax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     69\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m sure the weights are in PyTorch format.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         )\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, load_in_4bit=True, device_map=parameter_server.settings.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T09:05:35.618466Z",
     "start_time": "2024-03-15T09:05:33.839493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaTokenizerFast(name_or_path='bardsai/jaskier-7b-dpo-v6.1', vocab_size=32000, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T09:04:30.895659Z",
     "start_time": "2024-03-15T09:04:30.645777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['load_in_4bit'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m conversation \u001B[38;5;241m=\u001B[39m Conversation(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mХто такий Дарт Вейдер?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m conversation \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconversation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(conversation\u001B[38;5;241m.\u001B[39mmessages[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/pipelines/conversational.py:293\u001B[0m, in \u001B[0;36mConversationalPipeline.__call__\u001B[0;34m(self, conversations, num_workers, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(conversations, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(conversations[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    292\u001B[0m     conversations \u001B[38;5;241m=\u001B[39m [Conversation(conv) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m conversations]\n\u001B[0;32m--> 293\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconversations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/pipelines/base.py:1196\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1190\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1193\u001B[0m         )\n\u001B[1;32m   1194\u001B[0m     )\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/pipelines/base.py:1203\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1202\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1203\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1204\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/pipelines/base.py:1102\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1101\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1102\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/pipelines/conversational.py:312\u001B[0m, in \u001B[0;36mConversationalPipeline._forward\u001B[0;34m(self, model_inputs, minimum_tokens, **generate_kwargs)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_new_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    311\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_new_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m\n\u001B[0;32m--> 312\u001B[0m output_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder:\n\u001B[1;32m    314\u001B[0m     start_position \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/generation/utils.py:1325\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1323\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_model_class()\n\u001B[1;32m   1324\u001B[0m generation_config, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_generation_config(generation_config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1325\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_model_kwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;66;03m# 2. Set generation parameters if not already defined\u001B[39;00m\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/simulationllm/lib/python3.10/site-packages/transformers/generation/utils.py:1121\u001B[0m, in \u001B[0;36mGenerationMixin._validate_model_kwargs\u001B[0;34m(self, model_kwargs)\u001B[0m\n\u001B[1;32m   1118\u001B[0m         unused_model_args\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m unused_model_args:\n\u001B[0;32m-> 1121\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1122\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following `model_kwargs` are not used by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munused_model_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (note: typos in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1123\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m generate arguments will also show up in this list)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1124\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The following `model_kwargs` are not used by the model: ['load_in_4bit'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "conversation = Conversation(\"Хто такий Дарт Вейдер?\")\n",
    "conversation = generator(conversation, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "print(conversation.messages[-1][\"content\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T09:04:37.550152Z",
     "start_time": "2024-03-15T09:04:36.849153Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:26.775477Z",
     "start_time": "2024-03-14T08:51:26.773084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 1,703,936 || all params: 7,243,436,032 || trainable%: 0.023523863432663224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkovalch/miniconda3/envs/simulationllm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:29.608812Z",
     "start_time": "2024-03-14T08:51:29.469121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm()\n            (post_attention_layernorm): MistralRMSNorm()\n          )\n        )\n        (norm): MistralRMSNorm()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:51:47.932148Z",
     "start_time": "2024-03-14T08:51:47.929231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Source starts:---\n",
      "\n",
      "Наступного ранку рівно о одинадцятій годині, коли я сидів сам, дядько Том шаштався в готелі і попросив у лікаря підійти і побачити Джанге Банк, хто, воно здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\" - сказав я: \"Чому вам не піти до лікаря?\".\n",
      "\"Босс\" - сказав він: \"Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він є єдиний лікар в місті і Масса банки сильно погано обурудувані. Він відправив мені, щоб спитав чи погоджуйтеся хоч прийти\".\n",
      "\"Як чоловік до чоловіка\" - \n",
      "\n",
      "---Source ends:---\n",
      "\n",
      "\n",
      "---Target starts:---\n",
      "\n",
      "Наступного ранку рівно об одинадцятій годині, коли я сидів сам, дядько Том шастав у готелі і попросив у лікаря підійти і побачити Джанге Банк, здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\", — сказав я. — Чому вам не піти до лікаря?\".\n",
      "\"Босс, — сказав він. — Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він — єдиний лікар у місті і Масса банки сильно погано обурудувані. Він відправив мене, щоб спитав, чи погоджуєтеся хоч прийти\".\n",
      "\"Як чоловік до чоловіка, — я сказа\n",
      "\n",
      "---Target ends---\n",
      "\n",
      "\n",
      "---Annotation starts:---\n",
      "\n",
      "Наступного ранку рівно {о=>об:::error_type=Spelling} одинадцятій годині, коли я сидів сам, дядько Том {шаштався=>шастав:::error_type=G/UngrammaticalStructure} {в=>у:::error_type=Spelling} готелі і попросив у лікаря підійти і побачити Джанге Банк, {хто, воно =>:::error_type=G/UngrammaticalStructure}здавалось, це був майор і дуже хворий чоловік.\n",
      "\"Я не є доктор\"{=>,:::error_type=Punctuation} {-=>—:::error_type=Punctuation} сказав я{: \"=>. — :::error_type=Punctuation}Чому вам не піти до лікаря?\".\n",
      "\"Босс{\"=>,:::error_type=Punctuation} {-=>—:::error_type=Punctuation} сказав він{: \"=>. — :::error_type=Punctuation}Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він {є=>—:::\n",
      "Львівська\n",
      "\n",
      "---Annotation ends\n",
      "\n",
      "---Prompt starts\n",
      "text='Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Наступного ранку рівно о одинадцятій годині, коли я сидів сам, дядько Том шаштався в готелі і попросив у лікаря підійти і побачити Джанге Банк, хто, воно здавалось, це був майор і дуже хворий чоловік.\\n\"Я не є доктор\" - сказав я: \"Чому вам не піти до лікаря?\".\\n\"Босс\" - сказав він: \"Доктор Хоскінс має проїхати 20 миль по країні, щоб побачити хворих персон. Він є єдиний лікар в місті і Масса банки сильно погано обурудувані. Він відправив мені, щоб спитав чи погоджуйтеся хоч прийти\".\\n\"Як чоловік до чоловіка\" - \\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'\n"
     ]
    }
   ],
   "source": [
    "from src.packages.constants.error_constants import ErrorConstants\n",
    "from src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\n",
    "from ua_gec import Corpus\n",
    "corpus = Corpus(partition=\"test\", annotation_layer=\"gec-only\")\n",
    "for doc in corpus:\n",
    "    print(\"\\n---Source starts:---\\n\")\n",
    "    print(doc.source[:512])         # \"I likes it.\"\n",
    "    print(\"\\n---Source ends:---\\n\")\n",
    "    print(\"\\n---Target starts:---\\n\")\n",
    "    print(doc.target[:512])         # \"I like it.\"\n",
    "    print(\"\\n---Target ends---\\n\")\n",
    "    print(\"\\n---Annotation starts:---\\n\")\n",
    "    print(str(doc.annotated)[:700])      # <AnnotatedText(\"I {likes=>like} it.\")\n",
    "    print(doc.meta.region)    # \"Київська\"\n",
    "    print(\"\\n---Annotation ends\")\n",
    "    print(\"\\n---Prompt starts\")\n",
    "    prompt = InstructionTuningGecPrompts.PROMPT.format_prompt(\n",
    "        query=doc.source[:512],\n",
    "        error_types=ErrorConstants.ERROR_TYPES\n",
    "    )\n",
    "    print(prompt)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:39:56.290851Z",
     "start_time": "2024-03-14T09:39:56.282713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "2860"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(prompt.to_string()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:54:22.454505Z",
     "start_time": "2024-03-14T08:54:22.450844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "'Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо мене занесло у Instagram. Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\n\\nМоє бачення Instagram\\nКолись давно я прочитав статтю,\\nERROR_TYPES: [\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nFINAL ANSWER:'"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_string()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:30:24.220727Z",
     "start_time": "2024-03-14T09:30:24.218748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    1, 12628,  4734, 17426,  1243,  2245, 28725,  4714,  7559,   513,\n          2169, 28725, 16427,   288,   420,  8619,   325, 28777,  3212,  3479,\n          7419,  3198, 19445, 28731, 10290,   354, 23129,   753, 15589, 28723,\n            13,  8543,  2296,   808,   302,  7559,  4734,  4732, 28730,  3657,\n         28735, 21021,    13,  1410,  3188, 28718,  2090,   647,   464, 28777,\n          3212,  3479,   647,   464, 28753, 18181, 10223,   647,   464,  4941,\n          3572,  1421,    13,  3381,   368,   682,  6705,  2118,  4734,  4732,\n          3548,  8270, 22561,  4734, 28743,  1017,   896,  5324,  1243,  2296,\n           272,  4693,   304,  1229,  6219,  2118,  1212,  4734,  4732, 28730,\n          3657, 21021,    13, 28751,   618,  4732,  1243,  4929,   618, 28743,\n          1017,   896,  5324,  1243,   564, 28747,   618,  4732, 28730,  3657,\n          1243, 28752,    13, 15681,  3578,  1840,  3493,  2245, 28723,    13,\n            13, 17426, 28747, 20376,   354,  4843, 22860,   981, 28856, 28813,\n         28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,  1279,  1120,\n          5378,  1351, 18351, 28838,    13, 28874, 12073,  2077, 28705, 28770,\n          7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,  1225,   922,\n          1931,  1622, 28836,  1586, 20982,   914,   929,  2206,  1696, 15610,\n           929,  1051,  5777, 28869,  2937,  2385,  2200, 11071, 28725, 28497,\n           853,  1974,   929,   800, 28841,  8240,  1351,  1131,  8977,  1119,\n         28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,   649,  7361,\n          5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,   939,  1226,\n         10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224, 16822, 28813,\n         28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,  2149, 28788,\n          4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,    13,    13,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "model_inputs = tokenizer(prompt.to_string(), max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:22:42.190547Z",
     "start_time": "2024-03-14T09:22:42.181272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[    1, 12628,  4734, 17426,  1243,  2245, 28725,  4714,  7559,   513,\n          2169, 28725, 16427,   288,   420,  8619,   325, 28777,  3212,  3479,\n          7419,  3198, 19445, 28731, 10290,   354, 23129,   753, 15589, 28723,\n            13,  8543,  2296,   808,   302,  7559,  4734,  4732, 28730,  3657,\n         28735, 21021,    13,  1410,  3188, 28718,  2090,   647,   464, 28777,\n          3212,  3479,   647,   464, 28753, 18181, 10223,   647,   464,  4941,\n          3572,  1421,    13,  3381,   368,   682,  6705,  2118,  4734,  4732,\n          3548,  8270, 22561,  4734, 28743,  1017,   896,  5324,  1243,  2296,\n           272,  4693,   304,  1229,  6219,  2118,  1212,  4734,  4732, 28730,\n          3657, 21021,    13, 28751,   618,  4732,  1243,  4929,   618, 28743,\n          1017,   896,  5324,  1243,   564, 28747,   618,  4732, 28730,  3657,\n          1243, 28752,    13, 15681,  3578,  1840,  3493,  2245, 28723,    13,\n            13, 17426, 28747, 20376,   354,  4843, 22860,   981, 28856, 28813,\n         28819,  2149, 28788, 28791,  3319,  6048,  6775,  1971,  1279,  1120,\n          5378,  1351, 18351, 28838,    13, 28874, 12073,  2077, 28705, 28770,\n          7864,  1931,  3299,  4025,   803,  7157, 21074,  3542,  1225,   922,\n          1931,  1622, 28836,  1586, 20982,   914,   929,  2206,  1696, 15610,\n           929,  1051,  5777, 28869,  2937,  2385,  2200, 11071, 28725, 28497,\n           853,  1974,   929,   800, 28841,  8240,  1351,  1131,  8977,  1119,\n         28809, 20241,  1120,  1931,  3697,  6531,  1049,   917,   649,  7361,\n          5109,  8481, 28778,  2937,  3265,   728,  8009, 28705,   939,  1226,\n         10217, 28869,  8647, 28817,  2127, 10410,  3139,  1224, 16822, 28813,\n         28842,  1586, 13014,  2813,  8647,  2054,  4025,   803,  2149, 28788,\n          4093,  2454,  1279,  1120, 28810,  1226,  6238, 28723,    13,    13,\n         28844, 28822, 14197, 28799,  2077,  9355,  1901,  4093, 28803,  2127,\n          3882, 15143,  3213,   929,  4093,  6986,  3806,  1051,  1956, 28791,\n          6048,  2266,  1279,  1120, 28810,  1351, 18351, 28725,  1622, 20768,\n          2127,  4025, 28869,  7726, 20159,  1878,   728,  1107, 28725,  4978,\n         28841,   702,  1454, 28725, 23008,  3806,  1119, 28805,   608, 18941,\n         28725,  1622, 20768,  2127,  3882, 28725, 15143,  3806,  1119,  5482,\n         28810,  4978,  2200, 28810,  1224, 12399, 11413,  1563, 28723,    13,\n          4732, 28747,  2807, 12073,  2077, 28705, 28770,  7864,  1931,  3299,\n            13, 28743,  1017,   896,  5324, 28747,  2807, 12073,  2077, 15397,\n          7864,  1931,  3299,  6210, 28747,  4941,  3572,    13,  4732, 28747,\n          4025,   803,  7157, 21074,    13, 28743,  1017,   896,  5324, 28747,\n          4025,   803,  7157, 28786, 10377,  6210]], device='mps:0')"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = peft_model.generate(\n",
    "    input_ids=model_inputs[\"input_ids\"].to(\"mps\"),\n",
    "    attention_mask=model_inputs[\"attention_mask\"].to(\"mps\"),\n",
    "    max_new_tokens=100\n",
    ")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:27:44.797842Z",
     "start_time": "2024-03-14T09:26:47.900216Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 356])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:27:44.800094Z",
     "start_time": "2024-03-14T09:27:44.796758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "['<s> Given (\"SOURCE\") text, correct errors if present, fulfilling GEC (Grammar Error Correction) Task for Ukrainian Language.\\nUse following set of errors (\"ERROR_TYPES\"):\\n[\\'Fluency\\', \\'Grammar\\', \\'Punctuation\\', \\'Spelling\\']\\nIf you would detect error (\"ERROR\"), generate correction (\"CORRECTION\") following the structure and specifying error type (\"ERROR_TYPE\"):\\n{(\"ERROR\")=>(\"CORRECTION\"):::(\"ERROR_TYPE\")}\\nOtherwise keep original text.\\n\\nSOURCE: Byte for France або “Мій досвід ведення блогу у Instagram”\\nОстанні 3 місяці мого життя видалися аж занадто насиченими на події та емоції, але ось нарешті у мене з’явилося декілька вільних годин та трохи енергії щоб продовжити серію записів щодо мого досвіду блогерства.\\n\\nСьогодні розповім про те як і навіщо я почав вести блог у Instagram, а також про мої перші кроки, перешкоди, які я зустрів, а також про те, як я зміг перемогти над ними.\\nERROR: Останні 3 місяці\\nCORRECTION: Останні три місяці :::Spelling\\nERROR: мого життя\\nCORRECTION: мого життя ::']"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(response.detach().cpu().numpy(), skip_special_tokens=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T09:28:34.343851Z",
     "start_time": "2024-03-14T09:28:34.340439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps', index=0)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model(\"привіт\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T08:56:43.947386Z",
     "start_time": "2024-03-14T08:56:43.945578Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
