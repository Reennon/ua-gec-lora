{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Reennon/ua-gec-lora.git\n!cd ua-gec-lora && pip install -r requirements.txt\n!pwd && ls -a\n# Install additional libs\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n!pip install ua_gec\n!pip install datasets==2.16.0\n!pip install nltk\n!pip install wandb -q -U\n# CD into the project directory\n%cd ua-gec-lora\n!git pull origin \"feature/fine-tuning-research\"\n!git status","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:34:18.284818Z","iopub.execute_input":"2024-03-21T22:34:18.285097Z","iopub.status.idle":"2024-03-21T22:39:46.507002Z","shell.execute_reply.started":"2024-03-21T22:34:18.285072Z","shell.execute_reply":"2024-03-21T22:39:46.505803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, pipeline, Conversation, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\nfrom src.packages.constants.error_constants import ErrorConstants\nfrom src.packages.prompts.instruction_tuning_gec_prompts import InstructionTuningGecPrompts\nfrom ua_gec import Corpus\nfrom langchain.prompts import PromptTemplate\nfrom kaggle_secrets import UserSecretsClient\nimport torch\nimport nltk\nimport wandb\n\nnltk.download('punkt')  # Download the necessary resources for sentence tokenization\n\nfrom nltk.tokenize import sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:40:05.691677Z","iopub.execute_input":"2024-03-21T22:40:05.692546Z","iopub.status.idle":"2024-03-21T22:40:23.278978Z","shell.execute_reply.started":"2024-03-21T22:40:05.692493Z","shell.execute_reply":"2024-03-21T22:40:23.278003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:40:33.615160Z","iopub.execute_input":"2024-03-21T22:40:33.616029Z","iopub.status.idle":"2024-03-21T22:40:33.622588Z","shell.execute_reply.started":"2024-03-21T22:40:33.615993Z","shell.execute_reply":"2024-03-21T22:40:33.621697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load HuggingFace and Weights & Biases secrets","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:40:35.639669Z","iopub.execute_input":"2024-03-21T22:40:35.640100Z","iopub.status.idle":"2024-03-21T22:40:35.992021Z","shell.execute_reply.started":"2024-03-21T22:40:35.640068Z","shell.execute_reply":"2024-03-21T22:40:35.991273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Login to HuggingFace","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:40:38.596515Z","iopub.execute_input":"2024-03-21T22:40:38.596854Z","iopub.status.idle":"2024-03-21T22:40:40.153275Z","shell.execute_reply.started":"2024-03-21T22:40:38.596829Z","shell.execute_reply":"2024-03-21T22:40:40.151762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Login to Weights & Biases and connect to project","metadata":{}},{"cell_type":"code","source":"wandb_project_name = 'UA-GEC LoRA fine tuning mistral 7B'\n\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project=wandb_project_name, \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:40:42.162993Z","iopub.execute_input":"2024-03-21T22:40:42.163747Z","iopub.status.idle":"2024-03-21T22:41:17.147769Z","shell.execute_reply.started":"2024-03-21T22:40:42.163704Z","shell.execute_reply":"2024-03-21T22:41:17.146789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" #\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\" ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:56:13.936540Z","iopub.execute_input":"2024-03-21T22:56:13.937087Z","iopub.status.idle":"2024-03-21T22:56:13.944765Z","shell.execute_reply.started":"2024-03-21T22:56:13.937049Z","shell.execute_reply":"2024-03-21T22:56:13.943903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16, \n    device_map={'':torch.cuda.current_device()},)\n\n# model.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:56:15.810214Z","iopub.execute_input":"2024-03-21T22:56:15.810993Z","iopub.status.idle":"2024-03-21T22:58:27.602657Z","shell.execute_reply.started":"2024-03-21T22:56:15.810943Z","shell.execute_reply":"2024-03-21T22:58:27.601269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=4,\n    lora_alpha=16,\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\n\n# base_model.enable_input_require_grads()\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:58:34.750108Z","iopub.execute_input":"2024-03-21T22:58:34.750505Z","iopub.status.idle":"2024-03-21T22:58:35.022944Z","shell.execute_reply.started":"2024-03-21T22:58:34.750476Z","shell.execute_reply":"2024-03-21T22:58:35.021993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"[INST] Given a text (\"ORIGINAL_TEXT\") in Ukrainian with potential errors, correct them to fulfill the GEC (Grammar Error Correction) Task, especially tailored for Mistral 7B LLM.\nConsider the provided set of error types (\"ERROR_TYPES\"):\n{error_types}\nWhen you identify an error (\"ERROR\") in the text, correct it according to the format:\n(\"ERROR\") => (\"CORRECTION\")\nThe correction should address the error without providing explicit reasoning for the change.\nThe resulting text (\"FIXED_TEXT\") should be error-free, maintaining the original information's semantics.\nFocus solely on correcting Ukrainian language errors.\nEnsure that the corrected text doesn't include original errors, additional text, comments, or parts of these instructions.\n\nORIGINAL_TEXT: {query}\nFIXED_TEXT:\n[/INST]\"\"\"\n\nit_prompt = PromptTemplate(\n    template=template,\n    input_variables=['query', 'error_types']\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:58:37.151085Z","iopub.execute_input":"2024-03-21T22:58:37.151466Z","iopub.status.idle":"2024-03-21T22:58:37.157307Z","shell.execute_reply.started":"2024-03-21T22:58:37.151437Z","shell.execute_reply":"2024-03-21T22:58:37.156181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_sentences = 4","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:06:08.886294Z","iopub.execute_input":"2024-03-21T23:06:08.887375Z","iopub.status.idle":"2024-03-21T23:06:08.892585Z","shell.execute_reply.started":"2024-03-21T23:06:08.887338Z","shell.execute_reply":"2024-03-21T23:06:08.891571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\nfor doc in corpus:\n#     print(\"\\n---Source starts:---\\n\")\n#     print(doc.source)         # \"I likes it.\"\n#     print(\"\\n---Source ends:---\\n\")\n#     print(\"\\n---Target starts:---\\n\")\n#     print(doc.target)         # \"I like it.\"\n#     print(\"\\n---Target ends---\\n\")\n#     print(\"\\n---Annotation starts:---\\n\")\n#     print(str(doc.annotated)[:1200])      # <AnnotatedText(\"I {likes=>like} it.\")\n#     print(doc.meta.region)    # \"Київська\"\n#     print(\"\\n---Annotation ends\")\n    print(\"\\n---Prompt for training:\")\n    source = \"\".join(sent_tokenize(doc.source)[:max_sentences])\n    target = \"\".join(sent_tokenize(doc.target)[:max_sentences])\n    prompt = it_prompt.format_prompt(\n        query=source,\n        error_types=ErrorConstants.ERROR_TYPES\n    ).to_string()\n    sample_text = ' '.join(prompt.split())\n    target_text = ' '.join(target.split())\n    sample_text += target_text\n    print(sample_text)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:06:09.918733Z","iopub.execute_input":"2024-03-21T23:06:09.919166Z","iopub.status.idle":"2024-03-21T23:06:09.958150Z","shell.execute_reply.started":"2024-03-21T23:06:09.919131Z","shell.execute_reply":"2024-03-21T23:06:09.956791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    padding_side='left',\n    trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.add_eos_token = True","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:05:46.769594Z","iopub.execute_input":"2024-03-21T23:05:46.769991Z","iopub.status.idle":"2024-03-21T23:05:47.019515Z","shell.execute_reply.started":"2024-03-21T23:05:46.769939Z","shell.execute_reply":"2024-03-21T23:05:47.018352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_len = len(tokenizer.tokenize(sample_text))\ntokenizer_max_len = 900\nmax_correction_addtional_tokens = 0.1\nmax_new_tokens = int(prompt_len * 1.1)\n\nprint(f\"\"\"\nPrompt len: {prompt_len}\nTokenize max length: {tokenizer_max_len}\nMax token difference because of corrections: {max_correction_addtional_tokens}\nMax new tokens len (output without input): {max_new_tokens}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:07:02.690102Z","iopub.execute_input":"2024-03-21T23:07:02.690507Z","iopub.status.idle":"2024-03-21T23:07:02.699916Z","shell.execute_reply.started":"2024-03-21T23:07:02.690475Z","shell.execute_reply":"2024-03-21T23:07:02.698921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix padding token for Mistral and Phi-2 models\ntokenizer.pad_token = \"[PAD]\"","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:07:06.622789Z","iopub.execute_input":"2024-03-21T23:07:06.623438Z","iopub.status.idle":"2024-03-21T23:07:06.629256Z","shell.execute_reply.started":"2024-03-21T23:07:06.623388Z","shell.execute_reply":"2024-03-21T23:07:06.628111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_inputs = tokenizer(\n    prompt, \n    max_length=tokenizer_max_len, \n    padding=\"max_length\", \n    truncation=True, \n    return_tensors=\"pt\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:07:07.927709Z","iopub.execute_input":"2024-03-21T23:07:07.928103Z","iopub.status.idle":"2024-03-21T23:07:07.937625Z","shell.execute_reply.started":"2024-03-21T23:07:07.928072Z","shell.execute_reply":"2024-03-21T23:07:07.936372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model = peft_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:07:10.091147Z","iopub.execute_input":"2024-03-21T23:07:10.091990Z","iopub.status.idle":"2024-03-21T23:07:10.111055Z","shell.execute_reply.started":"2024-03-21T23:07:10.091936Z","shell.execute_reply":"2024-03-21T23:07:10.109658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = peft_model.generate(\n    input_ids=model_inputs[\"input_ids\"].to(device),\n    attention_mask=model_inputs[\"attention_mask\"].to(device),\n    max_new_tokens=max_new_tokens\n)\nresponse","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:07:11.319827Z","iopub.execute_input":"2024-03-21T23:07:11.320501Z","iopub.status.idle":"2024-03-21T23:07:39.654638Z","shell.execute_reply.started":"2024-03-21T23:07:11.320467Z","shell.execute_reply":"2024-03-21T23:07:39.653537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded_outputs = tokenizer.batch_decode(response.detach().cpu().numpy(), skip_special_tokens=True)\ntext = decoded_outputs[0][len(prompt):]\ntext","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:08:00.253566Z","iopub.execute_input":"2024-03-21T23:08:00.254236Z","iopub.status.idle":"2024-03-21T23:08:00.265939Z","shell.execute_reply.started":"2024-03-21T23:08:00.254192Z","shell.execute_reply":"2024-03-21T23:08:00.263903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from difflib import SequenceMatcher\nimport re\n\ndef normalize_spaces(text):\n    return ' '.join(text.split())\n\ndef highlight_changes(text1, text2):\n    # Tokenize the texts into words\n    words1 = re.findall(r'\\w+|[^\\w\\s]', text1)\n    words2 = re.findall(r'\\w+|[^\\w\\s]', text2)\n\n\n    # Find the unique words present in both texts\n    all_words = set(words1 + words2)\n\n    # Initialize a SequenceMatcher object\n    matcher = SequenceMatcher(None, words1, words2)\n\n    # Get the differences\n    diff = matcher.get_opcodes()\n\n    highlighted_text = []\n\n    for op, start1, end1, start2, end2 in diff:\n        if op == 'equal':\n            # No change, just append the words as is\n            highlighted_text.extend(words1[start1:end1])\n        elif op == 'delete':\n            # Word(s) removed, highlight with red\n            for word in words1[start1:end1]:\n                word = '\\u0336'.join(word) + '\\u0336'\n                highlighted_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        elif op == 'insert':\n            # Word(s) added, highlight with green\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        elif op == 'replace':\n            # Word(s) replaced, highlight with yellow\n            for word in words2[start2:end2]:\n                highlighted_text.append('\\033[93m\\033[1m' + word + '\\033[0m')\n\n    return ' '.join(highlighted_text)\n\ndef generate_original_corrected_texts(original_text, corrected_text, highlighted_comparison):\n    # Split the original and corrected texts\n    original_words = original_text.split()\n    corrected_words = corrected_text.split()\n\n    # Initialize empty lists for marked original and corrected texts\n    marked_original_text = []\n    marked_corrected_text = []\n\n    # Track words from the original text that were removed\n    removed_words = set(original_words) - set(corrected_words)\n\n    # Track words from the corrected text that were added\n    added_words = set(corrected_words) - set(original_words)\n\n    # Mark removed words in the original text as red\n    for word in original_words:\n        if word in removed_words:\n            marked_original_text.append('\\033[91m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_original_text.append(word)\n\n    # Mark added words in the corrected text as green\n    for word in corrected_words:\n        if word in added_words:\n            marked_corrected_text.append('\\033[92m\\033[1m' + word + '\\033[0m')\n        else:\n            marked_corrected_text.append(word)\n\n    return (' '.join(marked_original_text), ' '.join(marked_corrected_text), highlighted_comparison)\n\ntext1 = normalize_spaces(\"\".join(sent_tokenize(doc.source)[:max_sentences]))\ntext2 = normalize_spaces(text[1:])\n\nhighlighted_text = highlight_changes(text1, text2)\n\noriginal_text, corrected_text, _ = generate_original_corrected_texts(\n    original_text=text1, \n    corrected_text=text2, \n    highlighted_comparison=highlighted_text)\n\nprint(\"Original Text:\")\nprint(original_text)\nprint()\n\nprint(\"Corrected Text:\")\nprint(corrected_text)\nprint()\n\nprint(\"Changes comparison:\")\nprint(highlighted_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:08:23.574229Z","iopub.execute_input":"2024-03-21T23:08:23.575007Z","iopub.status.idle":"2024-03-21T23:08:23.606743Z","shell.execute_reply.started":"2024-03-21T23:08:23.574946Z","shell.execute_reply":"2024-03-21T23:08:23.605699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\nclass UAGECDataset(Dataset):\n    def __init__(\n        self, \n        generator: object, \n        device: str,\n        prompt: object,\n        max_sentences=None,\n        samples: int = None # if none will use all\n    ):\n        self.text_data =  generator#list(generator)\n        \n        if samples:\n            self.text_data = self.text_data[:samples]\n        \n        self.max_sentences = max_sentences\n        self.device = device\n        self.prompt = prompt\n\n    def __len__(self):\n        return len(self.text_data)\n\n    def __getitem__(self, idx):\n        sample = self.text_data[idx]\n        \n        inputs: str = self._preprocess_text(\n            text=sample.source, \n            target_text=sample.target\n        )\n        encodings = self._tokenize_text(\n            text=inputs,\n        ).to(self.device)\n\n        return {\n            'prompt': inputs,\n            'input_ids': encodings[\"input_ids\"].squeeze(0),\n            'attention_mask': encodings[\"attention_mask\"].squeeze(0),\n        }\n    \n    def _preprocess_text(self, text: str, target_text: str) -> torch.tensor:\n        # Select top n sentences\n        text = \"\".join(sent_tokenize(text)[:self.max_sentences] if self.max_sentences else sent_tokenize(text))\n        target_text = \"\".join(sent_tokenize(target_text)[:self.max_sentences] if self.max_sentences else sent_tokenize(target_text))\n        # Add instructions (prepend prompt)\n        text = self._format_prompt(text=text)\n\n        text = self._normalize_spaces(text=text)\n        target_text = self._normalize_spaces(text=target_text)\n        # Add target response to input text\n        text += target_text\n        \n        return text\n    \n    def _format_prompt(self, text: str) -> str:\n        return self.prompt.format_prompt(\n            query=text,\n            error_types=ErrorConstants.ERROR_TYPES\n        ).to_string()\n    \n    def _tokenize_text(self, text: str):\n        return tokenizer(\n            text, \n            max_length=tokenizer_max_len, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n    \n    def _add_target(self, text: str, target_text: str):\n        return self.tokenizer(\n            text, \n            max_length=self.tokenizer_max_len, \n            padding=\"max_length\", \n            truncation=True, \n            return_tensors=\"pt\"\n        )\n    \n    def _normalize_spaces(self, text):\n        return ' '.join(text.split())","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:08:27.001788Z","iopub.execute_input":"2024-03-21T23:08:27.002162Z","iopub.status.idle":"2024-03-21T23:08:27.019268Z","shell.execute_reply.started":"2024-03-21T23:08:27.002132Z","shell.execute_reply":"2024-03-21T23:08:27.017910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_corpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\ntrain_list = list(train_corpus)[:500]\ntest_list = list(train_corpus)[500:550]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:39.573557Z","iopub.execute_input":"2024-03-21T23:12:39.573943Z","iopub.status.idle":"2024-03-21T23:12:40.899331Z","shell.execute_reply.started":"2024-03-21T23:12:39.573912Z","shell.execute_reply":"2024-03-21T23:12:40.898307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_corpus = Corpus(partition=\"train\", annotation_layer=\"gec-only\")\n# test_corpus = Corpus(partition=\"test\", annotation_layer=\"gec-only\")\ntrain_dataset, val_dataset = [UAGECDataset(\n    generator=corpus,\n    device=device,\n    prompt=it_prompt,\n    max_sentences=max_sentences,\n) for corpus in [train_list,test_list]]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:42.436514Z","iopub.execute_input":"2024-03-21T23:12:42.436894Z","iopub.status.idle":"2024-03-21T23:12:42.442047Z","shell.execute_reply.started":"2024-03-21T23:12:42.436863Z","shell.execute_reply":"2024-03-21T23:12:42.440888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:44.850683Z","iopub.execute_input":"2024-03-21T23:12:44.851426Z","iopub.status.idle":"2024-03-21T23:12:44.896732Z","shell.execute_reply.started":"2024-03-21T23:12:44.851394Z","shell.execute_reply":"2024-03-21T23:12:44.895793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nfine_tuned_model_name = \"mistral-7b-ua-gec\"\n\n# # Since the model is loaded in 4bit precision, use right-side padding for tokenizer\npeft_model.config.use_cache = False\ntokenizer.padding_side = 'right'\n\ntraining_arguments = TrainingArguments(\n    output_dir=fine_tuned_model_name,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=True,\n    learning_rate=2e-4,\n    logging_steps=25,\n    num_train_epochs=5,\n    save_total_limit = 2,\n    save_strategy=\"no\",\n    load_best_model_at_end=True,\n    hub_private_repo=False,\n    report_to='wandb',\n    optim=\"paged_adamw_32bit\",\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n)\npeft_model = peft_model.to(device)\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"prompt\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    max_seq_length=tokenizer_max_len,\n    packing=False,\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:48.404549Z","iopub.execute_input":"2024-03-21T23:12:48.405217Z","iopub.status.idle":"2024-03-21T23:12:48.439367Z","shell.execute_reply.started":"2024-03-21T23:12:48.405181Z","shell.execute_reply":"2024-03-21T23:12:48.438606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments.device","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:51.574861Z","iopub.execute_input":"2024-03-21T23:12:51.575612Z","iopub.status.idle":"2024-03-21T23:12:51.581335Z","shell.execute_reply.started":"2024-03-21T23:12:51.575580Z","shell.execute_reply":"2024-03-21T23:12:51.580257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    print(gc.collect())","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:52.711360Z","iopub.execute_input":"2024-03-21T23:12:52.712232Z","iopub.status.idle":"2024-03-21T23:12:52.716710Z","shell.execute_reply.started":"2024-03-21T23:12:52.712198Z","shell.execute_reply":"2024-03-21T23:12:52.715702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clear_gpu_memory()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:08:46.375780Z","iopub.execute_input":"2024-03-21T23:08:46.376615Z","iopub.status.idle":"2024-03-21T23:08:46.884717Z","shell.execute_reply.started":"2024-03-21T23:08:46.376580Z","shell.execute_reply":"2024-03-21T23:08:46.883621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n\n    for _ in range(max_retries):\n        clear_gpu_memory()\n        info = nvmlDeviceGetMemoryInfo(handle)\n        if info.free >= min_memory_available:\n            break\n        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n        time.sleep(sleep_time)\n    else:\n        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n\n# Usage example\nmin_memory_available = 2 * 1024 * 1024 * 1024  # 2GB\nclear_gpu_memory()\nwait_until_enough_gpu_memory(min_memory_available)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:55.258539Z","iopub.execute_input":"2024-03-21T23:12:55.259455Z","iopub.status.idle":"2024-03-21T23:12:56.434760Z","shell.execute_reply.started":"2024-03-21T23:12:55.259421Z","shell.execute_reply":"2024-03-21T23:12:56.433734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:12:59.475090Z","iopub.execute_input":"2024-03-21T23:12:59.475486Z","iopub.status.idle":"2024-03-21T23:14:43.839167Z","shell.execute_reply.started":"2024-03-21T23:12:59.475455Z","shell.execute_reply":"2024-03-21T23:14:43.837512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(fine_tuned_model_name)\nwandb.finish()\npeft_model.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:10:01.122587Z","iopub.execute_input":"2024-03-21T23:10:01.123269Z","iopub.status.idle":"2024-03-21T23:10:06.653825Z","shell.execute_reply.started":"2024-03-21T23:10:01.123231Z","shell.execute_reply":"2024-03-21T23:10:06.652980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T23:10:10.600155Z","iopub.execute_input":"2024-03-21T23:10:10.600521Z","iopub.status.idle":"2024-03-21T23:10:15.964125Z","shell.execute_reply.started":"2024-03-21T23:10:10.600490Z","shell.execute_reply":"2024-03-21T23:10:15.963208Z"},"trusted":true},"execution_count":null,"outputs":[]}]}